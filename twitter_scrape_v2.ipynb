{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# New FISP Presidential Project Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depreceated function from when data was saved to a Google sheet\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "def authenticate_gspread():\n",
    "  # scopes that your application should be granted access\n",
    "  scope = ['https://spreadsheets.google.com/feeds'] \n",
    "  # Create a Credentials object from the service account's credentials and the scopes\n",
    "  credentials = ServiceAccountCredentials.from_json_keyfile_name('auth.json', scope)\n",
    "  gc = gspread.authorize(credentials)\n",
    "  return gc\n",
    "  \n",
    "# gets the list of cand or pac and returns it in a list\n",
    "def gspread_get_lists(worksheet, is_cand):\n",
    "  names = filter(lambda x: len(x) > 0, worksheet.col_values(2))\n",
    "  max_ids = worksheet.col_values(3)[:len(names)]\n",
    "  counts = worksheet.col_values(4)[:len(names)]\n",
    "  indices = range(1,len(names)+1)\n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dropbox import DropboxOAuth2FlowNoRedirect\n",
    "def authenticate_dropbox():\n",
    "  auth_flow = DropboxOAuth2FlowNoRedirect(ac.APP_KEY, ac.APP_SECRET)\n",
    "  \n",
    "  authorize_url = auth_flow.start()\n",
    "  print \"1. Go to: \" + authorize_url\n",
    "  print \"2. Click \\\"Allow\\\" (you might have to log in first).\"\n",
    "  print \"3. Copy the authorization code.\"\n",
    "  auth_code = raw_input(\"Enter the authorization code here: \").strip()\n",
    "  \n",
    "  try:\n",
    "    oauth_result = auth_flow.finish(auth_code)\n",
    "  except Exception, e:\n",
    "    print ('Error: %s' % (e,))\n",
    "    return\n",
    "  \n",
    "  dbx = dropbox.Dropbox(oauth_result.access_token)\n",
    "  return dbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take the rows with multiple tweets checked and make an individual row for each tweets\n",
    "def expand_lists(df):\n",
    "  # create a list for each columns and a dict to later convert into an df\n",
    "  id_ = []\n",
    "  ratings = []\n",
    "  sources = []\n",
    "  tweets = {'id': id_, 'rating': ratings, 'source': sources}\n",
    "  \n",
    "  # loop thru each row and if tweet id is stored in a list then create df \n",
    "  # with each id in a separate row with its fact check data\n",
    "  for index, row in df.iterrows():\n",
    "    if (type(row[0]) == list):\n",
    "      for i in row[0]:\n",
    "        id_.append(i)\n",
    "        ratings.append(row[1])\n",
    "        sources.append(row[2])\n",
    "      # drop the row containing multiple tweets\n",
    "      df.drop(index, inplace=True)\n",
    "  # create new df with tweets in their own row, then append them to the original dataframe\n",
    "  new_df = pd.DataFrame(tweets)\n",
    "  df = df.append(new_df)\n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets â†“\n",
    "\n",
    "All the following functions write to excel or csv sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pull Func\n",
    "\n",
    "The following functin gets the most up to date tweets and writes them to the master excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_data(is_cand):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "   \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # Lessign has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False)\n",
    "  \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(100)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.to_excel(list_writer, sheet_name=sheetname, index=False)\n",
    "  tweet_writer.save()\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/Tweet_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/Presidential_Tweets.xlsx\n",
      "INFO:__main__:Downloading 1 tweets from BernieSanders\n",
      "INFO:__main__:Updated new tweets on spreadsheet for BernieSanders\n",
      "INFO:__main__:Downloading 1 tweets from BobbyJindal\n",
      "INFO:__main__:Updated new tweets on spreadsheet for BobbyJindal\n",
      "INFO:__main__:Downloading 0 tweets from CarlyFiorina\n",
      "INFO:__main__:Downloading 0 tweets from ChrisChristie\n",
      "INFO:__main__:Downloading 7 tweets from gov_gilmore\n",
      "INFO:__main__:Updated new tweets on spreadsheet for gov_gilmore\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1f677763f8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-90bb34d0ab42>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(is_cand)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Updated new tweets on spreadsheet for %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0;31m# write the updated list and save the changes to the excel sheets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "collect_data(True)\n",
    "collect_data(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued update of metadata\n",
    "\n",
    "A tweets ability to stay in the public discouse is dependent on the number of retweets and favorites. The initial pull of a tweet will not complete picture of the tweets effectiveness. This script allows us to continously update a tweet's metadata counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params: is_cand - determines whether to pull candidates tweets or PAC tweets\n",
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(is_cand):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():       \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # Lessig has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(path + tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    #tweets = get_new_tweets(name, 1)\n",
    "    #updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    #updates_df = updates_df[[0, 6, 7]]\n",
    "    #updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    #tweets_df = update_metadata(tweets_df, updates_df, name)\n",
    "    dupe_check(tweets_df, name)\n",
    "    \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    #tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    # 100 second pause between data pulls to voud token exceptions\n",
    "    #time.sleep(20)\n",
    "  \n",
    "  # tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "def update_metadata(tweets_df, updates_df, cand_name): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('text', inplace=True)\n",
    "  \n",
    "  # loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['text'], inplace=True)\n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dupe_check(tweets_df, cand_name):\n",
    "  # check for duplicates\n",
    "  dupe_df = tweets_df[tweets_df.id.duplicated()]\n",
    "  \n",
    "  # if there exists a duplicate save it to a csv\n",
    "  if (len(dupe_df) > 0):\n",
    "    # make sure that the dupe_folder dir exists\n",
    "    dupe_df.to_csv('./dupe_folder/' + cand_name + '.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/Tweet_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/Presidential_Tweets.xlsx\n",
      "INFO:__main__:Downloaded tweets list\n",
      "INFO:__main__:Retrived data from spreadsheet for BernieSanders\n",
      "INFO:__main__:Updated data on spreadsheet for BernieSanders\n",
      "INFO:__main__:Retrived data from spreadsheet for BobbyJindal\n",
      "INFO:__main__:Updated data on spreadsheet for BobbyJindal\n",
      "INFO:__main__:Retrived data from spreadsheet for CarlyFiorina\n",
      "INFO:__main__:Updated data on spreadsheet for CarlyFiorina\n",
      "INFO:__main__:Retrived data from spreadsheet for ChrisChristie\n",
      "INFO:__main__:Updated data on spreadsheet for ChrisChristie\n",
      "INFO:__main__:Retrived data from spreadsheet for gov_gilmore\n",
      "INFO:__main__:Updated data on spreadsheet for gov_gilmore\n",
      "INFO:__main__:Retrived data from spreadsheet for GovernorPataki\n",
      "INFO:__main__:Updated data on spreadsheet for GovernorPataki\n",
      "INFO:__main__:Retrived data from spreadsheet for GovernorPerry\n",
      "INFO:__main__:Updated data on spreadsheet for GovernorPerry\n",
      "INFO:__main__:Retrived data from spreadsheet for GovMikeHuckabee\n",
      "INFO:__main__:Updated data on spreadsheet for GovMikeHuckabee\n",
      "INFO:__main__:Retrived data from spreadsheet for HillaryClinton\n",
      "INFO:__main__:Updated data on spreadsheet for HillaryClinton\n",
      "INFO:__main__:Retrived data from spreadsheet for JebBush\n",
      "INFO:__main__:Updated data on spreadsheet for JebBush\n",
      "INFO:__main__:Retrived data from spreadsheet for JimWebbUSA\n",
      "INFO:__main__:Updated data on spreadsheet for JimWebbUSA\n",
      "INFO:__main__:Retrived data from spreadsheet for JohnKasich\n",
      "INFO:__main__:Updated data on spreadsheet for JohnKasich\n",
      "INFO:__main__:Retrived data from spreadsheet for LincolnChafee\n",
      "INFO:__main__:Updated data on spreadsheet for LincolnChafee\n",
      "INFO:__main__:Retrived data from spreadsheet for LindseyGrahamSC\n",
      "INFO:__main__:Updated data on spreadsheet for LindseyGrahamSC\n",
      "INFO:__main__:Retrived data from spreadsheet for marcorubio\n",
      "INFO:__main__:Updated data on spreadsheet for marcorubio\n",
      "INFO:__main__:Retrived data from spreadsheet for MartinOMalley\n",
      "INFO:__main__:Updated data on spreadsheet for MartinOMalley\n",
      "INFO:__main__:Retrived data from spreadsheet for POTUS\n",
      "INFO:__main__:Updated data on spreadsheet for POTUS\n",
      "INFO:__main__:Retrived data from spreadsheet for RandPaul\n",
      "INFO:__main__:Updated data on spreadsheet for RandPaul\n",
      "INFO:__main__:Retrived data from spreadsheet for RealBenCarson\n",
      "INFO:__main__:Updated data on spreadsheet for RealBenCarson\n",
      "INFO:__main__:Retrived data from spreadsheet for realDonaldTrump\n",
      "INFO:__main__:Updated data on spreadsheet for realDonaldTrump\n",
      "INFO:__main__:Retrived data from spreadsheet for RickSantorum\n",
      "INFO:__main__:Updated data on spreadsheet for RickSantorum\n",
      "INFO:__main__:Retrived data from spreadsheet for ScottWalker\n",
      "INFO:__main__:Updated data on spreadsheet for ScottWalker\n",
      "INFO:__main__:Retrived data from spreadsheet for tedcruz\n",
      "INFO:__main__:Updated data on spreadsheet for tedcruz\n",
      "INFO:__main__:Done collecting additional data\n",
      "INFO:__main__:Time Elapsed: 9\n"
     ]
    }
   ],
   "source": [
    "collect_addition_data(True)\n",
    "# collect_addition_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_full_url(short_urls, full_urls):\n",
    "for i, us in enumerate(short_urls):\n",
    "full = []\n",
    "  if not us.startswith(\"http\"):\n",
    "    continue\n",
    "  for url in us.split(\" \"):\n",
    "    if not url.startswith(\"http\"):\n",
    "      continue\n",
    "    try:\n",
    "      r = requests.head(url, allow_redirects=True)\n",
    "      full.append(r.url)\n",
    "    except:\n",
    "      logger.info(\"Error occurred for URL - %s\" % url)\n",
    "      continue\n",
    "  if i % 500 == 0:\n",
    "      logger.info(\"Extracting URL %d/%d\" % (i, len(short_urls)))\n",
    "      time.sleep(60)\n",
    "  full_urls[i] = \" \".join(full)q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_full_url(is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "    \n",
    "  logger.info(\"Successfully download the list...\")\n",
    "  for e, entry in enumerate(list_df):\n",
    "    if e < 15:\n",
    "      continue\n",
    "\n",
    "    name, since_id, count, index = entry[0], entry[1],entry[2], entry[3]\n",
    "\n",
    "    short_urls = worksheet.col_values(6)\n",
    "    logger.info(\"Downloaded %s URL\", name)\n",
    "    url_datas = ['' for i in xrange(len(short_urls))]\n",
    "    url_datas[0] = 'full URL'\n",
    "\n",
    "    get_full_url(short_urls, url_datas) # transfer short url to full urls and store in url_datas\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    while count < len(short_urls):\n",
    "      amount = min(100, len(short_urls) - count)\n",
    "      cells = worksheet.range('I'+str(count)+':'+'I'+str(count+amount-1))\n",
    "      assert(len(cells) == amount)\n",
    "      for i in range(amount):\n",
    "        cells[i].value = url_datas[count-1]\n",
    "        count += 1\n",
    "      worksheet.update_cells(cells)\n",
    "      logger.info(\"Update cells %d/%d for %s\" %(count, len(short_urls), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_full_url(True)\n",
    "#update_full_url(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaPo Fact Checking\n",
    "The cell below collects fact checks from the Washington Post's '2016 Election Fact Checker' and 'RealDonaldContext' chrome extension. The election fact checker data was hand collected and is stored in a json file while the extension data is pulled directly from the online hosted json file from the extension's developer blog.\n",
    "They are collected into an single dataframe consisting of the tweet id, rating, and source. They are then merged with a master sheet using tweet id.\n",
    "\n",
    "['2016 Election Fact Checker'](https://www.washingtonpost.com/graphics/politics/2016-election/fact-checker/)\n",
    "\n",
    "['RealDonaldContext'](https://chrome.google.com/webstore/detail/realdonaldcontext/ddbkmnomngnlcdglabflidgmhmcafogn?hl=en-US)\n",
    "\n",
    "['RealDonaldContext json file'](https://www.pbump.net/files/post/extension/core/data.php)\n",
    "\n",
    "['Rating System Scale'](https://www.washingtonpost.com/news/fact-checker/about-the-fact-checker/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code is from the fact checking portion of this project. It grabs the fact checked tweets from\n",
    "# the WaPo Trump tweet fact checking extension and adds the ratings to correspoding tweets in the spreadsheet\n",
    "\n",
    "# sheetnames\n",
    "trump_sheet = 'realDonaldTrump'\n",
    "potus_sheet = 'POTUS'\n",
    "\n",
    "logger.info(\"Start...\")\n",
    "\n",
    "# read in WaPo fact checks of Donald Trump from the WaPo Trump tweet chrome extension\n",
    "trump_check = pd.read_json('https://www.pbump.net/files/post/extension/core/data.php')\n",
    "# rename columns and remove text columns\n",
    "trump_check.columns = ['id', 'rating', 'tweet', 'source']\n",
    "trump_check = trump_check[['id', 'rating', 'source']]\n",
    "# call expand lists to turn fact checks of multiple tweets into multiple columns\n",
    "trump_check = expand_lists(trump_check)\n",
    "\n",
    "# load pre-election fact checks and filter for just id, rating, and source\n",
    "election_checks = pd.read_json('preelection_wapo.json')\n",
    "election_checks = election_checks[['id', 'rating', 'source']]\n",
    "\n",
    "# append the hand collected data with the data collected from the extension\n",
    "trump_check = trump_check.append(election_checks, ignore_index=True)\n",
    "trump_check.columns = ['id', 'WAPO_RATING', 'WAPO_SOURCE']\n",
    "logger.info(\"read in fact checks\")\n",
    "\n",
    "# set file pathway variables an expand to HOME\n",
    "in_path = '~/Dropbox/Summer_of_Tweets/fact_checking/Presidential_Fact_Checking.xlsx'\n",
    "in_path = os.path.expanduser(in_path)\n",
    "\n",
    "# properly load spreadsheet to append new data\n",
    "work_book = load_workbook(in_path)\n",
    "tweet_writer = pd.ExcelWriter(in_path, engine='openpyxl')\n",
    "tweet_writer.book = work_book\n",
    "tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)\n",
    "tweets_df = pd.read_excel(in_path, sheetname=trump_sheet, dtype={'id': str})\n",
    "logger.info(\"Downloaded excel sheets list\")\n",
    "\n",
    "# change data type to match excel sheet's\n",
    "trump_check['id'] = trump_check['id'].astype(str)\n",
    "#merge the fact check data set with the tweets set using tweet id\n",
    "merged_df = tweets_df.merge(trump_check, on='id', how='left')\n",
    "\n",
    "logger.info(merged_df.shape) # used for debugging\n",
    "# write merged data to the excel sheet\n",
    "merged_df.to_excel(tweet_writer, sheet_name=trump_sheet, index=False)\n",
    "tweet_writer.save()\n",
    "\n",
    "# merged_df.to_csv('WaPo.csv', encoding='utf-8') # used for viewing test results\n",
    "\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follower Growth\n",
    "The follower growth for Hillary Clinton and Donald Trump is being collected for the project. After some research on which sites are best for follower growth data, [Trackalytics](http://www.trackalytics.com) is the best free resource for tracking follower growth. However it does not have comprehensive follower growth data for the rest of the candidates, the others either are not present on the site or their data starts to get collected well into the election cycle.\n",
    "\n",
    "The data is scraped using the IMPORTHTML function in google sheets. Information on the function and how to use it can be found [here](http://lenagroeger.s3.amazonaws.com/talks/orlando/gettingdata.html)  while the sheet itself can be found [here](https://docs.google.com/spreadsheets/d/1rahomcsDJFf_za0S_Tbzi1kv79bdNM2ZqNZ_H7XcMIM/edit?usp=sharing). \n",
    "\n",
    "The following function runs to clean the data sheet to move daily delta in followers into its own column and then downloading and moving the sheet onto the FISP dropbox.\n",
    "\n",
    "Implemented using the df2gspread module, documentation for the module can be found [here](https://github.com/maybelinot/df2gspread)\n",
    "\n",
    "[Trump Follower Tracker](http://www.trackalytics.com/twitter/profile/RealDonaldTrump/)\n",
    "\n",
    "[Clinton Follower Tracker](http://www.trackalytics.com/twitter/profile/HillaryClinton/)\n",
    "\n",
    "*The Site is missing data for July 3-5th 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the path to the follower growth sheeet in my drive\n",
    "sheet_path = \"/fisp_twitter/sheets/follower_growth\"\n",
    "\n",
    "# currently it contains Clinton and Trump follower growth\n",
    "clinton_sheet = \"HillaryClinton\"\n",
    "trump_sheet = \"realDonaldTrump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# passed into the apply func to remove daily change in metadata for twitter accounts\n",
    "def split_func (string):\n",
    "  return string.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import module for downloading the sheets from the drive \n",
    "from df2gspread import gspread2df as g2d\n",
    "\n",
    "def gen_cand_followers (file_path, sheet_name):\n",
    "  # download the follower growth sheet from gdrive\n",
    "  df = g2d.download(file_path, sheet_name, col_names = True)\n",
    "  \n",
    "  # take the top row and convert it to the column header\n",
    "  df.columns = df.iloc[0]\n",
    "  df = df.reindex(df.index.drop(0))\n",
    "\n",
    "  # trim out the change data that is appended at the end ot the daily value for the metadata\n",
    "  df['Followers'] = df['Followers - (change)'].apply(cut_change)\n",
    "  df['Following'] = df['Following - (change)'].apply(cut_change)\n",
    "  df['Tweets'] = df['Tweets - (change)'].apply(cut_change)\n",
    "  \n",
    "  # drop the change rows\n",
    "  df = df.drop(['Followers - (change)', 'Following - (change)', 'Tweets - (change)'], 1)\n",
    "  \n",
    "  # convert date to MM/DD/YYYY format and rename the column\n",
    "  df['Date'] = pd.to_datetime(df.Date)\n",
    "  df['Date'] = df['Date'].dt.strftime('%m/%d/%Y')\n",
    "  df = df.rename(index=str, columns={'Date': 'date'})\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton_follower_df = gen_cand_followers (sheet_path, clinton_sheet)\n",
    "trump_follower_df = gen_cand_followers (sheet_path, trump_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append row with each candidate's last name\n",
    "clinton_follower_df['lastname'] = 'Clinton'\n",
    "trump_follower_df['lastname'] = 'Trump'\n",
    "gen_cand_follower_df = clinton_follower_df.append(trump_follower_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save csv's path and load it into a panda's dataframe\n",
    "path = '~/Dropbox/Summer_of_Tweets/Deduped_Tweets/polling_merged.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# duplicated the tweet created at columns to made a modify it to a MM/DD/YY format\n",
    "df['date'] = df['created_at']\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['date'] = df['date'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the full data set with the general cadidate's follower growth using the date and lastname fields\n",
    "merged_df = df.merge(gen_cand_follower_df.drop(['Following', 'Tweets'], 1), on=['date', 'lastname'], how='inner')\n",
    "follower_df = merged_df.to_csv('followers_polling_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polling Data Match\n",
    "\n",
    "This set of code will take the polling data from 538 and match them to their corresponding day's tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "prim_df = pd.read_csv('national_primary_poll_average_2016.csv')\n",
    "# drop unncessary columns\n",
    "prim_df = prim_df[['lastname', 'poll_avg', 'forecastdate']]\n",
    "\n",
    "# reformat dates into a mm/dd/YY format and then rename columns for consistency\n",
    "prim_df['forecastdate'] = pd.to_datetime(prim_df.forecastdate)\n",
    "prim_df['forecastdate'] = prim_df['forecastdate'].dt.strftime('%m/%d/%Y')\n",
    "prim_df.columns = ['lastname', 'forecast', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sanders', 'Clinton', 'Trump', 'Kasich', 'Cruz', 'Rubio', 'Carson',\n",
       "       'Bush', 'Fiorina', 'Christie', 'Santorum', 'Paul', \"O'Malley\",\n",
       "       'Huckabee', 'Pataki', 'Graham', 'Jindal', 'Lessig', 'Chafee',\n",
       "       'Webb', 'Walker', 'Perry'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_df.lastname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in the nat data and filter out for polls-only data\n",
    "nat_df = pd.read_csv('national_topline.csv')\n",
    "nat_df = nat_df[nat_df.type == 'polls-only']\n",
    "\n",
    "# reformat dates into a mm/dd/YY format\n",
    "nat_df['forecastdate'] = pd.to_datetime(nat_df.forecastdate)\n",
    "nat_df['forecastdate'] = nat_df['forecastdate'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "# remove all but date and prediction score for trump and clinton\n",
    "nat_df = nat_df[['forecastdate', 'ecwin_clinton', 'ecwin_trump']]\n",
    "nat_df.head()\n",
    "\n",
    "# create separate dataframes for each candidate to make it easier to manipulate and combine with primary data\n",
    "clinton_df = nat_df[['forecastdate', 'ecwin_clinton']]\n",
    "trump_df = nat_df[['forecastdate', 'ecwin_trump']]\n",
    "\n",
    "# add a corresponding column for lastname to match primary prediction data format\n",
    "clinton_df['lastname'] = 'Clinton'\n",
    "trump_df['lastname'] = 'Trump'\n",
    "\n",
    "# rename and rearrange columns for consistency with primary data\n",
    "clinton_df.columns = ['date', 'forecast', 'lastname']\n",
    "trump_df.columns = ['date', 'forecast', 'lastname']\n",
    "clinton_df = clinton_df[['lastname', 'forecast', 'date']]\n",
    "trump_df = trump_df[['lastname', 'forecast', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append general election forecast data with primary election forecast data\n",
    "forecast_df = prim_df.append(clinton_df, ignore_index=True)\n",
    "forecast_df = forecast_df.append(trump_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save csv's path and load it into a panda's dataframe\n",
    "path = '~/Dropbox/Summer_of_Tweets/Deduped_Tweets/deduped_tweets.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# duplicated the tweet created at columns to made a modify it to a MM/DD/YY format\n",
    "df['date'] = df['created_at']\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['date'] = df['date'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the cand names from the original sheet and make a parallel array of each cand's last name to match polling data\n",
    "cand = df.Candidate.unique()\n",
    "cand_lastname = ['Carson', 'Sanders', 'Jindal', 'Jindal', 'Fiorina', 'Fiorina', 'Christie', 'Christie', 'Sanders',\n",
    "                 'Pataki', 'Perry', 'Gilmore', 'Huckabee', 'Trump', 'Clinton', 'Clinton', 'Bush', 'Bush', 'Webb',\n",
    "                 'Kasich', 'Kasich', 'Lessig', 'Chaffee', 'Graham', 'Graham', 'Rubio', 'Rubio', \"O'Malley\", \"O'Malley\", \n",
    "                 'Huckabee', 'Sanders', 'Paul', 'Paul', 'Carson', 'Trump', 'Trump', 'Perry',\n",
    "                 'Santorum', 'Santorum', 'Walker', 'Walker', 'Cruz', 'Cruz']\n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "cand_to_lastname = {}\n",
    "for (cand, lastname) in zip(cand, cand_lastname):\n",
    "    cand_to_lastname[cand] = lastname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a parallel list of each cand's last name to be appeneded to the original dataframe\n",
    "lastname_col = []\n",
    "for cand in df.Candidate:\n",
    "  lastname_col.append(cand_to_lastname[cand])\n",
    "  \n",
    "df['lastname'] = lastname_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df = df.merge(forecast_df, on=['date', 'lastname'])\n",
    "polling_merged = merged_df.to_csv('polling_merged_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99032, 48)\n"
     ]
    }
   ],
   "source": [
    "print merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp function to properly sort the tweets by date in ascending order\n",
    "def sort_sheet (is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "      \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "      \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "        \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_book = load_workbook(path + tweet_list)\n",
    "  list_writer = pd.ExcelWriter(path + tweet_list, engine='openpyxl')\n",
    "  list_writer.book = list_book\n",
    "  list_writer.sheets = dict((ws.title, ws) for ws in list_book.worksheets)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  work_book = load_workbook(path + tweet_sheet)\n",
    "  tweet_writer = pd.ExcelWriter(path + tweet_sheet, engine='openpyxl')\n",
    "  tweet_writer.book = work_book\n",
    "  tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)    \n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "       \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    tweets_df = pd.read_excel(path + tweet_sheet, sheetname=name)\n",
    "    \n",
    "    tweets_df = tweets_df.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"BernieSanders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hashtag#</th>\n",
       "      <th>at@</th>\n",
       "      <th>link</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>full URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For the last 40 years the great middle class o...</td>\n",
       "      <td>753762514950000000.000</td>\n",
       "      <td>2016-07-15 01:26:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>838.000</td>\n",
       "      <td>2544.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is it that kids are jailed for possessing ...</td>\n",
       "      <td>752238973947000064.000</td>\n",
       "      <td>2016-07-10 20:32:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5951.000</td>\n",
       "      <td>12486.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If we are serious about criminal justice refor...</td>\n",
       "      <td>743899023748999936.000</td>\n",
       "      <td>2016-06-17 20:12:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3169.000</td>\n",
       "      <td>7533.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We must always remember that change almost nev...</td>\n",
       "      <td>740558323847000064.000</td>\n",
       "      <td>2016-06-08 14:57:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6601.000</td>\n",
       "      <td>12930.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We must not allow our children to be poisoned ...</td>\n",
       "      <td>737273130495000064.000</td>\n",
       "      <td>2016-05-30 13:23:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2324.000</td>\n",
       "      <td>6000.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump and his friends should realize th...</td>\n",
       "      <td>735612438628999936.000</td>\n",
       "      <td>2016-05-25 23:24:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2847.000</td>\n",
       "      <td>6455.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It is not good enough to talk about public edu...</td>\n",
       "      <td>734502616248000000.000</td>\n",
       "      <td>2016-05-22 21:54:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2027.000</td>\n",
       "      <td>5745.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oregon  turn in your ballot for Bernie at the ...</td>\n",
       "      <td>731272583770000000.000</td>\n",
       "      <td>2016-05-13 23:59:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/yZxxEfwxxD</td>\n",
       "      <td>1532.000</td>\n",
       "      <td>2928.000</td>\n",
       "      <td>https://berniesanders.com/oregon-bernie-drop-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Corporate greed is a scourge on this country, ...</td>\n",
       "      <td>726490087676999936.000</td>\n",
       "      <td>2016-04-30 19:15:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1557.000</td>\n",
       "      <td>3430.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This election is not just about electing a pre...</td>\n",
       "      <td>725494988336000000.000</td>\n",
       "      <td>2016-04-28 01:20:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3883.000</td>\n",
       "      <td>8307.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>In many states, it is legal to fire someone fo...</td>\n",
       "      <td>725359612514000000.000</td>\n",
       "      <td>2016-04-27 16:23:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5535.000</td>\n",
       "      <td>9886.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>It's immoral and unjust to leave tens of milli...</td>\n",
       "      <td>724992702300000000.000</td>\n",
       "      <td>2016-04-26 16:05:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1327.000</td>\n",
       "      <td>3480.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I'm motivated by a vision which exists in all ...</td>\n",
       "      <td>724964069930000000.000</td>\n",
       "      <td>2016-04-26 14:11:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/RVZoN9GzcJ</td>\n",
       "      <td>2901.000</td>\n",
       "      <td>4682.000</td>\n",
       "      <td>https://amp.twimg.com/v/2f5b7c34-8145-4627-a04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I dont believe it is a radical idea to say tha...</td>\n",
       "      <td>724781814896999936.000</td>\n",
       "      <td>2016-04-26 02:07:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3542.000</td>\n",
       "      <td>7850.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Corporate greed is a scourge on this country, ...</td>\n",
       "      <td>724009014564999936.000</td>\n",
       "      <td>2016-04-23 22:56:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1325.000</td>\n",
       "      <td>3093.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Public college and university education in Ame...</td>\n",
       "      <td>723899989358000000.000</td>\n",
       "      <td>2016-04-23 15:43:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2898.000</td>\n",
       "      <td>6401.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I want to see our country have one of the high...</td>\n",
       "      <td>722945216203000064.000</td>\n",
       "      <td>2016-04-21 00:29:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965.000</td>\n",
       "      <td>6018.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>It is extremely sad that the United States, on...</td>\n",
       "      <td>721701996546000000.000</td>\n",
       "      <td>2016-04-17 14:08:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2780.000</td>\n",
       "      <td>7508.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>People should be rewarded for getting an educa...</td>\n",
       "      <td>719350309890000000.000</td>\n",
       "      <td>2016-04-11 02:24:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5690.000</td>\n",
       "      <td>11936.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>We need to federally fund and require body cam...</td>\n",
       "      <td>717089129415000064.000</td>\n",
       "      <td>2016-04-04 20:39:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1732.000</td>\n",
       "      <td>5043.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I am proud to have introduced the most compreh...</td>\n",
       "      <td>716707437256999936.000</td>\n",
       "      <td>2016-04-03 19:22:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3183.000</td>\n",
       "      <td>11445.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Change cannot take place without political par...</td>\n",
       "      <td>716272508618000000.000</td>\n",
       "      <td>2016-04-02 14:34:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>It is wrong that women are paid less than men ...</td>\n",
       "      <td>715637800876000000.000</td>\n",
       "      <td>2016-03-31 20:32:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>One in five people can't afford the prescripti...</td>\n",
       "      <td>715322238779000064.000</td>\n",
       "      <td>2016-03-30 23:38:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2923.000</td>\n",
       "      <td>6993.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Our movement is about creating an America that...</td>\n",
       "      <td>715187593928000000.000</td>\n",
       "      <td>2016-03-30 14:43:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>We need a grassroots movement to tell the Koch...</td>\n",
       "      <td>714881577311000064.000</td>\n",
       "      <td>2016-03-29 18:27:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A job must lift workers out of poverty, not ke...</td>\n",
       "      <td>714813020594000000.000</td>\n",
       "      <td>2016-03-29 13:54:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3270.000</td>\n",
       "      <td>7895.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>We need police reform so that young people can...</td>\n",
       "      <td>714629660772999936.000</td>\n",
       "      <td>2016-03-29 01:46:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5112.000</td>\n",
       "      <td>11166.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>We need to ensure access to quality affordable...</td>\n",
       "      <td>714442180924999936.000</td>\n",
       "      <td>2016-03-28 13:21:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1452.000</td>\n",
       "      <td>4251.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>We need to federally fund and require body cam...</td>\n",
       "      <td>713554088538000000.000</td>\n",
       "      <td>2016-03-26 02:32:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1860.000</td>\n",
       "      <td>6051.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Join Bernie for town meetings in #SanFrancisco...</td>\n",
       "      <td>522041818004000000.000</td>\n",
       "      <td>2014-10-14 15:10:43</td>\n",
       "      <td>#SanFrancisco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/nVkfxG8LQ9</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.facebook.com/friendsofbernie/events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Join Bernie for town meetings in #SanFrancisco...</td>\n",
       "      <td>521754871956000000.000</td>\n",
       "      <td>2014-10-13 20:10:29</td>\n",
       "      <td>#SanFrancisco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/nVkfxG8LQ9</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.facebook.com/friendsofbernie/events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Join Bernie for town meetings in #SanFrancisco...</td>\n",
       "      <td>521301822322000000.000</td>\n",
       "      <td>2014-10-12 14:10:14</td>\n",
       "      <td>#SanFrancisco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/nVkfxG8LQ9</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.facebook.com/friendsofbernie/events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Sign Up for a Strategy Call with Sen. Bernie S...</td>\n",
       "      <td>486594361900000000.000</td>\n",
       "      <td>2014-07-08 19:35:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@MoveOn</td>\n",
       "      <td>http://t.co/9GH7CUFSSu</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>http://moveon.org/pac/kochbros/Sen_Sanders_sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>The obscene level of wealth and income inequal...</td>\n",
       "      <td>447071486852000000.000</td>\n",
       "      <td>2014-03-21 18:05:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Bernie Lawn Signs Are In! Call 802-861-2738 to...</td>\n",
       "      <td>255363991881000000.000</td>\n",
       "      <td>2012-10-08 17:48:14</td>\n",
       "      <td>#Vt #Vermont #VTpoli #Bernie2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>In 2010, the richest 1% of Americans received ...</td>\n",
       "      <td>233554717677000000.000</td>\n",
       "      <td>2012-08-09 13:25:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Join Sen. Sanders this weekend for town meetin...</td>\n",
       "      <td>215167191745000000.000</td>\n",
       "      <td>2012-06-19 19:40:30</td>\n",
       "      <td>#Vermont #Vt #Rutland #Bennington</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://t.co/GDrKYRCN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://berniesanders.com/whatsatstake/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>The US has the most unequal distribution of we...</td>\n",
       "      <td>198128736225000000.000</td>\n",
       "      <td>2012-05-03 19:15:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Between 1980 and 2005, 80% of all new income c...</td>\n",
       "      <td>185070165561000000.000</td>\n",
       "      <td>2012-03-28 18:25:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>The US has the most unequal distribution of we...</td>\n",
       "      <td>185019746495000000.000</td>\n",
       "      <td>2012-03-28 15:05:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>There is something profoundly wrong when one f...</td>\n",
       "      <td>810222537695000064.000</td>\n",
       "      <td>2016-12-17 23:17:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4245.000</td>\n",
       "      <td>12494.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>We are living in a nation which worships wealt...</td>\n",
       "      <td>815665657166000000.000</td>\n",
       "      <td>2017-01-01 22:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6157.000</td>\n",
       "      <td>20476.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>There is something profoundly wrong when one f...</td>\n",
       "      <td>817909943580000000.000</td>\n",
       "      <td>2017-01-08 02:55:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3949.000</td>\n",
       "      <td>14152.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>The struggle for our rights is not a one-time ...</td>\n",
       "      <td>833806440640999936.000</td>\n",
       "      <td>2017-02-21 00:15:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4280.000</td>\n",
       "      <td>14594.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>We will not allow Republicans to punish the el...</td>\n",
       "      <td>834099121400999936.000</td>\n",
       "      <td>2017-02-21 20:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6209.000</td>\n",
       "      <td>23817.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>We are living in a nation which worships wealt...</td>\n",
       "      <td>843098666156000000.000</td>\n",
       "      <td>2017-03-18 15:45:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13430.000</td>\n",
       "      <td>40379.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>We must never forget. Health care is a human r...</td>\n",
       "      <td>844224083684000000.000</td>\n",
       "      <td>2017-03-21 18:29:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7819.000</td>\n",
       "      <td>26397.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>RT @Residente: En Washington DC junto a @berni...</td>\n",
       "      <td>905496790296000000.000</td>\n",
       "      <td>2017-09-06 18:23:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>296.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text                     id  \\\n",
       "0    For the last 40 years the great middle class o... 753762514950000000.000   \n",
       "1    Why is it that kids are jailed for possessing ... 752238973947000064.000   \n",
       "2    If we are serious about criminal justice refor... 743899023748999936.000   \n",
       "3    We must always remember that change almost nev... 740558323847000064.000   \n",
       "4    We must not allow our children to be poisoned ... 737273130495000064.000   \n",
       "5    Donald Trump and his friends should realize th... 735612438628999936.000   \n",
       "6    It is not good enough to talk about public edu... 734502616248000000.000   \n",
       "7    Oregon  turn in your ballot for Bernie at the ... 731272583770000000.000   \n",
       "8    Corporate greed is a scourge on this country, ... 726490087676999936.000   \n",
       "9    This election is not just about electing a pre... 725494988336000000.000   \n",
       "10   In many states, it is legal to fire someone fo... 725359612514000000.000   \n",
       "11   It's immoral and unjust to leave tens of milli... 724992702300000000.000   \n",
       "12   I'm motivated by a vision which exists in all ... 724964069930000000.000   \n",
       "13   I dont believe it is a radical idea to say tha... 724781814896999936.000   \n",
       "14   Corporate greed is a scourge on this country, ... 724009014564999936.000   \n",
       "15   Public college and university education in Ame... 723899989358000000.000   \n",
       "16   I want to see our country have one of the high... 722945216203000064.000   \n",
       "17   It is extremely sad that the United States, on... 721701996546000000.000   \n",
       "18   People should be rewarded for getting an educa... 719350309890000000.000   \n",
       "19   We need to federally fund and require body cam... 717089129415000064.000   \n",
       "20   I am proud to have introduced the most compreh... 716707437256999936.000   \n",
       "21   Change cannot take place without political par... 716272508618000000.000   \n",
       "22   It is wrong that women are paid less than men ... 715637800876000000.000   \n",
       "23   One in five people can't afford the prescripti... 715322238779000064.000   \n",
       "24   Our movement is about creating an America that... 715187593928000000.000   \n",
       "25   We need a grassroots movement to tell the Koch... 714881577311000064.000   \n",
       "26   A job must lift workers out of poverty, not ke... 714813020594000000.000   \n",
       "27   We need police reform so that young people can... 714629660772999936.000   \n",
       "28   We need to ensure access to quality affordable... 714442180924999936.000   \n",
       "29   We need to federally fund and require body cam... 713554088538000000.000   \n",
       "..                                                 ...                    ...   \n",
       "298  Join Bernie for town meetings in #SanFrancisco... 522041818004000000.000   \n",
       "299  Join Bernie for town meetings in #SanFrancisco... 521754871956000000.000   \n",
       "300  Join Bernie for town meetings in #SanFrancisco... 521301822322000000.000   \n",
       "301  Sign Up for a Strategy Call with Sen. Bernie S... 486594361900000000.000   \n",
       "302  The obscene level of wealth and income inequal... 447071486852000000.000   \n",
       "303  Bernie Lawn Signs Are In! Call 802-861-2738 to... 255363991881000000.000   \n",
       "304  In 2010, the richest 1% of Americans received ... 233554717677000000.000   \n",
       "305  Join Sen. Sanders this weekend for town meetin... 215167191745000000.000   \n",
       "306  The US has the most unequal distribution of we... 198128736225000000.000   \n",
       "307  Between 1980 and 2005, 80% of all new income c... 185070165561000000.000   \n",
       "308  The US has the most unequal distribution of we... 185019746495000000.000   \n",
       "309  There is something profoundly wrong when one f... 810222537695000064.000   \n",
       "310  We are living in a nation which worships wealt... 815665657166000000.000   \n",
       "311  There is something profoundly wrong when one f... 817909943580000000.000   \n",
       "312  The struggle for our rights is not a one-time ... 833806440640999936.000   \n",
       "313  We will not allow Republicans to punish the el... 834099121400999936.000   \n",
       "314  We are living in a nation which worships wealt... 843098666156000000.000   \n",
       "315  We must never forget. Health care is a human r... 844224083684000000.000   \n",
       "316  RT @Residente: En Washington DC junto a @berni... 905496790296000000.000   \n",
       "317                                                NaN                    nan   \n",
       "318                                                NaN                    nan   \n",
       "319                                                NaN                    nan   \n",
       "320                                                NaN                    nan   \n",
       "321                                                NaN                    nan   \n",
       "322                                                NaN                    nan   \n",
       "323                                                NaN                    nan   \n",
       "324                                                NaN                    nan   \n",
       "325                                                NaN                    nan   \n",
       "326                                                NaN                    nan   \n",
       "327                                                NaN                    nan   \n",
       "\n",
       "              created_at                           hashtag#      at@  \\\n",
       "0    2016-07-15 01:26:02                                NaN      NaN   \n",
       "1    2016-07-10 20:32:01                                NaN      NaN   \n",
       "2    2016-06-17 20:12:02                                NaN      NaN   \n",
       "3    2016-06-08 14:57:17                                NaN      NaN   \n",
       "4    2016-05-30 13:23:06                                NaN      NaN   \n",
       "5    2016-05-25 23:24:07                                NaN      NaN   \n",
       "6    2016-05-22 21:54:04                                NaN      NaN   \n",
       "7    2016-05-13 23:59:05                                NaN      NaN   \n",
       "8    2016-04-30 19:15:09                                NaN      NaN   \n",
       "9    2016-04-28 01:20:58                                NaN      NaN   \n",
       "10   2016-04-27 16:23:02                                NaN      NaN   \n",
       "11   2016-04-26 16:05:04                                NaN      NaN   \n",
       "12   2016-04-26 14:11:18                                NaN      NaN   \n",
       "13   2016-04-26 02:07:05                                NaN      NaN   \n",
       "14   2016-04-23 22:56:15                                NaN      NaN   \n",
       "15   2016-04-23 15:43:01                                NaN      NaN   \n",
       "16   2016-04-21 00:29:05                                NaN      NaN   \n",
       "17   2016-04-17 14:08:59                                NaN      NaN   \n",
       "18   2016-04-11 02:24:13                                NaN      NaN   \n",
       "19   2016-04-04 20:39:05                                NaN      NaN   \n",
       "20   2016-04-03 19:22:23                                NaN      NaN   \n",
       "21   2016-04-02 14:34:08                                NaN      NaN   \n",
       "22   2016-03-31 20:32:02                                NaN      NaN   \n",
       "23   2016-03-30 23:38:06                                NaN      NaN   \n",
       "24   2016-03-30 14:43:04                                NaN      NaN   \n",
       "25   2016-03-29 18:27:04                                NaN      NaN   \n",
       "26   2016-03-29 13:54:39                                NaN      NaN   \n",
       "27   2016-03-29 01:46:02                                NaN      NaN   \n",
       "28   2016-03-28 13:21:04                                NaN      NaN   \n",
       "29   2016-03-26 02:32:06                                NaN      NaN   \n",
       "..                   ...                                ...      ...   \n",
       "298  2014-10-14 15:10:43                      #SanFrancisco      NaN   \n",
       "299  2014-10-13 20:10:29                      #SanFrancisco      NaN   \n",
       "300  2014-10-12 14:10:14                      #SanFrancisco      NaN   \n",
       "301  2014-07-08 19:35:11                                NaN  @MoveOn   \n",
       "302  2014-03-21 18:05:23                                NaN      NaN   \n",
       "303  2012-10-08 17:48:14   #Vt #Vermont #VTpoli #Bernie2012      NaN   \n",
       "304  2012-08-09 13:25:58                                NaN      NaN   \n",
       "305  2012-06-19 19:40:30  #Vermont #Vt #Rutland #Bennington      NaN   \n",
       "306  2012-05-03 19:15:46                                NaN      NaN   \n",
       "307  2012-03-28 18:25:40                                NaN      NaN   \n",
       "308  2012-03-28 15:05:19                                NaN      NaN   \n",
       "309  2016-12-17 23:17:00                                NaN      NaN   \n",
       "310  2017-01-01 22:59:00                                NaN      NaN   \n",
       "311  2017-01-08 02:55:01                                NaN      NaN   \n",
       "312  2017-02-21 00:15:01                                NaN      NaN   \n",
       "313  2017-02-21 20:15:00                                NaN      NaN   \n",
       "314  2017-03-18 15:45:01                                NaN      NaN   \n",
       "315  2017-03-21 18:29:01                                NaN      NaN   \n",
       "316  2017-09-06 18:23:52                                NaN      NaN   \n",
       "317                  NaN                                NaN      NaN   \n",
       "318                  NaN                                NaN      NaN   \n",
       "319                  NaN                                NaN      NaN   \n",
       "320                  NaN                                NaN      NaN   \n",
       "321                  NaN                                NaN      NaN   \n",
       "322                  NaN                                NaN      NaN   \n",
       "323                  NaN                                NaN      NaN   \n",
       "324                  NaN                                NaN      NaN   \n",
       "325                  NaN                                NaN      NaN   \n",
       "326                  NaN                                NaN      NaN   \n",
       "327                  NaN                                NaN      NaN   \n",
       "\n",
       "                        link  retweets  favorites  \\\n",
       "0                        NaN   838.000   2544.000   \n",
       "1                        NaN  5951.000  12486.000   \n",
       "2                        NaN  3169.000   7533.000   \n",
       "3                        NaN  6601.000  12930.000   \n",
       "4                        NaN  2324.000   6000.000   \n",
       "5                        NaN  2847.000   6455.000   \n",
       "6                        NaN  2027.000   5745.000   \n",
       "7    https://t.co/yZxxEfwxxD  1532.000   2928.000   \n",
       "8                        NaN  1557.000   3430.000   \n",
       "9                        NaN  3883.000   8307.000   \n",
       "10                       NaN  5535.000   9886.000   \n",
       "11                       NaN  1327.000   3480.000   \n",
       "12   https://t.co/RVZoN9GzcJ  2901.000   4682.000   \n",
       "13                       NaN  3542.000   7850.000   \n",
       "14                       NaN  1325.000   3093.000   \n",
       "15                       NaN  2898.000   6401.000   \n",
       "16                       NaN  1965.000   6018.000   \n",
       "17                       NaN  2780.000   7508.000   \n",
       "18                       NaN  5690.000  11936.000   \n",
       "19                       NaN  1732.000   5043.000   \n",
       "20                       NaN  3183.000  11445.000   \n",
       "21                       NaN       nan        nan   \n",
       "22                       NaN       nan        nan   \n",
       "23                       NaN  2923.000   6993.000   \n",
       "24                       NaN       nan        nan   \n",
       "25                       NaN       nan        nan   \n",
       "26                       NaN  3270.000   7895.000   \n",
       "27                       NaN  5112.000  11166.000   \n",
       "28                       NaN  1452.000   4251.000   \n",
       "29                       NaN  1860.000   6051.000   \n",
       "..                       ...       ...        ...   \n",
       "298  https://t.co/nVkfxG8LQ9       nan        nan   \n",
       "299  https://t.co/nVkfxG8LQ9       nan        nan   \n",
       "300  https://t.co/nVkfxG8LQ9       nan        nan   \n",
       "301   http://t.co/9GH7CUFSSu       nan        nan   \n",
       "302                      NaN       nan        nan   \n",
       "303                      NaN       nan        nan   \n",
       "304                      NaN       nan        nan   \n",
       "305     http://t.co/GDrKYRCN       nan        nan   \n",
       "306                      NaN       nan        nan   \n",
       "307                      NaN       nan        nan   \n",
       "308                      NaN       nan        nan   \n",
       "309                      NaN  4245.000  12494.000   \n",
       "310                      NaN  6157.000  20476.000   \n",
       "311                      NaN  3949.000  14152.000   \n",
       "312                      NaN  4280.000  14594.000   \n",
       "313                      NaN  6209.000  23817.000   \n",
       "314                      NaN 13430.000  40379.000   \n",
       "315                      NaN  7819.000  26397.000   \n",
       "316                      NaN   296.000      0.000   \n",
       "317                      NaN       nan        nan   \n",
       "318                      NaN       nan        nan   \n",
       "319                      NaN       nan        nan   \n",
       "320                      NaN       nan        nan   \n",
       "321                      NaN       nan        nan   \n",
       "322                      NaN       nan        nan   \n",
       "323                      NaN       nan        nan   \n",
       "324                      NaN       nan        nan   \n",
       "325                      NaN       nan        nan   \n",
       "326                      NaN       nan        nan   \n",
       "327                      NaN       nan        nan   \n",
       "\n",
       "                                              full URL  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "5                                                  NaN  \n",
       "6                                                  NaN  \n",
       "7    https://berniesanders.com/oregon-bernie-drop-o...  \n",
       "8                                                  NaN  \n",
       "9                                                  NaN  \n",
       "10                                                 NaN  \n",
       "11                                                 NaN  \n",
       "12   https://amp.twimg.com/v/2f5b7c34-8145-4627-a04...  \n",
       "13                                                 NaN  \n",
       "14                                                 NaN  \n",
       "15                                                 NaN  \n",
       "16                                                 NaN  \n",
       "17                                                 NaN  \n",
       "18                                                 NaN  \n",
       "19                                                 NaN  \n",
       "20                                                 NaN  \n",
       "21                                                 NaN  \n",
       "22                                                 NaN  \n",
       "23                                                 NaN  \n",
       "24                                                 NaN  \n",
       "25                                                 NaN  \n",
       "26                                                 NaN  \n",
       "27                                                 NaN  \n",
       "28                                                 NaN  \n",
       "29                                                 NaN  \n",
       "..                                                 ...  \n",
       "298    https://www.facebook.com/friendsofbernie/events  \n",
       "299    https://www.facebook.com/friendsofbernie/events  \n",
       "300    https://www.facebook.com/friendsofbernie/events  \n",
       "301  http://moveon.org/pac/kochbros/Sen_Sanders_sig...  \n",
       "302                                                NaN  \n",
       "303                                                NaN  \n",
       "304                                                NaN  \n",
       "305            https://berniesanders.com/whatsatstake/  \n",
       "306                                                NaN  \n",
       "307                                                NaN  \n",
       "308                                                NaN  \n",
       "309                                                NaN  \n",
       "310                                                NaN  \n",
       "311                                                NaN  \n",
       "312                                                NaN  \n",
       "313                                                NaN  \n",
       "314                                                NaN  \n",
       "315                                                NaN  \n",
       "316                                                NaN  \n",
       "317                                                NaN  \n",
       "318                                                NaN  \n",
       "319                                                NaN  \n",
       "320                                                NaN  \n",
       "321                                                NaN  \n",
       "322                                                NaN  \n",
       "323                                                NaN  \n",
       "324                                                NaN  \n",
       "325                                                NaN  \n",
       "326                                                NaN  \n",
       "327                                                NaN  \n",
       "\n",
       "[328 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
