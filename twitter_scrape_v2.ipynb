{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# New FISP Presidential Project Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depreceated function from when data was saved to a Google sheet\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "def authenticate_gspread():\n",
    "  # scopes that your application should be granted access\n",
    "  scope = ['https://spreadsheets.google.com/feeds'] \n",
    "  # Create a Credentials object from the service account's credentials and the scopes\n",
    "  credentials = ServiceAccountCredentials.from_json_keyfile_name('auth.json', scope)\n",
    "  gc = gspread.authorize(credentials)\n",
    "  return gc\n",
    "  \n",
    "# gets the list of cand or pac and returns it in a list\n",
    "def gspread_get_lists(worksheet, is_cand):\n",
    "  names = filter(lambda x: len(x) > 0, worksheet.col_values(2))\n",
    "  max_ids = worksheet.col_values(3)[:len(names)]\n",
    "  counts = worksheet.col_values(4)[:len(names)]\n",
    "  indices = range(1,len(names)+1)\n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dropbox import DropboxOAuth2FlowNoRedirect\n",
    "def authenticate_dropbox():\n",
    "  auth_flow = DropboxOAuth2FlowNoRedirect(ac.APP_KEY, ac.APP_SECRET)\n",
    "  \n",
    "  authorize_url = auth_flow.start()\n",
    "  print \"1. Go to: \" + authorize_url\n",
    "  print \"2. Click \\\"Allow\\\" (you might have to log in first).\"\n",
    "  print \"3. Copy the authorization code.\"\n",
    "  auth_code = raw_input(\"Enter the authorization code here: \").strip()\n",
    "  \n",
    "  try:\n",
    "    oauth_result = auth_flow.finish(auth_code)\n",
    "  except Exception, e:\n",
    "    print ('Error: %s' % (e,))\n",
    "    return\n",
    "  \n",
    "  dbx = dropbox.Dropbox(oauth_result.access_token)\n",
    "  return dbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take the rows with multiple tweets checked and make an individual row for each tweets\n",
    "def expand_lists(df):\n",
    "  # create a list for each columns and a dict to later convert into an df\n",
    "  id_ = []\n",
    "  ratings = []\n",
    "  sources = []\n",
    "  tweets = {'id': id_, 'rating': ratings, 'source': sources}\n",
    "  \n",
    "  # loop thru each row and if tweet id is stored in a list then create df \n",
    "  # with each id in a separate row with its fact check data\n",
    "  for index, row in df.iterrows():\n",
    "    if (type(row[0]) == list):\n",
    "      for i in row[0]:\n",
    "        id_.append(i)\n",
    "        ratings.append(row[1])\n",
    "        sources.append(row[2])\n",
    "      # drop the row containing multiple tweets\n",
    "      df.drop(index, inplace=True)\n",
    "  # create new df with tweets in their own row, then append them to the original dataframe\n",
    "  new_df = pd.DataFrame(tweets)\n",
    "  df = df.append(new_df)\n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets â†“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_data(is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "   \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False)\n",
    "  \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(100)\n",
    "  \n",
    "  list_df.to_excel(list_writer, sheet_name=sheetname, index=False)\n",
    "  tweet_writer.save()\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets\")\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Tweet_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Presidential_Tweets.xlsx\n",
      "INFO:__main__:Downloading 0 tweets from BernieSanders\n",
      "INFO:__main__:Downloading 0 tweets from BobbyJindal\n",
      "INFO:__main__:Downloading 0 tweets from CarlyFiorina\n",
      "INFO:__main__:Downloading 0 tweets from ChrisChristie\n",
      "INFO:__main__:Downloading 0 tweets from gov_gilmore\n",
      "INFO:__main__:Downloading 1 tweets from GovernorPataki\n",
      "INFO:__main__:Updated new tweets on spreadsheet for GovernorPataki\n",
      "INFO:__main__:Downloading 0 tweets from GovernorPerry\n",
      "INFO:__main__:Downloading 32 tweets from GovMikeHuckabee\n",
      "INFO:__main__:Updated new tweets on spreadsheet for GovMikeHuckabee\n",
      "INFO:__main__:Downloading 8 tweets from HillaryClinton\n",
      "INFO:__main__:Updated new tweets on spreadsheet for HillaryClinton\n",
      "INFO:__main__:Downloading 14 tweets from JebBush\n",
      "INFO:__main__:Updated new tweets on spreadsheet for JebBush\n",
      "INFO:__main__:Downloading 0 tweets from JimWebbUSA\n",
      "INFO:__main__:Downloading 28 tweets from JohnKasich\n",
      "INFO:__main__:Updated new tweets on spreadsheet for JohnKasich\n",
      "INFO:__main__:Downloading 0 tweets from Lessig2016\n",
      "INFO:__main__:Downloading 0 tweets from LincolnChafee\n",
      "INFO:__main__:Downloading 71 tweets from LindseyGrahamSC\n",
      "INFO:__main__:Updated new tweets on spreadsheet for LindseyGrahamSC\n",
      "INFO:__main__:Downloading 32 tweets from marcorubio\n",
      "INFO:__main__:Updated new tweets on spreadsheet for marcorubio\n",
      "INFO:__main__:Downloading 6 tweets from MartinOMalley\n",
      "INFO:__main__:Updated new tweets on spreadsheet for MartinOMalley\n",
      "INFO:__main__:Downloading 50 tweets from POTUS\n",
      "INFO:__main__:Updated new tweets on spreadsheet for POTUS\n",
      "INFO:__main__:Downloading 21 tweets from RandPaul\n",
      "INFO:__main__:Updated new tweets on spreadsheet for RandPaul\n",
      "INFO:__main__:Downloading 0 tweets from RealBenCarson\n",
      "INFO:__main__:Downloading 88 tweets from realDonaldTrump\n",
      "INFO:__main__:Updated new tweets on spreadsheet for realDonaldTrump\n",
      "INFO:__main__:Downloading 3 tweets from RickSantorum\n",
      "INFO:__main__:Updated new tweets on spreadsheet for RickSantorum\n",
      "INFO:__main__:Downloading 54 tweets from ScottWalker\n",
      "INFO:__main__:Updated new tweets on spreadsheet for ScottWalker\n",
      "INFO:__main__:Downloading 33 tweets from tedcruz\n",
      "INFO:__main__:Updated new tweets on spreadsheet for tedcruz\n",
      "INFO:__main__:Done Updating new tweets\n",
      "INFO:__main__:Time Elapsed: 26\n"
     ]
    }
   ],
   "source": [
    "collect_data(True)\n",
    "# collect_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params: is_cand - determines whether to pull candidates tweets or PAC tweets\n",
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():       \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(path + tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    tweets_df = update_metadata(tweets_df, updates_df)\n",
    "    \n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False)\n",
    "    tweet_writer.save()\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    \n",
    "    return 'done'\n",
    "    \n",
    "    time.sleep(100)\n",
    "  logger.info(\"Done collecting additional data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# figuring out how to update the retweet/like count using an apply function\n",
    "def update_metadata(tweets_df, updates_df): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "\n",
    "  # loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['text'], inplace=True)\n",
    "  tweets_df.reset_index(inplace=True)\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Tweet_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Presidential_Tweets.xlsx\n",
      "INFO:__main__:Downloaded tweets list\n",
      "INFO:__main__:Retrived data from spreadsheet for BernieSanders\n",
      "INFO:__main__:Downloading 3215 tweets from BernieSanders\n",
      "INFO:__main__:Updated data on spreadsheet for BernieSanders\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_addition_data(True)\n",
    "# collect_addition_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_full_url(short_urls, full_urls):\n",
    "for i, us in enumerate(short_urls):\n",
    "full = []\n",
    "  if not us.startswith(\"http\"):\n",
    "    continue\n",
    "  for url in us.split(\" \"):\n",
    "    if not url.startswith(\"http\"):\n",
    "      continue\n",
    "    try:\n",
    "      r = requests.head(url, allow_redirects=True)\n",
    "      full.append(r.url)\n",
    "    except:\n",
    "      logger.info(\"Error occurred for URL - %s\" % url)\n",
    "      continue\n",
    "  if i % 500 == 0:\n",
    "      logger.info(\"Extracting URL %d/%d\" % (i, len(short_urls)))\n",
    "      time.sleep(60)\n",
    "  full_urls[i] = \" \".join(full)q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_full_url(is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "    \n",
    "  logger.info(\"Successfully download the list...\")\n",
    "  for e, entry in enumerate(lists):\n",
    "    if e < 15:\n",
    "      continue\n",
    "\n",
    "    d = defaultdict(list)\n",
    "    name, since_id, count, index = entry[0], entry[1],entry[2], entry[3]\n",
    "\n",
    "    short_urls = worksheet.col_values(6)\n",
    "    logger.info(\"Downloaded %s URL\", name)\n",
    "    url_datas = ['' for i in xrange(len(short_urls))]\n",
    "    url_datas[0] = 'full URL'\n",
    "\n",
    "    get_full_url(short_urls, url_datas) # transfer short url to full urls and store in url_datas\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    while count < len(short_urls):\n",
    "      amount = min(100, len(short_urls) - count)\n",
    "      cells = worksheet.range('I'+str(count)+':'+'I'+str(count+amount-1))\n",
    "      assert(len(cells) == amount)\n",
    "      for i in range(amount):\n",
    "        cells[i].value = url_datas[count-1]\n",
    "        count += 1\n",
    "      worksheet.update_cells(cells)\n",
    "      logger.info(\"Update cells %d/%d for %s\" %(count, len(short_urls), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_full_url(True)\n",
    "#update_full_url(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below collects fact checks from the Washington Post's '2016 Election Fact Checker' and 'RealDonaldContext' chrome extension. The election fact checker data was hand collected and is stored in a json file while the extension data is pulled directly from the online hosted json file from the extension's developer blog.\n",
    "They are collected into an single dataframe consisting of the tweet id, rating, and source. They are then merged with a master sheet using tweet id.\n",
    "\n",
    "['2016 Election Fact Checker'](https://www.washingtonpost.com/graphics/politics/2016-election/fact-checker/)\n",
    "\n",
    "['RealDonaldContext'](https://chrome.google.com/webstore/detail/realdonaldcontext/ddbkmnomngnlcdglabflidgmhmcafogn?hl=en-US)\n",
    "\n",
    "['RealDonaldContext json file'](https://www.pbump.net/files/post/extension/core/data.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:read in fact checks\n",
      "INFO:__main__:Downloaded excel sheets list\n",
      "INFO:__main__:(215, 15)\n",
      "INFO:__main__:done\n"
     ]
    }
   ],
   "source": [
    "# this code is from the fact checking portion of this project. It grabs the fact checked tweets from\n",
    "# the WaPo Trump tweet fact checking extension and adds the ratings to correspoding tweets in the spreadsheet\n",
    "\n",
    "# sheetnames\n",
    "trump_sheet = 'realDonaldTrump'\n",
    "potus_sheet = 'POTUS'\n",
    "\n",
    "logger.info(\"Start...\")\n",
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)\n",
    "\n",
    "# read in WaPo fact checks of Donald Trump from the WaPo Trump tweet chrome extension\n",
    "trump_check = pd.read_json('https://www.pbump.net/files/post/extension/core/data.php')\n",
    "# rename columns and remove text columns\n",
    "trump_check.columns = ['id', 'rating', 'tweet', 'source']\n",
    "trump_check = trump_check[['id', 'rating', 'source']]\n",
    "# call expand lists to turn fact checks of multiple tweets into multiple columns\n",
    "trump_check = expand_lists(trump_check)\n",
    "\n",
    "# load pre-election fact checks and filter for just id, rating, and source\n",
    "election_checks = pd.read_json('preelection_wapo.json')\n",
    "election_checks = election_checks[['id', 'rating', 'source']]\n",
    "\n",
    "# append the hand collected data with the data collected from the extension\n",
    "trump_check = trump_check.append(election_checks, ignore_index=True)\n",
    "trump_check.columns = ['id', 'WAPO_RATING', 'WAPO_SOURCE']\n",
    "logger.info(\"read in fact checks\")\n",
    "\n",
    "# set file pathway variables an expand to HOME\n",
    "in_path = '~/Dropbox/Summer_of_Tweets/fact_checking/Presidential_Fact_Checking.xlsx'\n",
    "in_path = os.path.expanduser(in_path)\n",
    "\n",
    "# properly load spreadsheet to append new data\n",
    "work_book = load_workbook(in_path)\n",
    "tweet_writer = pd.ExcelWriter(in_path, engine='openpyxl')\n",
    "tweet_writer.book = work_book\n",
    "tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)\n",
    "tweets_df = pd.read_excel(in_path, sheetname=trump_sheet, dtype={'id': str})\n",
    "logger.info(\"Downloaded excel sheets list\")\n",
    "\n",
    "# change data type to match excel sheet's\n",
    "trump_check['id'] = trump_check['id'].astype(str)\n",
    "#merge the fact check data set with the tweets set using tweet id\n",
    "merged_df = tweets_df.merge(trump_check, on='id', how='left')\n",
    "\n",
    "logger.info(merged_df.shape) # used for debugging\n",
    "# write merged data to the excel sheet\n",
    "merged_df.to_excel(tweet_writer, sheet_name=trump_sheet, index=False)\n",
    "tweet_writer.save()\n",
    "\n",
    "# merged_df.to_csv('WaPo.csv', encoding='utf-8') # used for viewing test results\n",
    "\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp function to properly sort the tweets by date in ascending order\n",
    "def sort_sheet (is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "      \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "      \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "        \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_book = load_workbook(path + tweet_list)\n",
    "  list_writer = pd.ExcelWriter(path + tweet_list, engine='openpyxl')\n",
    "  list_writer.book = list_book\n",
    "  list_writer.sheets = dict((ws.title, ws) for ws in list_book.worksheets)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  work_book = load_workbook(path + tweet_sheet)\n",
    "  tweet_writer = pd.ExcelWriter(path + tweet_sheet, engine='openpyxl')\n",
    "  tweet_writer.book = work_book\n",
    "  tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)    \n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "       \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    tweets_df = pd.read_excel(path + tweet_sheet, sheetname=name)\n",
    "    \n",
    "    tweets_df = tweets_df.sort_values('id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
