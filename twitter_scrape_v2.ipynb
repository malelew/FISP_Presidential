{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# New FISP Presidential Project Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depreceated function from when data was saved to a Google sheet\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "def authenticate_gspread():\n",
    "  # scopes that your application should be granted access\n",
    "  scope = ['https://spreadsheets.google.com/feeds'] \n",
    "  # Create a Credentials object from the service account's credentials and the scopes\n",
    "  credentials = ServiceAccountCredentials.from_json_keyfile_name('auth.json', scope)\n",
    "  gc = gspread.authorize(credentials)\n",
    "  return gc\n",
    "  \n",
    "# gets the list of cand or pac and returns it in a list\n",
    "def gspread_get_lists(worksheet, is_cand):\n",
    "  names = filter(lambda x: len(x) > 0, worksheet.col_values(2))\n",
    "  max_ids = worksheet.col_values(3)[:len(names)]\n",
    "  counts = worksheet.col_values(4)[:len(names)]\n",
    "  indices = range(1,len(names)+1)\n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dropbox import DropboxOAuth2FlowNoRedirect\n",
    "def authenticate_dropbox():\n",
    "  auth_flow = DropboxOAuth2FlowNoRedirect(ac.APP_KEY, ac.APP_SECRET)\n",
    "  \n",
    "  authorize_url = auth_flow.start()\n",
    "  print \"1. Go to: \" + authorize_url\n",
    "  print \"2. Click \\\"Allow\\\" (you might have to log in first).\"\n",
    "  print \"3. Copy the authorization code.\"\n",
    "  auth_code = raw_input(\"Enter the authorization code here: \").strip()\n",
    "  \n",
    "  try:\n",
    "    oauth_result = auth_flow.finish(auth_code)\n",
    "  except Exception, e:\n",
    "    print ('Error: %s' % (e,))\n",
    "    return\n",
    "  \n",
    "  dbx = dropbox.Dropbox(oauth_result.access_token)\n",
    "  return dbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take the rows with multiple tweets checked and make an individual row for each tweets\n",
    "def expand_lists(df):\n",
    "  # create a list for each columns and a dict to later convert into an df\n",
    "  id_ = []\n",
    "  ratings = []\n",
    "  sources = []\n",
    "  tweets = {'id': id_, 'rating': ratings, 'source': sources}\n",
    "  \n",
    "  # loop thru each row and if tweet id is stored in a list then create df \n",
    "  # with each id in a separate row with its fact check data\n",
    "  for index, row in df.iterrows():\n",
    "    if (type(row[0]) == list):\n",
    "      for i in row[0]:\n",
    "        id_.append(i)\n",
    "        ratings.append(row[1])\n",
    "        sources.append(row[2])\n",
    "      # drop the row containing multiple tweets\n",
    "      df.drop(index, inplace=True)\n",
    "  # create new df with tweets in their own row, then append them to the original dataframe\n",
    "  new_df = pd.DataFrame(tweets)\n",
    "  df = df.append(new_df)\n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets â†“\n",
    "\n",
    "All the following functions write to excel or csv sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pull Func\n",
    "\n",
    "The following functin gets the most up to date tweets and writes them to the master excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_data(is_cand):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "   \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # Lessign has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False)\n",
    "  \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(100)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.to_excel(list_writer, sheet_name=sheetname, index=False)\n",
    "  tweet_writer.save()\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "collect_data(True)\n",
    "collect_data(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued update of metadata\n",
    "\n",
    "A tweets ability to stay in the public discouse is dependent on the number of retweets and favorites. The initial pull of a tweet will not complete picture of the tweets effectiveness. This script allows us to continously update a tweet's metadata counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params: is_cand - determines whether to pull candidates tweets or PAC tweets\n",
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(is_cand):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():       \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # Lessig has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(path + tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df)\n",
    "    \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    \n",
    "    # 100 second pause between data pulls to voud token exceptions\n",
    "    time.sleep(100)\n",
    "  \n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "def update_metadata(tweets_df, updates_df): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  # check for duplicates\n",
    "  dupe_df = tweets_df[tweets_df.index.duplicated()]\n",
    "  print dupe_df\n",
    "  \n",
    "  # loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['text'], inplace=True)\n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Tweet_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Presidential_Tweets.xlsx\n",
      "INFO:__main__:Downloaded tweets list\n",
      "INFO:__main__:Retrived data from spreadsheet for BernieSanders\n",
      "INFO:__main__:Downloading 3215 tweets from BernieSanders\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [created_at, text, hashtag#, at@, link, retweets, favorites, full URL]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Updated data on spreadsheet for BernieSanders\n",
      "INFO:__main__:Retrived data from spreadsheet for BobbyJindal\n",
      "INFO:__main__:Downloading 3220 tweets from BobbyJindal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [created_at, text, hashtag#, at@, link, retweets, favorites, full URL]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Updated data on spreadsheet for BobbyJindal\n",
      "INFO:__main__:Retrived data from spreadsheet for CarlyFiorina\n",
      "INFO:__main__:Downloading 3198 tweets from CarlyFiorina\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [created_at, text, hashtag#, at@, link, retweets, favorites, full URL]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Updated data on spreadsheet for CarlyFiorina\n",
      "INFO:__main__:Retrived data from spreadsheet for ChrisChristie\n",
      "INFO:__main__:Downloading 2844 tweets from ChrisChristie\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            created_at  \\\n",
      "id                                       \n",
      "397425000000000000 2013-11-04 18:08:35   \n",
      "\n",
      "                                                                 text  \\\n",
      "id                                                                      \n",
      "397425000000000000  #thegov has arrived at 3rd stop in freehold ht...   \n",
      "\n",
      "                   hashtag#  at@                     link  retweets  \\\n",
      "id                                                                    \n",
      "397425000000000000  #thegov  NaN  https://t.co/EevdL6UUVR       NaN   \n",
      "\n",
      "                    favorites                       full URL  \n",
      "id                                                            \n",
      "397425000000000000        NaN  https://vine.co/v/hjUTJbXzQjJ  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e82dc2583137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollect_addition_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# collect_addition_data(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a8eebcf716c5>\u001b[0m in \u001b[0;36mcollect_addition_data\u001b[0;34m(is_cand)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# call helper fuction to match updated metadata with correct tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# write the updated data to the twitter profile's sheet to be saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-a563bbc86dbb>\u001b[0m in \u001b[0;36mupdate_metadata\u001b[0;34m(tweets_df, updates_df)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# loop through the updates metadata and updates the tweet sheet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retweets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'favorites'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mset_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m             \u001b[0;31m# set using a non-recursive method & reset the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mreindex_axis\u001b[0;34m(self, labels, axis, method, level, copy, limit, fill_value)\u001b[0m\n\u001b[1;32m   2739\u001b[0m                      \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2740\u001b[0m                                         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2741\u001b[0;31m                                         limit=limit, fill_value=fill_value)\n\u001b[0m\u001b[1;32m   2742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mreindex_axis\u001b[0;34m(self, labels, axis, method, level, copy, limit, fill_value)\u001b[0m\n\u001b[1;32m   2602\u001b[0m                                                  limit=limit)\n\u001b[1;32m   2603\u001b[0m         return self._reindex_with_indexers({axis: [new_index, indexer]},\n\u001b[0;32m-> 2604\u001b[0;31m                                            fill_value=fill_value, copy=copy)\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m     def _reindex_with_indexers(self, reindexers, fill_value=np.nan, copy=False,\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                                                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m                                                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m                                                 copy=copy)\n\u001b[0m\u001b[1;32m   2628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   3884\u001b[0m         \u001b[0;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3885\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3886\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36m_can_reindex\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   2834\u001b[0m         \u001b[0;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2836\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2838\u001b[0m     def reindex(self, target, method=None, level=None, limit=None,\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "collect_addition_data(True)\n",
    "# collect_addition_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_full_url(short_urls, full_urls):\n",
    "for i, us in enumerate(short_urls):\n",
    "full = []\n",
    "  if not us.startswith(\"http\"):\n",
    "    continue\n",
    "  for url in us.split(\" \"):\n",
    "    if not url.startswith(\"http\"):\n",
    "      continue\n",
    "    try:\n",
    "      r = requests.head(url, allow_redirects=True)\n",
    "      full.append(r.url)\n",
    "    except:\n",
    "      logger.info(\"Error occurred for URL - %s\" % url)\n",
    "      continue\n",
    "  if i % 500 == 0:\n",
    "      logger.info(\"Extracting URL %d/%d\" % (i, len(short_urls)))\n",
    "      time.sleep(60)\n",
    "  full_urls[i] = \" \".join(full)q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_full_url(is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "    \n",
    "  logger.info(\"Successfully download the list...\")\n",
    "  for e, entry in enumerate(list_df):\n",
    "    if e < 15:\n",
    "      continue\n",
    "\n",
    "    name, since_id, count, index = entry[0], entry[1],entry[2], entry[3]\n",
    "\n",
    "    short_urls = worksheet.col_values(6)\n",
    "    logger.info(\"Downloaded %s URL\", name)\n",
    "    url_datas = ['' for i in xrange(len(short_urls))]\n",
    "    url_datas[0] = 'full URL'\n",
    "\n",
    "    get_full_url(short_urls, url_datas) # transfer short url to full urls and store in url_datas\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    while count < len(short_urls):\n",
    "      amount = min(100, len(short_urls) - count)\n",
    "      cells = worksheet.range('I'+str(count)+':'+'I'+str(count+amount-1))\n",
    "      assert(len(cells) == amount)\n",
    "      for i in range(amount):\n",
    "        cells[i].value = url_datas[count-1]\n",
    "        count += 1\n",
    "      worksheet.update_cells(cells)\n",
    "      logger.info(\"Update cells %d/%d for %s\" %(count, len(short_urls), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_full_url(True)\n",
    "#update_full_url(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaPo Fact Checking\n",
    "The cell below collects fact checks from the Washington Post's '2016 Election Fact Checker' and 'RealDonaldContext' chrome extension. The election fact checker data was hand collected and is stored in a json file while the extension data is pulled directly from the online hosted json file from the extension's developer blog.\n",
    "They are collected into an single dataframe consisting of the tweet id, rating, and source. They are then merged with a master sheet using tweet id.\n",
    "\n",
    "['2016 Election Fact Checker'](https://www.washingtonpost.com/graphics/politics/2016-election/fact-checker/)\n",
    "\n",
    "['RealDonaldContext'](https://chrome.google.com/webstore/detail/realdonaldcontext/ddbkmnomngnlcdglabflidgmhmcafogn?hl=en-US)\n",
    "\n",
    "['RealDonaldContext json file'](https://www.pbump.net/files/post/extension/core/data.php)\n",
    "\n",
    "['Rating System Scale'](https://www.washingtonpost.com/news/fact-checker/about-the-fact-checker/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code is from the fact checking portion of this project. It grabs the fact checked tweets from\n",
    "# the WaPo Trump tweet fact checking extension and adds the ratings to correspoding tweets in the spreadsheet\n",
    "\n",
    "# sheetnames\n",
    "trump_sheet = 'realDonaldTrump'\n",
    "potus_sheet = 'POTUS'\n",
    "\n",
    "logger.info(\"Start...\")\n",
    "\n",
    "# read in WaPo fact checks of Donald Trump from the WaPo Trump tweet chrome extension\n",
    "trump_check = pd.read_json('https://www.pbump.net/files/post/extension/core/data.php')\n",
    "# rename columns and remove text columns\n",
    "trump_check.columns = ['id', 'rating', 'tweet', 'source']\n",
    "trump_check = trump_check[['id', 'rating', 'source']]\n",
    "# call expand lists to turn fact checks of multiple tweets into multiple columns\n",
    "trump_check = expand_lists(trump_check)\n",
    "\n",
    "# load pre-election fact checks and filter for just id, rating, and source\n",
    "election_checks = pd.read_json('preelection_wapo.json')\n",
    "election_checks = election_checks[['id', 'rating', 'source']]\n",
    "\n",
    "# append the hand collected data with the data collected from the extension\n",
    "trump_check = trump_check.append(election_checks, ignore_index=True)\n",
    "trump_check.columns = ['id', 'WAPO_RATING', 'WAPO_SOURCE']\n",
    "logger.info(\"read in fact checks\")\n",
    "\n",
    "# set file pathway variables an expand to HOME\n",
    "in_path = '~/Dropbox/Summer_of_Tweets/fact_checking/Presidential_Fact_Checking.xlsx'\n",
    "in_path = os.path.expanduser(in_path)\n",
    "\n",
    "# properly load spreadsheet to append new data\n",
    "work_book = load_workbook(in_path)\n",
    "tweet_writer = pd.ExcelWriter(in_path, engine='openpyxl')\n",
    "tweet_writer.book = work_book\n",
    "tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)\n",
    "tweets_df = pd.read_excel(in_path, sheetname=trump_sheet, dtype={'id': str})\n",
    "logger.info(\"Downloaded excel sheets list\")\n",
    "\n",
    "# change data type to match excel sheet's\n",
    "trump_check['id'] = trump_check['id'].astype(str)\n",
    "#merge the fact check data set with the tweets set using tweet id\n",
    "merged_df = tweets_df.merge(trump_check, on='id', how='left')\n",
    "\n",
    "logger.info(merged_df.shape) # used for debugging\n",
    "# write merged data to the excel sheet\n",
    "merged_df.to_excel(tweet_writer, sheet_name=trump_sheet, index=False)\n",
    "tweet_writer.save()\n",
    "\n",
    "# merged_df.to_csv('WaPo.csv', encoding='utf-8') # used for viewing test results\n",
    "\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follower Growth\n",
    "The follower growth for Hillary Clinton and Donald Trump is being collected for the project. After some research on which sites are best for follower growth data, [Trackalytics](http://www.trackalytics.com) is the best free resource for tracking follower growth. However it does not have comprehensive follower growth data for the rest of the candidates, the others either are not present on the site or their data starts to get collected well into the election cycle.\n",
    "\n",
    "The data is scraped using the IMPORTHTML function in google sheets. Information on the function and how to use it can be found [here](http://lenagroeger.s3.amazonaws.com/talks/orlando/gettingdata.html)  while the sheet itself can be found [here](https://docs.google.com/spreadsheets/d/1rahomcsDJFf_za0S_Tbzi1kv79bdNM2ZqNZ_H7XcMIM/edit?usp=sharing). \n",
    "\n",
    "The following function runs to clean the data sheet to move daily delta in followers into its own column and then downloading and moving the sheet onto the FISP dropbox.\n",
    "\n",
    "Currently stuck trying to implement this [method](https://github.com/davidasboth/blog-notebooks/blob/master/connecting-to-google-sheets/Connecting%20to%20a%20Google%20Sheet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_cand_followers ():\n",
    "  logger.info(\"Start...\")\n",
    "  #gc = authenticate_gspread()\n",
    "  \n",
    "  csv_url = \"{}/export?gid=0&format=csv\"\\\n",
    "    .format(\"https://docs.google.com/spreadsheets/d/1rahomcsDJFf_za0S_Tbzi1kv79bdNM2ZqNZ_H7XcMIM\")\n",
    "  \n",
    "  clinton_growth_df = pd.read_excel(csv_url, 0)\n",
    "  #trump_growth_df = pd.read_excel(ac.follower_sheet_url, 1)\n",
    "  \n",
    "  # clinton_growth_df = pd.read_excel(ac.follower_sheet_url, 0)\n",
    "  # trump_growth_df = pd.read_excel(ac.follower_sheet_url, 1)\n",
    "  \n",
    "  clinton_growth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_cand_followers ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 538 Polling Data Pull\n",
    "\n",
    "The cell below collects [538's National Polling](https://projects.fivethirtyeight.com/2016-election-forecast/national-polls/) data aggregation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Response.json of <Response [200]>>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requests will retrieve the webpage and bs4 allows up to pull out the data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# get the webpage and convert it into a BeautifulSoup object for easier manipulation\n",
    "nat_url = 'https://projects.fivethirtyeight.com/2016-election-forecast/national-polls/'\n",
    "page = requests.get(nat_url)\n",
    "nat_page = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "page.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp function to properly sort the tweets by date in ascending order\n",
    "def sort_sheet (is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "      \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "      \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "        \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_book = load_workbook(path + tweet_list)\n",
    "  list_writer = pd.ExcelWriter(path + tweet_list, engine='openpyxl')\n",
    "  list_writer.book = list_book\n",
    "  list_writer.sheets = dict((ws.title, ws) for ws in list_book.worksheets)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  work_book = load_workbook(path + tweet_sheet)\n",
    "  tweet_writer = pd.ExcelWriter(path + tweet_sheet, engine='openpyxl')\n",
    "  tweet_writer.book = work_book\n",
    "  tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)    \n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "       \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    tweets_df = pd.read_excel(path + tweet_sheet, sheetname=name)\n",
    "    \n",
    "    tweets_df = tweets_df.sort_values('id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
