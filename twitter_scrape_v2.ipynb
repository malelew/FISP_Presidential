{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FISP Presidential Project Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import lxml\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depreceated function from when data was saved to a Google sheet\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "def authenticate_gspread():\n",
    "  # scopes that your application should be granted access\n",
    "  scope = ['https://spreadsheets.google.com/feeds'] \n",
    "  # Create a Credentials object from the service account's credentials and the scopes\n",
    "  credentials = ServiceAccountCredentials.from_json_keyfile_name('auth.json', scope)\n",
    "  gc = gspread.authorize(credentials)\n",
    "  return gc\n",
    "  \n",
    "# gets the list of cand or pac and returns it in a list\n",
    "def gspread_get_lists(worksheet, is_cand):\n",
    "  names = filter(lambda x: len(x) > 0, worksheet.col_values(2))\n",
    "  max_ids = worksheet.col_values(3)[:len(names)]\n",
    "  counts = worksheet.col_values(4)[:len(names)]\n",
    "  indices = range(1,len(names)+1)\n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dropbox import DropboxOAuth2FlowNoRedirect\n",
    "def authenticate_dropbox():\n",
    "  auth_flow = DropboxOAuth2FlowNoRedirect(ac.APP_KEY, ac.APP_SECRET)\n",
    "  \n",
    "  authorize_url = auth_flow.start()\n",
    "  print \"1. Go to: \" + authorize_url\n",
    "  print \"2. Click \\\"Allow\\\" (you might have to log in first).\"\n",
    "  print \"3. Copy the authorization code.\"\n",
    "  auth_code = raw_input(\"Enter the authorization code here: \").strip()\n",
    "  \n",
    "  try:\n",
    "    oauth_result = auth_flow.finish(auth_code)\n",
    "  except Exception, e:\n",
    "    print ('Error: %s' % (e,))\n",
    "    return\n",
    "  \n",
    "  dbx = dropbox.Dropbox(oauth_result.access_token)\n",
    "  return dbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take the rows with multiple tweets checked and make an individual row for each tweets\n",
    "def expand_lists(df):\n",
    "  # create a list for each columns and a dict to later convert into an df\n",
    "  id_ = []\n",
    "  ratings = []\n",
    "  sources = []\n",
    "  tweets = {'id': id_, 'rating': ratings, 'source': sources}\n",
    "  \n",
    "  # loop thru each row and if tweet id is stored in a list then create df \n",
    "  # with each id in a separate row with its fact check data\n",
    "  for index, row in df.iterrows():\n",
    "    if (type(row[0]) == list):\n",
    "      for i in row[0]:\n",
    "        id_.append(i)\n",
    "        ratings.append(row[1])\n",
    "        sources.append(row[2])\n",
    "      # drop the row containing multiple tweets\n",
    "      df.drop(index, inplace=True)\n",
    "  # create new df with tweets in their own row, then append them to the original dataframe\n",
    "  new_df = pd.DataFrame(tweets)\n",
    "  df = df.append(new_df)\n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dupe_check(tweets_df, cand_name):\n",
    "  # check for duplicates\n",
    "  dupe_df = tweets_df[tweets_df.id.duplicated()]\n",
    "  \n",
    "  # if there exists a duplicate save it to a csv\n",
    "  if (len(dupe_df) > 0):\n",
    "    # make sure that the dupe_folder dir exists\n",
    "    dupe_df.to_csv('./dupe_folder/' + cand_name + '.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp function to properly sort the tweets by date in ascending order\n",
    "def sort_sheet (tweet_sheet, sheetname, tweet_list):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "        \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_book = load_workbook(tweet_list)\n",
    "  list_writer = pd.ExcelWriter(tweet_list, engine='openpyxl')\n",
    "  list_writer.book = list_book\n",
    "  list_writer.sheets = dict((ws.title, ws) for ws in list_book.worksheets)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  work_book = load_workbook(tweet_sheet)\n",
    "  tweet_writer = pd.ExcelWriter(tweet_sheet, engine='openpyxl')\n",
    "  tweet_writer.book = work_book\n",
    "  tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)    \n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "       \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    \n",
    "    tweets_df = tweets_df.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set file pathway variables an expand to HOME\n",
    "path = '~/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/'\n",
    "tweet_list = \"Tweet_List.xlsx\"\n",
    "cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "path = os.path.expanduser(path)\n",
    "\n",
    "# sheetnames\n",
    "cand_sheet = 'candidate'\n",
    "pac_sheet = 'pac'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets â†“\n",
    "\n",
    "All the following functions write to excel or csv sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pull Func\n",
    "\n",
    "The following functin gets the most up to date tweets and writes them to the master excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_data(tweet_sheet, sheetname, tweet_list):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "   \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # Lessign has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    # grab new tweets since last id and save it to a dataframe\n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    \n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False)\n",
    "  \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(100)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.to_excel(list_writer, sheet_name=sheetname, index=False)\n",
    "  tweet_writer.save()\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "collect_data(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "collect_data(path + pac_tweets, pac_sheet, path + tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update of metadata\n",
    "\n",
    "A tweets ability to stay in the public discouse is dependent on the number of retweets and favorites. The initial pull of a tweet will not complete picture of the tweets effectiveness. This script allows us to continously update a tweet's metadata counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params: is_cand - determines whether to pull candidates tweets or PAC tweets\n",
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(tweet_sheet, sheetname, tweet_list):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():       \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # Lessig has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df, name)\n",
    "    \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    # 100 second pause between data pulls to avoid token exceptions\n",
    "    time.sleep(20)\n",
    "  \n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "def update_metadata(tweets_df, updates_df, cand_name): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  ## loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['created_at'], inplace=True)\n",
    "  \n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collect_addition_data(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "collect_addition_data(path + pac_tweets, pac_sheet, path + tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_full_url(short_urls, full_urls):\n",
    "for i, us in enumerate(short_urls):\n",
    "full = []\n",
    "  if not us.startswith(\"http\"):\n",
    "    continue\n",
    "  for url in us.split(\" \"):\n",
    "    if not url.startswith(\"http\"):\n",
    "      continue\n",
    "    try:\n",
    "      r = requests.head(url, allow_redirects=True)\n",
    "      full.append(r.url)\n",
    "    except:\n",
    "      logger.info(\"Error occurred for URL - %s\" % url)\n",
    "      continue\n",
    "  if i % 500 == 0:\n",
    "      logger.info(\"Extracting URL %d/%d\" % (i, len(short_urls)))\n",
    "      time.sleep(60)\n",
    "  full_urls[i] = \" \".join(full)q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_full_url(tweets_df, updates_df, cand_name):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "    \n",
    "  logger.info(\"Successfully download the list...\")\n",
    "  for e, entry in enumerate(list_df):\n",
    "    if e < 15:\n",
    "      continue\n",
    "\n",
    "    name, since_id, count, index = entry[0], entry[1],entry[2], entry[3]\n",
    "\n",
    "    short_urls = worksheet.col_values(6)\n",
    "    logger.info(\"Downloaded %s URL\", name)\n",
    "    url_datas = ['' for i in xrange(len(short_urls))]\n",
    "    url_datas[0] = 'full URL'\n",
    "\n",
    "    get_full_url(short_urls, url_datas) # transfer short url to full urls and store in url_datas\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    while count < len(short_urls):\n",
    "      amount = min(100, len(short_urls) - count)\n",
    "      cells = worksheet.range('I'+str(count)+':'+'I'+str(count+amount-1))\n",
    "      assert(len(cells) == amount)\n",
    "      for i in range(amount):\n",
    "        cells[i].value = url_datas[count-1]\n",
    "        count += 1\n",
    "      worksheet.update_cells(cells)\n",
    "      logger.info(\"Update cells %d/%d for %s\" %(count, len(short_urls), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_full_url(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "update_full_url(path + pac_tweets, pac_sheet, path + tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert into one large csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_xlsx_csv (tweet_sheet, sheetname, tweet_list):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4) \n",
    "  \n",
    "  #merged_corpus = pd.DataFrame(columns=['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'full URL'])\n",
    "  merged_df = pd.DataFrame()\n",
    "\n",
    "  initial_loop = True\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():\n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    if(name == 'POTUS'):\n",
    "      continue\n",
    "    \n",
    "    if (initial_loop):\n",
    "      merged_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      merged_df['Name'] = name\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      initial_loop = False \n",
    "    \n",
    "    else:\n",
    "      # read current cand tweet sheet\n",
    "      curr_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      curr_df['Name'] = name\n",
    "      #print (curr_df)\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      \n",
    "      merged_df = merged_df.append(curr_df)\n",
    "      if(name =='ChrisChristie'):\n",
    "        break\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  merged_df.to_csv('merged_corpus.csv', encoding='utf-8')\n",
    "  \n",
    "  logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_xlsx_csv(path + cand_tweets, cand_sheet, path + tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaPo Fact Checking\n",
    "The cell below collects fact checks from the Washington Post's '2016 Election Fact Checker' and 'RealDonaldContext' chrome extension. The election fact checker data was hand collected and is stored in a json file while the extension data is pulled directly from the online hosted json file from the extension's developer blog.\n",
    "They are collected into an single dataframe consisting of the tweet id, rating, and source. They are then merged with a master sheet using tweet id.\n",
    "\n",
    "['2016 Election Fact Checker'](https://www.washingtonpost.com/graphics/politics/2016-election/fact-checker/)\n",
    "\n",
    "['RealDonaldContext'](https://chrome.google.com/webstore/detail/realdonaldcontext/ddbkmnomngnlcdglabflidgmhmcafogn?hl=en-US)\n",
    "\n",
    "['RealDonaldContext json file'](https://www.pbump.net/files/post/extension/core/data.php)\n",
    "\n",
    "['Rating System Scale'](https://www.washingtonpost.com/news/fact-checker/about-the-fact-checker/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code is from the fact checking portion of this project. It grabs the fact checked tweets from\n",
    "# the WaPo Trump tweet fact checking extension and adds the ratings to correspoding tweets in the spreadsheet\n",
    "\n",
    "# sheetnames\n",
    "trump_sheet = 'realDonaldTrump'\n",
    "potus_sheet = 'POTUS'\n",
    "\n",
    "logger.info(\"Start...\")\n",
    "\n",
    "# read in WaPo fact checks of Donald Trump from the WaPo Trump tweet chrome extension\n",
    "trump_check = pd.read_json('https://www.pbump.net/files/post/extension/core/data.php')\n",
    "# rename columns and remove text columns\n",
    "trump_check.columns = ['id', 'rating', 'tweet', 'source']\n",
    "trump_check = trump_check[['id', 'rating', 'source']]\n",
    "# call expand lists to turn fact checks of multiple tweets into multiple columns\n",
    "trump_check = expand_lists(trump_check)\n",
    "\n",
    "# load pre-election fact checks and filter for just id, rating, and source\n",
    "election_checks = pd.read_json('preelection_wapo.json')\n",
    "election_checks = election_checks[['id', 'rating', 'source']]\n",
    "\n",
    "# append the hand collected data with the data collected from the extension\n",
    "trump_check = trump_check.append(election_checks, ignore_index=True)\n",
    "trump_check.columns = ['id', 'WAPO_RATING', 'WAPO_SOURCE']\n",
    "logger.info(\"read in fact checks\")\n",
    "\n",
    "# set file pathway variables an expand to HOME\n",
    "in_path = '~/Dropbox/Summer_of_Tweets/fact_checking/Presidential_Fact_Checking.xlsx'\n",
    "in_path = os.path.expanduser(in_path)\n",
    "\n",
    "# properly load spreadsheet to append new data\n",
    "work_book = load_workbook(in_path)\n",
    "tweet_writer = pd.ExcelWriter(in_path, engine='openpyxl')\n",
    "tweet_writer.book = work_book\n",
    "tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)\n",
    "tweets_df = pd.read_excel(in_path, sheetname=trump_sheet, dtype={'id': str})\n",
    "logger.info(\"Downloaded excel sheets list\")\n",
    "\n",
    "# change data type to match excel sheet's\n",
    "trump_check['id'] = trump_check['id'].astype(str)\n",
    "#merge the fact check data set with the tweets set using tweet id\n",
    "merged_df = tweets_df.merge(trump_check, on='id', how='left')\n",
    "\n",
    "logger.info(merged_df.shape) # used for debugging\n",
    "# write merged data to the excel sheet\n",
    "merged_df.to_excel(tweet_writer, sheet_name=trump_sheet, index=False)\n",
    "tweet_writer.save()\n",
    "\n",
    "# merged_df.to_csv('WaPo.csv', encoding='utf-8') # used for viewing test results\n",
    "\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follower Growth\n",
    "The follower growth for Hillary Clinton and Donald Trump is being collected for the project. After some research on which sites are best for follower growth data, [Trackalytics](http://www.trackalytics.com) is the best free resource for tracking follower growth. However it does not have comprehensive follower growth data for the rest of the candidates, the others either are not present on the site or their data starts to get collected well into the election cycle.\n",
    "\n",
    "The data is scraped using the IMPORTHTML function in google sheets. Information on the function and how to use it can be found [here](http://lenagroeger.s3.amazonaws.com/talks/orlando/gettingdata.html)  while the sheet itself can be found [here](https://docs.google.com/spreadsheets/d/1rahomcsDJFf_za0S_Tbzi1kv79bdNM2ZqNZ_H7XcMIM/edit?usp=sharing). \n",
    "\n",
    "The following function runs to clean the data sheet to move daily delta in followers into its own column and then downloading and moving the sheet onto the FISP dropbox.\n",
    "\n",
    "Implemented using the df2gspread module, documentation for the module can be found [here](https://github.com/maybelinot/df2gspread)\n",
    "\n",
    "[Trump Follower Tracker](http://www.trackalytics.com/twitter/profile/RealDonaldTrump/)\n",
    "\n",
    "[Clinton Follower Tracker](http://www.trackalytics.com/twitter/profile/HillaryClinton/)\n",
    "\n",
    "*The Site is missing data for July 3-5th 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the path to the follower growth sheeet in my drive\n",
    "#sheet_path = \"archive/fisp_twitter/sheets/follower_growth_test\"\n",
    "sheet_path = \"./follower_growth.xlsx\"\n",
    "\n",
    "# currently it contains Clinton and Trump follower growth\n",
    "clinton_sheet = \"HillaryClinton\"\n",
    "trump_sheet = \"realDonaldTrump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# passed into the apply func to remove daily change in metadata for twitter accounts\n",
    "def split_func (string):\n",
    "  return string.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import module for downloading the sheets from the drive \n",
    "from df2gspread import gspread2df as g2d\n",
    "\n",
    "def gen_cand_followers (file_path, sheet_name):\n",
    "  # download the follower growth sheet from gdrive\n",
    "  #df = g2d.download(file_path, sheet_name, col_names = True)\n",
    "  df = pd.read_excel(file_path, sheet_name)\n",
    "\n",
    "  # take the top row and convert it to the column header, this code is only needed if data is pulled from gdrive\n",
    "  #df.columns = df.iloc[0]\n",
    "  #df = df.reindex(df.index.drop(0))\n",
    "  \n",
    "  # trim out the change data that is appended at the end ot the daily value for the metadata\n",
    "  df['Followers'] = df.Followers_change.apply(split_func)\n",
    "  df['Following'] = df.Following_change.apply(split_func)\n",
    "  df['Tweets'] = df.Tweets_change.apply(split_func)\n",
    "  \n",
    "  # drop the change rows\n",
    "  df = df.drop(['Followers_change', 'Following_change', 'Tweets_change', \n",
    "                'Lists_change', 'Favourites_change', 'Tweets', 'id', 'Following'], 1)\n",
    "  \n",
    "  # convert date to MM/DD/YYYY format and rename the column\n",
    "  df['Date'] = pd.to_datetime(df.Date)\n",
    "  df['Date'] = df['Date'].dt.strftime('%m/%d/%Y')\n",
    "  df = df.rename(index=str, columns={'Date': 'date'})\n",
    "  \n",
    "  # add col with candidate name\n",
    "  df['name'] = sheet_name\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton_follower_df = gen_cand_followers (sheet_path, clinton_sheet)\n",
    "trump_follower_df = gen_cand_followers (sheet_path, trump_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_follower_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append row with each candidate's last name\n",
    "clinton_follower_df['lastname'] = 'Clinton'\n",
    "trump_follower_df['lastname'] = 'Trump'\n",
    "gen_cand_follower_df = clinton_follower_df.append(trump_follower_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save csv's path and load it into a panda's dataframe\n",
    "path = '~/Dropbox/Summer_of_Tweets/Deduped_Tweets/polling_merged.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# duplicated the tweet created at columns to made a modify it to a MM/DD/YY format\n",
    "df['date'] = df['created_at']\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['date'] = df['date'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the full data set with the general cadidate's follower growth using the date and lastname fields\n",
    "merged_df = df.merge(gen_cand_follower_df.drop(['Following', 'Tweets'], 1), on=['date', 'lastname'], how='inner')\n",
    "follower_df = merged_df.to_csv('followers_polling_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wayback Machine Follower Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path variables\n",
    "twitter_pages = '~/fisp_testing/pages/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_follower_growth(pages_dir):\n",
    "  handles = [name for name in os.listdir(os.path.expanduser(pages_dir)) if not name.startswith('.')]\n",
    "  \n",
    "  for handle in handles:\n",
    "\n",
    "\n",
    "    dates = [date for date in os.listdir(os.path.expanduser(pages_dir + handle)) if not date.startswith('.')]\n",
    "    \n",
    "    follower_count_dict = {}\n",
    "    failed_dates = []\n",
    "    passed_dates = []\n",
    "    print handle\n",
    "    for date in dates:\n",
    "      num_len = 0\n",
    "      num = False\n",
    "      \n",
    "      count_date = datetime.strptime(date, \"%Y%m%d%H%M%S\")\n",
    "      page = twitter_pages + handle + '/' + date + '/twitter.com' + '/' + handle\n",
    "      soup = BeautifulSoup(open(os.path.expanduser(page)), 'html.parser')\n",
    "      soup = str(soup)\n",
    "      \n",
    "      init = soup.find('followers_count')\n",
    "      \n",
    "      if (init == -1):\n",
    "        failed_dates.append(date[0:4])\n",
    "        continue\n",
    "      passed_dates.append(date[0:4])\n",
    "      \n",
    "      while (not num):\n",
    "        init+=1\n",
    "        num = soup[init].isdigit()\n",
    "      \n",
    "      while (num):\n",
    "        num_len+=1\n",
    "        num = soup[init:init+num_len].isdigit()     \n",
    "    \n",
    "      #logger.info(date)\n",
    "      \n",
    "      follower_count = int(soup[init:init+num_len-1])\n",
    "      follower_count_dict[date] = follower_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BobbyJindal\n",
      "gov_gilmore\n",
      "ChrisChristie\n"
     ]
    }
   ],
   "source": [
    "compile_follower_growth(twitter_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print pd.Series(failed_dates).value_counts()\n",
    "print pd.Series(passed_dates).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(failed_dates)\n",
    "print len(cand_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_files = [f for f in os.listdir(os.path.expanduser(twitter_pages)) if dir]\n",
    "dates = [date for date in os.listdir(os.path.expanduser(twitter_pages + cand_files[1])) if not date.startswith('.')]\n",
    "count_date = datetime.strptime(dates[0], \"%Y%m%d%H%M%S\")\n",
    "page = twitter_pages + cand_files[1] + '/' + dates[0] + '/twitter.com/' + cand_files[1]\n",
    "soup = BeautifulSoup(open(os.path.expanduser(page)), 'html.parser')\n",
    "soup = soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = soup.find('followers_count')\n",
    "num_len = 0\n",
    "num = False\n",
    "while (not num):\n",
    "  init+=1\n",
    "  num = soup[init].isdigit()\n",
    "  print num\n",
    "\n",
    "while (num):\n",
    "  num_len+=1\n",
    "  num = soup[init:init+num_len].isdigit()\n",
    "\n",
    "print int(soup[init:init+num_len-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polling Data Match\n",
    "\n",
    "This set of code will take the polling data from 538 and match them to their corresponding day's tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# load in the data\n",
    "prim_df = pd.read_csv('./data/national_primary_poll_average_2016.csv')\n",
    "# drop unncessary columns\n",
    "prim_df = prim_df[['lastname', 'poll_avg', 'forecastdate']]\n",
    "\n",
    "# reformat dates into a mm/dd/YY format and then rename columns for consistency\n",
    "prim_df['forecastdate'] = pd.to_datetime(prim_df.forecastdate)\n",
    "prim_df['forecastdate'] = prim_df['forecastdate'].dt.strftime('%m/%d/%Y')\n",
    "prim_df.columns = ['lastname', 'forecast', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prim_df.lastname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in the nat data and filter out for polls-only data\n",
    "nat_df = pd.read_csv('./data/national_topline.csv')\n",
    "nat_df = nat_df[nat_df.type == 'polls-only']\n",
    "\n",
    "# reformat dates into a mm/dd/YY format\n",
    "nat_df['forecastdate'] = pd.to_datetime(nat_df.forecastdate)\n",
    "nat_df['forecastdate'] = nat_df['forecastdate'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "# remove all but date and prediction score for trump and clinton\n",
    "nat_df = nat_df[['forecastdate', 'ecwin_clinton', 'ecwin_trump']]\n",
    "nat_df.head()\n",
    "\n",
    "# create separate dataframes for each candidate to make it easier to manipulate and combine with primary data\n",
    "clinton_df = nat_df[['forecastdate', 'ecwin_clinton']]\n",
    "trump_df = nat_df[['forecastdate', 'ecwin_trump']]\n",
    "\n",
    "# add a corresponding column for lastname to match primary prediction data format\n",
    "clinton_df['lastname'] = 'Clinton'\n",
    "trump_df['lastname'] = 'Trump'\n",
    "\n",
    "# rename and rearrange columns for consistency with primary data\n",
    "clinton_df.columns = ['date', 'forecast', 'lastname']\n",
    "trump_df.columns = ['date', 'forecast', 'lastname']\n",
    "clinton_df = clinton_df[['lastname', 'forecast', 'date']]\n",
    "trump_df = trump_df[['lastname', 'forecast', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append general election forecast data with primary election forecast data\n",
    "forecast_df = prim_df.append(clinton_df, ignore_index=True)\n",
    "forecast_df = forecast_df.append(trump_df, ignore_index=True)\n",
    "#forecast_df.to_csv('538_polling.csv') # export forecast to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_df.lastname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save csv's path and load it into a panda's dataframe\n",
    "path = '~/Dropbox/Summer_of_Tweets/Deduped_Tweets/deduped_tweets.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# duplicated the tweet created at columns to made a modify it to a MM/DD/YY format\n",
    "df['date'] = df['created_at']\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['date'] = df['date'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the cand names from the original sheet and make a parallel array of each cand's last name to match polling data\n",
    "cand = df.Candidate.unique()\n",
    "cand_lastname = ['Carson', 'Sanders', 'Jindal', 'Jindal', 'Fiorina', 'Fiorina', 'Christie', 'Christie', 'Sanders',\n",
    "                 'Pataki', 'Perry', 'Gilmore', 'Huckabee', 'Trump', 'Clinton', 'Clinton', 'Bush', 'Bush', 'Webb',\n",
    "                 'Kasich', 'Kasich', 'Lessig', 'Chaffee', 'Graham', 'Graham', 'Rubio', 'Rubio', \"O'Malley\", \"O'Malley\", \n",
    "                 'Huckabee', 'Sanders', 'Paul', 'Paul', 'Carson', 'Trump', 'Trump', 'Perry',\n",
    "                 'Santorum', 'Santorum', 'Walker', 'Walker', 'Cruz', 'Cruz']\n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "cand_to_lastname = {}\n",
    "for (cand, lastname) in zip(cand, cand_lastname):\n",
    "    cand_to_lastname[cand] = lastname\n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "lastname_to_cand = {}\n",
    "for (cand, lastname) in zip(cand_lastname, cand):\n",
    "    lastname_to_cand[cand] = lastname\n",
    "\n",
    "    \n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "lastname_to_cand = {}\n",
    "for (cand, lastname) in zip(cand_lastname, cand):\n",
    "    lastname_to_cand[cand] = lastname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Candidate.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the cand names from the original sheet and make a parallel array of each cand's last name to match polling data\n",
    "cand = df.Candidate.unique()\n",
    "\n",
    "cand_lastname = ['Carson', 'Sanders', 'Jindal', 'Jindal', 'Fiorina', 'Fiorina', 'Christie', 'Christie', 'Sanders',\n",
    "                 'Pataki', 'Perry', 'Gilmore', 'Huckabee', 'Trump', 'Clinton', 'Clinton', 'Bush', 'Bush', 'Webb',\n",
    "                 'Kasich', 'Kasich', 'Lessig', 'Chafee', 'Graham', 'Graham', 'Rubio', 'Rubio', \"O'Malley\", \"O'Malley\", \n",
    "                 'Huckabee', 'Sanders', 'Paul', 'Paul', 'Carson', 'Trump', 'Trump', 'Perry',\n",
    "                 'Santorum', 'Santorum', 'Walker', 'Walker', 'Cruz', 'Cruz']\n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "lastname_to_cand = {}\n",
    "for (cand, lastname) in zip(cand_lastname, cand):\n",
    "    lastname_to_cand[cand] = lastname\n",
    "\n",
    "lastname_to_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lastname_to_cand = {'Carson': 'Ben_Carson', 'Sanders': 'BernieSanders', 'Jindal': 'BobbyJindal', \n",
    "                    'Fiorina': 'CarlyFiorina', 'Christie':'ChrisChristie', 'Pataki': 'GovernorPataki',\n",
    "                    'Perry': 'GovernorPerry', 'Gilmore': 'gov_gilmore', 'Huckabee': 'GovMikeHuckabee',\n",
    "                    'Trump': 'realDonaldTrump', 'Clinton': 'Hillary_Clinton', 'Bush': 'JebBush', 'Webb':'JimWebbUSA', \n",
    "                    'Kasich': 'John_Kasich', 'Lessig': 'Lessig2016', 'Chafee': 'LincolnChafee',\n",
    "                    'Graham': 'Lindsey_Graham', 'Rubio': 'marcorubio', \"O'Malley\": 'MartinOMalley', 'Paul': 'RandPaul',\n",
    "                    'Perry': 'Rick_Perry','Santorum': 'RickSantorum', 'Walker': 'ScottWalker', 'Cruz': 'tedcruz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a parallel list of each cand's last name to be appeneded to the original dataframe\n",
    "lastname_col = []\n",
    "for cand in forecast_df.lastname:\n",
    "  lastname_col.append(lastname_to_cand[cand])\n",
    "  \n",
    "forecast_df['lastname'] = lastname_col\n",
    "\n",
    "forecast_df.to_csv('538_polling.csv')\n",
    "#forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a parallel list of each cand's last name to be appeneded to the original dataframe\n",
    "lastname_col = []\n",
    "for cand in df.Candidate:\n",
    "  lastname_col.append(cand_to_lastname[cand])\n",
    "  \n",
    "df['lastname'] = lastname_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_df.to_csv('538_polling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_df.lastname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df = df.merge(forecast_df, on=['date', 'lastname'])\n",
    "#polling_merged = merged_df.to_csv('polling_merged_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Lines to Pull and Update Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collect_data(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "#collect_data(path + pac_tweets, pac_sheet, path + tweet_list)\n",
    "\n",
    "collect_addition_data(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "#collect_addition_data(path + pac_tweets, pac_sheet, path + tweet_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
