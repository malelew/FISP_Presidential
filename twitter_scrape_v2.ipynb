{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# New FISP Presidential Project Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depreceated function from when data was saved to a Google sheet\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "def authenticate_gspread():\n",
    "  # scopes that your application should be granted access\n",
    "  scope = ['https://spreadsheets.google.com/feeds'] \n",
    "  # Create a Credentials object from the service account's credentials and the scopes\n",
    "  credentials = ServiceAccountCredentials.from_json_keyfile_name('auth.json', scope)\n",
    "  gc = gspread.authorize(credentials)\n",
    "  return gc\n",
    "  \n",
    "# gets the list of cand or pac and returns it in a list\n",
    "def gspread_get_lists(worksheet, is_cand):\n",
    "  names = filter(lambda x: len(x) > 0, worksheet.col_values(2))\n",
    "  max_ids = worksheet.col_values(3)[:len(names)]\n",
    "  counts = worksheet.col_values(4)[:len(names)]\n",
    "  indices = range(1,len(names)+1)\n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dropbox import DropboxOAuth2FlowNoRedirect\n",
    "def authenticate_dropbox():\n",
    "  auth_flow = DropboxOAuth2FlowNoRedirect(ac.APP_KEY, ac.APP_SECRET)\n",
    "  \n",
    "  authorize_url = auth_flow.start()\n",
    "  print \"1. Go to: \" + authorize_url\n",
    "  print \"2. Click \\\"Allow\\\" (you might have to log in first).\"\n",
    "  print \"3. Copy the authorization code.\"\n",
    "  auth_code = raw_input(\"Enter the authorization code here: \").strip()\n",
    "  \n",
    "  try:\n",
    "    oauth_result = auth_flow.finish(auth_code)\n",
    "  except Exception, e:\n",
    "    print ('Error: %s' % (e,))\n",
    "    return\n",
    "  \n",
    "  dbx = dropbox.Dropbox(oauth_result.access_token)\n",
    "  return dbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take the rows with multiple tweets checked and make an individual row for each tweets\n",
    "def expand_lists(df):\n",
    "  # create a list for each columns and a dict to later convert into an df\n",
    "  id_ = []\n",
    "  ratings = []\n",
    "  sources = []\n",
    "  tweets = {'id': id_, 'rating': ratings, 'source': sources}\n",
    "  \n",
    "  # loop thru each row and if tweet id is stored in a list then create df \n",
    "  # with each id in a separate row with its fact check data\n",
    "  for index, row in df.iterrows():\n",
    "    if (type(row[0]) == list):\n",
    "      for i in row[0]:\n",
    "        id_.append(i)\n",
    "        ratings.append(row[1])\n",
    "        sources.append(row[2])\n",
    "      # drop the row containing multiple tweets\n",
    "      df.drop(index, inplace=True)\n",
    "  # create new df with tweets in their own row, then append them to the original dataframe\n",
    "  new_df = pd.DataFrame(tweets)\n",
    "  df = df.append(new_df)\n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets â†“\n",
    "\n",
    "All the following functions write to excel or csv sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pull Func\n",
    "\n",
    "The following functin gets the most up to date tweets and writes them to the master excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_data(is_cand):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "   \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # Lessign has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False)\n",
    "  \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(100)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.to_excel(list_writer, sheet_name=sheetname, index=False)\n",
    "  tweet_writer.save()\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Tweet_List.xlsx\n"
     ]
    },
    {
     "ename": "BadZipfile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipfile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1f677763f8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-513329968d97>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(is_cand)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m# properly load spreadsheet to append new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mtweet_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sheets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtweet_sheet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;31m# loop through the list of Cand/PACs and updates each tweet sheet appropriately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-5cdeee5d587f>\u001b[0m in \u001b[0;36mload_sheets\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_sheets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0msheet_book\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0msheet_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'openpyxl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msheet_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msheet_book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msheet_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mws\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msheet_book\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworksheets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/reader/excel.pyc\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(filename, read_only, keep_vba, data_only, guess_types, keep_links)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mread_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/reader/excel.pyc\u001b[0m in \u001b[0;36m_validate_archive\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBadZipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepair_central_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/zipfile.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/zipfile.pyc\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipfile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "collect_data(True)\n",
    "collect_data(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued update of metadata\n",
    "\n",
    "A tweets ability to stay in the public discouse is dependent on the number of retweets and favorites. The initial pull of a tweet will not complete picture of the tweets effectiveness. This script allows us to continously update a tweet's metadata counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params: is_cand - determines whether to pull candidates tweets or PAC tweets\n",
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(is_cand):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():       \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # Lessig has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(path + tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df)\n",
    "    \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    #tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    \n",
    "    # 100 second pause between data pulls to voud token exceptions\n",
    "    time.sleep(20)\n",
    "  \n",
    "  # tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "def update_metadata(tweets_df, updates_df): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  # check for duplicates\n",
    "  dupe_df = tweets_df[tweets_df.index.duplicated()]\n",
    "  if (len(dupe_df) > 0):\n",
    "    print dupe_df\n",
    "  \n",
    "  ## loop through the updates metadata and updates the tweet sheet\n",
    "  #for row in updates_df.itertuples():\n",
    "  #  tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "  #  tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "#\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['text'], inplace=True)\n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Tweet_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets/Presidential_Tweets.xlsx\n",
      "INFO:__main__:Downloaded tweets list\n",
      "INFO:__main__:Retrived data from spreadsheet for BernieSanders\n",
      "INFO:__main__:Downloading 3205 tweets from BernieSanders\n",
      "INFO:__main__:Updated data on spreadsheet for BernieSanders\n",
      "INFO:__main__:Retrived data from spreadsheet for BobbyJindal\n",
      "INFO:__main__:Downloading 3224 tweets from BobbyJindal\n",
      "INFO:__main__:Updated data on spreadsheet for BobbyJindal\n",
      "INFO:__main__:Retrived data from spreadsheet for CarlyFiorina\n",
      "INFO:__main__:Downloading 3201 tweets from CarlyFiorina\n",
      "INFO:__main__:Updated data on spreadsheet for CarlyFiorina\n",
      "INFO:__main__:Retrived data from spreadsheet for ChrisChristie\n",
      "INFO:__main__:Downloading 2844 tweets from ChrisChristie\n",
      "INFO:__main__:Updated data on spreadsheet for ChrisChristie\n",
      "INFO:__main__:Retrived data from spreadsheet for gov_gilmore\n",
      "INFO:__main__:Downloading 2189 tweets from gov_gilmore\n",
      "INFO:__main__:Updated data on spreadsheet for gov_gilmore\n",
      "INFO:__main__:Retrived data from spreadsheet for GovernorPataki\n",
      "INFO:__main__:Downloading 2126 tweets from GovernorPataki\n",
      "INFO:__main__:Updated data on spreadsheet for GovernorPataki\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            created_at  \\\n",
      "id                                       \n",
      "591000000000000000 2015-04-23 14:03:20   \n",
      "\n",
      "                                                                 text  \\\n",
      "id                                                                      \n",
      "591000000000000000  RT @WMUR9: NHPrimarySource @marcorubio @Govern...   \n",
      "\n",
      "                             hashtag#  \\\n",
      "id                                      \n",
      "591000000000000000  #fitn #nhpolitics   \n",
      "\n",
      "                                                                  at@  \\\n",
      "id                                                                      \n",
      "591000000000000000  @WMUR9 @marcorubio @GovernorPataki @tedcruz @R...   \n",
      "\n",
      "                               link  retweets  favorites         full URL  \n",
      "id                                                                         \n",
      "591000000000000000  http://t.co/CM5       NaN        NaN  http://t.co/CM5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrived data from spreadsheet for GovernorPerry\n",
      "INFO:__main__:Downloading 3193 tweets from GovernorPerry\n",
      "INFO:__main__:Updated data on spreadsheet for GovernorPerry\n",
      "INFO:__main__:Retrived data from spreadsheet for GovMikeHuckabee\n",
      "INFO:__main__:Downloading 3202 tweets from GovMikeHuckabee\n",
      "INFO:__main__:Updated data on spreadsheet for GovMikeHuckabee\n",
      "INFO:__main__:Retrived data from spreadsheet for HillaryClinton\n",
      "INFO:__main__:Downloading 3235 tweets from HillaryClinton\n",
      "INFO:__main__:Updated data on spreadsheet for HillaryClinton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           created_at  \\\n",
      "id                                      \n",
      "7.16449003903e+17 2016-04-03 02:15:28   \n",
      "7.73484097159e+17 2016-09-07 11:32:34   \n",
      "7.73489172905e+17 2016-09-07 11:52:44   \n",
      "7.73520924608e+17 2016-09-07 13:58:54   \n",
      "7.73532014511e+17 2016-09-07 14:42:58   \n",
      "7.73545200808e+17 2016-09-07 15:35:22   \n",
      "7.73551774398e+17 2016-09-07 16:01:29   \n",
      "7.73560103354e+17 2016-09-07 16:34:35   \n",
      "7.73568795215e+17 2016-09-07 17:09:07   \n",
      "7.73573415488e+17 2016-09-07 17:27:29   \n",
      "7.73579721586e+17 2016-09-07 17:52:32   \n",
      "7.73600761775e+17 2016-09-07 19:16:09   \n",
      "7.73616523991e+17 2016-09-07 20:18:47   \n",
      "7.73627088683e+17 2016-09-07 21:00:45   \n",
      "7.73630924433e+17 2016-09-07 21:16:00   \n",
      "7.73648127367e+17 2016-09-07 22:24:21   \n",
      "7.73656536724e+17 2016-09-07 22:57:46   \n",
      "7.73660205163e+17 2016-09-07 23:12:21   \n",
      "7.73666123896e+17 2016-09-07 23:35:52   \n",
      "7.73668180355e+17 2016-09-07 23:44:02   \n",
      "7.73671107111e+17 2016-09-07 23:55:40   \n",
      "7.73673141323e+17 2016-09-08 00:03:45   \n",
      "7.73675333975e+17 2016-09-08 00:12:28   \n",
      "7.73676288624e+17 2016-09-08 00:16:16   \n",
      "7.73677916564e+17 2016-09-08 00:22:44   \n",
      "7.73678507826e+17 2016-09-08 00:25:05   \n",
      "7.73678952993e+17 2016-09-08 00:26:51   \n",
      "7.73679891435e+17 2016-09-08 00:30:35   \n",
      "7.73680224937e+17 2016-09-08 00:31:54   \n",
      "7.73681680398e+17 2016-09-08 00:37:41   \n",
      "7.73682724528e+17 2016-09-08 00:41:50   \n",
      "7.736832919e+17   2016-09-08 00:44:05   \n",
      "7.73684415055e+17 2016-09-08 00:48:33   \n",
      "7.73684718202e+17 2016-09-08 00:49:45   \n",
      "7.73685017042e+17 2016-09-08 00:50:57   \n",
      "\n",
      "                                                                text  \\\n",
      "id                                                                     \n",
      "7.16449003903e+17  Nueva York le dio una oportunidad a Hillary de...   \n",
      "7.73484097159e+17  \"I know more about ISIS than the generals do.\"...   \n",
      "7.73489172905e+17  RT @dallasnews: We recommend @HillaryClinton f...   \n",
      "7.73520924608e+17  World leaders on why Hillary's tenure as secre...   \n",
      "7.73532014511e+17  The @DallasNews hasn't endorsed a Democrat for...   \n",
      "7.73545200808e+17  When our SEALs took out bin Laden, they brough...   \n",
      "7.73551774398e+17  Donald Trump doesn't \"know why\" African Americ...   \n",
      "7.73560103354e+17  RT @Hillary_esp: \"Hillary tiene la experiencia...   \n",
      "7.73568795215e+17  We can't afford a president who would sabotage...   \n",
      "7.73573415488e+17  RT @HFA: 1. Download our app\\n2. Complete dail...   \n",
      "7.73579721586e+17  .@POTUS, @madeleine, and Leon Panetta agree: H...   \n",
      "7.73600761775e+17  Did Trump use his charitable foundation to pre...   \n",
      "7.73616523991e+17  The U.S. presidency isn't an entry-level job.\\...   \n",
      "7.73627088683e+17  A man who talks about our veterans and militar...   \n",
      "7.73630924433e+17  RT @timkaine: RT if you believe Donald Trump's...   \n",
      "7.73648127367e+17  A man you can bait with a tweet is not a man w...   \n",
      "7.73656536724e+17  Tonight, Americans will hear what Trump would ...   \n",
      "7.73660205163e+17  RT @TheBriefing2016: Trump's pay-to-play scand...   \n",
      "7.73666123896e+17  RT @HFA: \"We must be a force for unity in Amer...   \n",
      "7.73668180355e+17  RT @timkaine: A person who trash talks our own...   \n",
      "7.73671107111e+17  Donald Trump has proven over and over again th...   \n",
      "7.73673141323e+17  It takes experience, toughness, and a steady t...   \n",
      "7.73675333975e+17  Our next Commander-in-Chief needs to have the ...   \n",
      "7.73676288624e+17  Hillary has spent decades fighting for veteran...   \n",
      "7.73677916564e+17  \"I will not let the VA be privatized.\" Hillary...   \n",
      "7.73678507826e+17  We need to do everything we can to remove barr...   \n",
      "7.73678952993e+17  RT @TheBriefing2016: Unlike her opponent, Hill...   \n",
      "7.73679891435e+17  Not every wound can be seen. Our veterans dese...   \n",
      "7.73680224937e+17  Tonight, Trump is going to try and claim he's ...   \n",
      "7.73681680398e+17  RT @BuzzFeed: In 2002, Donald Trump said he su...   \n",
      "7.73682724528e+17  I have a very good brain. Donald Trump on his ...   \n",
      "7.736832919e+17    RT @Hillary_esp: No todas las heridas son visi...   \n",
      "7.73684415055e+17  The worst part is, this isnt the first time Do...   \n",
      "7.73684718202e+17  RT @HillaryClinton: A man who talks about our ...   \n",
      "7.73685017042e+17  RT @TheBriefing2016: Donald Trump's super secr...   \n",
      "\n",
      "                        hashtag#                          at@  \\\n",
      "id                                                              \n",
      "7.16449003903e+17            NaN                          NaN   \n",
      "7.73484097159e+17            NaN                          NaN   \n",
      "7.73489172905e+17            NaN  @dallasnews @HillaryClinton   \n",
      "7.73520924608e+17            NaN                          NaN   \n",
      "7.73532014511e+17            NaN                  @DallasNews   \n",
      "7.73545200808e+17            NaN                          NaN   \n",
      "7.73551774398e+17            NaN                          NaN   \n",
      "7.73560103354e+17            NaN                 @Hillary_esp   \n",
      "7.73568795215e+17            NaN                          NaN   \n",
      "7.73573415488e+17            NaN                         @HFA   \n",
      "7.73579721586e+17            NaN                   @madeleine   \n",
      "7.73600761775e+17            NaN                          NaN   \n",
      "7.73616523991e+17            NaN                          NaN   \n",
      "7.73627088683e+17            NaN                          NaN   \n",
      "7.73630924433e+17            NaN                    @timkaine   \n",
      "7.73648127367e+17            NaN                          NaN   \n",
      "7.73656536724e+17            NaN                          NaN   \n",
      "7.73660205163e+17            NaN             @TheBriefing2016   \n",
      "7.73666123896e+17            NaN                         @HFA   \n",
      "7.73668180355e+17            NaN                    @timkaine   \n",
      "7.73671107111e+17            NaN                          NaN   \n",
      "7.73673141323e+17  #NBCNewsForum                          NaN   \n",
      "7.73675333975e+17            NaN                          NaN   \n",
      "7.73676288624e+17  #NBCNewsForum                          NaN   \n",
      "7.73677916564e+17  #NBCNewsForum                          NaN   \n",
      "7.73678507826e+17  #NBCNewsForum                          NaN   \n",
      "7.73678952993e+17  #NBCNewsForum             @TheBriefing2016   \n",
      "7.73679891435e+17  #NBCNewsForum                          NaN   \n",
      "7.73680224937e+17            NaN             @TheBriefing2016   \n",
      "7.73681680398e+17            NaN                    @BuzzFeed   \n",
      "7.73682724528e+17            NaN                          NaN   \n",
      "7.736832919e+17    #NBCNewsForum                 @Hillary_esp   \n",
      "7.73684415055e+17  #NBCNewsForum                          NaN   \n",
      "7.73684718202e+17            NaN              @HillaryClinton   \n",
      "7.73685017042e+17  #NBCNewsForum             @TheBriefing2016   \n",
      "\n",
      "                                                              link  retweets  \\\n",
      "id                                                                             \n",
      "7.16449003903e+17                          https://t.co/odn7GITEdL       NaN   \n",
      "7.73484097159e+17                          https://t.co/6tFKlsqyyk    3241.0   \n",
      "7.73489172905e+17  https://t.co/eWtGJexQqz https://t.co/CazHEkM2pX    4612.0   \n",
      "7.73520924608e+17                          https://t.co/ROD8Jh50aV    3270.0   \n",
      "7.73532014511e+17                          https://t.co/lMhufMDjFk    2341.0   \n",
      "7.73545200808e+17                          https://t.co/QV5zDf2iAg    1646.0   \n",
      "7.73551774398e+17                          https://t.co/IY4xocgveh    1976.0   \n",
      "7.73560103354e+17                                      https://t.c     595.0   \n",
      "7.73568795215e+17                          https://t.co/bvya51W44z    1793.0   \n",
      "7.73573415488e+17                          https://t.co/neivbRsKF8     287.0   \n",
      "7.73579721586e+17                          https://t.co/UV6XBW7GCM    1323.0   \n",
      "7.73600761775e+17                          https://t.co/MZdZDzimY4    2031.0   \n",
      "7.73616523991e+17                          https://t.co/6akMEX6pRs    3429.0   \n",
      "7.73627088683e+17                          https://t.co/4s6SvAAyNA    4052.0   \n",
      "7.73630924433e+17                          https://t.co/VcLIIDNQdJ    4851.0   \n",
      "7.73648127367e+17                          https://t.co/qZ3Z10cqKN    4343.0   \n",
      "7.73656536724e+17                          https://t.co/1SfNIW8Y3Q    1091.0   \n",
      "7.73660205163e+17                                          https:/    1746.0   \n",
      "7.73666123896e+17                            https://t.co/AyI3H0mr     401.0   \n",
      "7.73668180355e+17                          https://t.co/h04p4QF1xz    2808.0   \n",
      "7.73671107111e+17                          https://t.co/uLoHJdzXte    2164.0   \n",
      "7.73673141323e+17                          https://t.co/d5m0pXRMQG    2130.0   \n",
      "7.73675333975e+17                          https://t.co/UV6XBW7GCM    1101.0   \n",
      "7.73676288624e+17                          https://t.co/GJ0NmH6hYn    1374.0   \n",
      "7.73677916564e+17                          https://t.co/FRkf6AnS2f     948.0   \n",
      "7.73678507826e+17                                              NaN    1251.0   \n",
      "7.73678952993e+17                          https://t.co/oD3OtTss9z     720.0   \n",
      "7.73679891435e+17                          https://t.co/vHoPy09BvC    3368.0   \n",
      "7.73680224937e+17                                              NaN    1080.0   \n",
      "7.73681680398e+17  https://t.co/B3071OFSQy https://t.co/On1ZfL88eC    1457.0   \n",
      "7.73682724528e+17                          https://t.co/48MePwjHh9    1946.0   \n",
      "7.736832919e+17                                              https     487.0   \n",
      "7.73684415055e+17                          https://t.co/N6ySNILYMx    3849.0   \n",
      "7.73684718202e+17                                        https://t    4052.0   \n",
      "7.73685017042e+17                          https://t.co/SBbCVgeaQD     830.0   \n",
      "\n",
      "                   favorites  \n",
      "id                            \n",
      "7.16449003903e+17        NaN  \n",
      "7.73484097159e+17     7329.0  \n",
      "7.73489172905e+17        0.0  \n",
      "7.73520924608e+17     6806.0  \n",
      "7.73532014511e+17     5080.0  \n",
      "7.73545200808e+17     4007.0  \n",
      "7.73551774398e+17     3711.0  \n",
      "7.73560103354e+17        0.0  \n",
      "7.73568795215e+17     4250.0  \n",
      "7.73573415488e+17        0.0  \n",
      "7.73579721586e+17     3330.0  \n",
      "7.73600761775e+17     3479.0  \n",
      "7.73616523991e+17     8089.0  \n",
      "7.73627088683e+17     8769.0  \n",
      "7.73630924433e+17        0.0  \n",
      "7.73648127367e+17     8103.0  \n",
      "7.73656536724e+17     2250.0  \n",
      "7.73660205163e+17        0.0  \n",
      "7.73666123896e+17        0.0  \n",
      "7.73668180355e+17        0.0  \n",
      "7.73671107111e+17     4892.0  \n",
      "7.73673141323e+17     6504.0  \n",
      "7.73675333975e+17     3044.0  \n",
      "7.73676288624e+17     2857.0  \n",
      "7.73677916564e+17     2343.0  \n",
      "7.73678507826e+17     3780.0  \n",
      "7.73678952993e+17        0.0  \n",
      "7.73679891435e+17     9074.0  \n",
      "7.73680224937e+17     3432.0  \n",
      "7.73681680398e+17        0.0  \n",
      "7.73682724528e+17     3550.0  \n",
      "7.736832919e+17          0.0  \n",
      "7.73684415055e+17     5954.0  \n",
      "7.73684718202e+17        0.0  \n",
      "7.73685017042e+17        0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrived data from spreadsheet for JebBush\n",
      "INFO:__main__:Downloading 3238 tweets from JebBush\n",
      "INFO:__main__:Updated data on spreadsheet for JebBush\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            created_at  \\\n",
      "id                                       \n",
      "752973911269122048 2016-07-12 21:12:24   \n",
      "753023205720358912 2016-07-13 00:28:17   \n",
      "753724387828772865 2016-07-14 22:54:32   \n",
      "753952442950164480 2016-07-15 14:00:44   \n",
      "754103997800198145 2016-07-16 00:02:58   \n",
      "754462142133174272 2016-07-16 23:46:06   \n",
      "754805564903796736 2016-07-17 22:30:44   \n",
      "755534158231855104 2016-07-19 22:45:55   \n",
      "755791099726225408 2016-07-20 15:46:54   \n",
      "756953724296146945 2016-07-23 20:46:46   \n",
      "885969842410471424 2017-07-14 21:10:45   \n",
      "885983151935307776 2017-07-14 22:03:38   \n",
      "886908029869973504 2017-07-17 11:18:46   \n",
      "886999179129749504 2017-07-17 17:20:58   \n",
      "\n",
      "                                                                 text  \\\n",
      "id                                                                      \n",
      "752973911269122048  RT @TheBushCenter: Remarks by President George...   \n",
      "753023205720358912  Outrageous. The dance continues, domestically ...   \n",
      "753724387828772865  Good new study from @AEI on policies to reduce...   \n",
      "753952442950164480  Heart breaks for victims and families of #Nice...   \n",
      "754103997800198145  Mike Pence is a good man. He will add value to...   \n",
      "754462142133174272  .@DevinNunes is right. @WSJopinion: This Trait...   \n",
      "754805564903796736  My piece in today's @washingtonpost on the fut...   \n",
      "755534158231855104  In address to @AFTunion, @HillaryClinton again...   \n",
      "755791099726225408  Venezuela Nightmare: A 30-Day Hunt for Food in...   \n",
      "756953724296146945  We filed an amicus brief with the NV Supreme C...   \n",
      "885969842410471424  Looking forward to #OZYFEST on July 22 LIVE in...   \n",
      "885983151935307776  My kind of fan. #MarlinsMan https://t.co/87KOV...   \n",
      "886908029869973504  How Cuba Runs Venezuela   Maduro needs to go. ...   \n",
      "886999179129749504  RT @GeorgeHWBush: Very pleased @PointsofLight ...   \n",
      "\n",
      "                     hashtag#                        at@  \\\n",
      "id                                                         \n",
      "752973911269122048        NaN             @TheBushCenter   \n",
      "753023205720358912        NaN                        NaN   \n",
      "753724387828772865        NaN                       @AEI   \n",
      "753952442950164480      #Nice                        NaN   \n",
      "754103997800198145        NaN                        NaN   \n",
      "754462142133174272        NaN                @WSJopinion   \n",
      "754805564903796736        NaN            @washingtonpost   \n",
      "755534158231855104        NaN  @AFTunion @HillaryClinton   \n",
      "755791099726225408        NaN                        NaN   \n",
      "756953724296146945  #edchoice                        NaN   \n",
      "885969842410471424        NaN                        NaN   \n",
      "885983151935307776        NaN                        NaN   \n",
      "886908029869973504        NaN                        NaN   \n",
      "886999179129749504        NaN                        NaN   \n",
      "\n",
      "                                       link  retweets  favorites full URL  \n",
      "id                                                                         \n",
      "752973911269122048  https://t.co/wnQx45b41S     127.0        0.0      NaN  \n",
      "753023205720358912  https://t.co/kwt5F9hzbR     134.0      205.0      NaN  \n",
      "753724387828772865  https://t.co/NmXUV4i5u1      40.0      148.0      NaN  \n",
      "753952442950164480                      NaN     162.0      641.0      NaN  \n",
      "754103997800198145                      NaN     458.0     1472.0      NaN  \n",
      "754462142133174272  https://t.co/TaYQy8g2RI      56.0      117.0      NaN  \n",
      "754805564903796736  https://t.co/PndaqICznP     454.0      876.0      NaN  \n",
      "755534158231855104  https://t.co/CKGMLEqATH     155.0      222.0      NaN  \n",
      "755791099726225408  https://t.co/f4VqcZ5H97     186.0      204.0      NaN  \n",
      "756953724296146945  https://t.co/tlcjac7VUM      62.0      192.0      NaN  \n",
      "885969842410471424                      NaN      31.0      175.0      NaN  \n",
      "885983151935307776                      NaN     202.0     1102.0      NaN  \n",
      "886908029869973504                      NaN      96.0      212.0      NaN  \n",
      "886999179129749504                      NaN      39.0        0.0      NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrived data from spreadsheet for JimWebbUSA\n",
      "INFO:__main__:Downloading 1942 tweets from JimWebbUSA\n",
      "INFO:__main__:Updated data on spreadsheet for JimWebbUSA\n",
      "INFO:__main__:Retrived data from spreadsheet for JohnKasich\n",
      "INFO:__main__:Downloading 3246 tweets from JohnKasich\n",
      "INFO:__main__:Updated data on spreadsheet for JohnKasich\n",
      "INFO:__main__:Retrived data from spreadsheet for LincolnChafee\n",
      "INFO:__main__:Downloading 3177 tweets from LincolnChafee\n",
      "INFO:__main__:Updated data on spreadsheet for LincolnChafee\n",
      "INFO:__main__:Retrived data from spreadsheet for LindseyGrahamSC\n",
      "INFO:__main__:Downloading 3207 tweets from LindseyGrahamSC\n",
      "INFO:__main__:Updated data on spreadsheet for LindseyGrahamSC\n",
      "INFO:__main__:Retrived data from spreadsheet for marcorubio\n"
     ]
    },
    {
     "ename": "TweepError",
     "evalue": "Failed to parse JSON payload: Unterminated string starting at: line 1 column 504536 (char 504535)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e82dc2583137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollect_addition_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# collect_addition_data(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-f5bae05d6548>\u001b[0m in \u001b[0;36mcollect_addition_data\u001b[0;34m(is_cand)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# retreive updated tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_new_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mupdates_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-58ec7c3b591b>\u001b[0m in \u001b[0;36mget_new_tweets\u001b[0;34m(tweet_name, since_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmax_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tweets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnew_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_timeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msince_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msince_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmax_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/tweepy/binder.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/tweepy/binder.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m# Store result into cache if one is available.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/tweepy/parsers.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, method, payload)\u001b[0m\n\u001b[1;32m     93\u001b[0m                              '%s' % method.payload_type)\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mjson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/tweepy/parsers.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, method, payload)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mjson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to parse JSON payload: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mneeds_cursors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cursor'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTweepError\u001b[0m: Failed to parse JSON payload: Unterminated string starting at: line 1 column 504536 (char 504535)"
     ]
    }
   ],
   "source": [
    "collect_addition_data(True)\n",
    "# collect_addition_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_full_url(short_urls, full_urls):\n",
    "for i, us in enumerate(short_urls):\n",
    "full = []\n",
    "  if not us.startswith(\"http\"):\n",
    "    continue\n",
    "  for url in us.split(\" \"):\n",
    "    if not url.startswith(\"http\"):\n",
    "      continue\n",
    "    try:\n",
    "      r = requests.head(url, allow_redirects=True)\n",
    "      full.append(r.url)\n",
    "    except:\n",
    "      logger.info(\"Error occurred for URL - %s\" % url)\n",
    "      continue\n",
    "  if i % 500 == 0:\n",
    "      logger.info(\"Extracting URL %d/%d\" % (i, len(short_urls)))\n",
    "      time.sleep(60)\n",
    "  full_urls[i] = \" \".join(full)q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_full_url(is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "  \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(path + tweet_list)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(path + tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "    \n",
    "  logger.info(\"Successfully download the list...\")\n",
    "  for e, entry in enumerate(list_df):\n",
    "    if e < 15:\n",
    "      continue\n",
    "\n",
    "    name, since_id, count, index = entry[0], entry[1],entry[2], entry[3]\n",
    "\n",
    "    short_urls = worksheet.col_values(6)\n",
    "    logger.info(\"Downloaded %s URL\", name)\n",
    "    url_datas = ['' for i in xrange(len(short_urls))]\n",
    "    url_datas[0] = 'full URL'\n",
    "\n",
    "    get_full_url(short_urls, url_datas) # transfer short url to full urls and store in url_datas\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    while count < len(short_urls):\n",
    "      amount = min(100, len(short_urls) - count)\n",
    "      cells = worksheet.range('I'+str(count)+':'+'I'+str(count+amount-1))\n",
    "      assert(len(cells) == amount)\n",
    "      for i in range(amount):\n",
    "        cells[i].value = url_datas[count-1]\n",
    "        count += 1\n",
    "      worksheet.update_cells(cells)\n",
    "      logger.info(\"Update cells %d/%d for %s\" %(count, len(short_urls), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_full_url(True)\n",
    "#update_full_url(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaPo Fact Checking\n",
    "The cell below collects fact checks from the Washington Post's '2016 Election Fact Checker' and 'RealDonaldContext' chrome extension. The election fact checker data was hand collected and is stored in a json file while the extension data is pulled directly from the online hosted json file from the extension's developer blog.\n",
    "They are collected into an single dataframe consisting of the tweet id, rating, and source. They are then merged with a master sheet using tweet id.\n",
    "\n",
    "['2016 Election Fact Checker'](https://www.washingtonpost.com/graphics/politics/2016-election/fact-checker/)\n",
    "\n",
    "['RealDonaldContext'](https://chrome.google.com/webstore/detail/realdonaldcontext/ddbkmnomngnlcdglabflidgmhmcafogn?hl=en-US)\n",
    "\n",
    "['RealDonaldContext json file'](https://www.pbump.net/files/post/extension/core/data.php)\n",
    "\n",
    "['Rating System Scale'](https://www.washingtonpost.com/news/fact-checker/about-the-fact-checker/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code is from the fact checking portion of this project. It grabs the fact checked tweets from\n",
    "# the WaPo Trump tweet fact checking extension and adds the ratings to correspoding tweets in the spreadsheet\n",
    "\n",
    "# sheetnames\n",
    "trump_sheet = 'realDonaldTrump'\n",
    "potus_sheet = 'POTUS'\n",
    "\n",
    "logger.info(\"Start...\")\n",
    "\n",
    "# read in WaPo fact checks of Donald Trump from the WaPo Trump tweet chrome extension\n",
    "trump_check = pd.read_json('https://www.pbump.net/files/post/extension/core/data.php')\n",
    "# rename columns and remove text columns\n",
    "trump_check.columns = ['id', 'rating', 'tweet', 'source']\n",
    "trump_check = trump_check[['id', 'rating', 'source']]\n",
    "# call expand lists to turn fact checks of multiple tweets into multiple columns\n",
    "trump_check = expand_lists(trump_check)\n",
    "\n",
    "# load pre-election fact checks and filter for just id, rating, and source\n",
    "election_checks = pd.read_json('preelection_wapo.json')\n",
    "election_checks = election_checks[['id', 'rating', 'source']]\n",
    "\n",
    "# append the hand collected data with the data collected from the extension\n",
    "trump_check = trump_check.append(election_checks, ignore_index=True)\n",
    "trump_check.columns = ['id', 'WAPO_RATING', 'WAPO_SOURCE']\n",
    "logger.info(\"read in fact checks\")\n",
    "\n",
    "# set file pathway variables an expand to HOME\n",
    "in_path = '~/Dropbox/Summer_of_Tweets/fact_checking/Presidential_Fact_Checking.xlsx'\n",
    "in_path = os.path.expanduser(in_path)\n",
    "\n",
    "# properly load spreadsheet to append new data\n",
    "work_book = load_workbook(in_path)\n",
    "tweet_writer = pd.ExcelWriter(in_path, engine='openpyxl')\n",
    "tweet_writer.book = work_book\n",
    "tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)\n",
    "tweets_df = pd.read_excel(in_path, sheetname=trump_sheet, dtype={'id': str})\n",
    "logger.info(\"Downloaded excel sheets list\")\n",
    "\n",
    "# change data type to match excel sheet's\n",
    "trump_check['id'] = trump_check['id'].astype(str)\n",
    "#merge the fact check data set with the tweets set using tweet id\n",
    "merged_df = tweets_df.merge(trump_check, on='id', how='left')\n",
    "\n",
    "logger.info(merged_df.shape) # used for debugging\n",
    "# write merged data to the excel sheet\n",
    "merged_df.to_excel(tweet_writer, sheet_name=trump_sheet, index=False)\n",
    "tweet_writer.save()\n",
    "\n",
    "# merged_df.to_csv('WaPo.csv', encoding='utf-8') # used for viewing test results\n",
    "\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follower Growth\n",
    "The follower growth for Hillary Clinton and Donald Trump is being collected for the project. After some research on which sites are best for follower growth data, [Trackalytics](http://www.trackalytics.com) is the best free resource for tracking follower growth. However it does not have comprehensive follower growth data for the rest of the candidates, the others either are not present on the site or their data starts to get collected well into the election cycle.\n",
    "\n",
    "The data is scraped using the IMPORTHTML function in google sheets. Information on the function and how to use it can be found [here](http://lenagroeger.s3.amazonaws.com/talks/orlando/gettingdata.html)  while the sheet itself can be found [here](https://docs.google.com/spreadsheets/d/1rahomcsDJFf_za0S_Tbzi1kv79bdNM2ZqNZ_H7XcMIM/edit?usp=sharing). \n",
    "\n",
    "The following function runs to clean the data sheet to move daily delta in followers into its own column and then downloading and moving the sheet onto the FISP dropbox.\n",
    "\n",
    "Currently stuck trying to implement this [method](https://github.com/davidasboth/blog-notebooks/blob/master/connecting-to-google-sheets/Connecting%20to%20a%20Google%20Sheet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_cand_followers ():\n",
    "  logger.info(\"Start...\")\n",
    "  #gc = authenticate_gspread()\n",
    "  \n",
    "  csv_url = \"{}/export?gid=0&format=csv\"\\\n",
    "    .format(\"https://docs.google.com/spreadsheets/d/1rahomcsDJFf_za0S_Tbzi1kv79bdNM2ZqNZ_H7XcMIM\")\n",
    "  \n",
    "  clinton_growth_df = pd.read_excel(csv_url, 0)\n",
    "  #trump_growth_df = pd.read_excel(ac.follower_sheet_url, 1)\n",
    "  \n",
    "  # clinton_growth_df = pd.read_excel(ac.follower_sheet_url, 0)\n",
    "  # trump_growth_df = pd.read_excel(ac.follower_sheet_url, 1)\n",
    "  \n",
    "  clinton_growth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_cand_followers ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polling Data Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of code will take the polling data from 538 and match them to their corresponding day's tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "prim_df = pd.read_csv('national_primary_poll_average_2016.csv')\n",
    "# drop unncessary columns\n",
    "prim_df = prim_df[['lastname', 'poll_avg', 'forecastdate']]\n",
    "\n",
    "# reformat dates into a mm/dd/YY format and then rename columns for consistency\n",
    "prim_df['forecastdate'] = pd.to_datetime(prim_df.forecastdate)\n",
    "prim_df['forecastdate'] = prim_df['forecastdate'].dt.strftime('%m/%d/%Y')\n",
    "prim_df.columns = ['lastname', 'forecast', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sanders', 'Clinton', 'Trump', 'Kasich', 'Cruz', 'Rubio', 'Carson',\n",
       "       'Bush', 'Fiorina', 'Christie', 'Santorum', 'Paul', \"O'Malley\",\n",
       "       'Huckabee', 'Pataki', 'Graham', 'Jindal', 'Lessig', 'Chafee',\n",
       "       'Webb', 'Walker', 'Perry'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_df.lastname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the nat data and filter out for polls-only data\n",
    "nat_df = pd.read_csv('national_topline.csv')\n",
    "nat_df = nat_df[nat_df.type == 'polls-only']\n",
    "\n",
    "# reformat dates into a mm/dd/YY format\n",
    "nat_df['forecastdate'] = pd.to_datetime(nat_df.forecastdate)\n",
    "nat_df['forecastdate'] = nat_df['forecastdate'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "# remove all but date and prediction score for trump and clinton\n",
    "nat_df = nat_df[['forecastdate', 'ecwin_clinton', 'ecwin_trump']]\n",
    "nat_df.head()\n",
    "\n",
    "# create separate dataframes for each candidate to make it easier to manipulate and combine with primary data\n",
    "clinton_df = nat_df[['forecastdate', 'ecwin_clinton']]\n",
    "trump_df = nat_df[['forecastdate', 'ecwin_trump']]\n",
    "\n",
    "# add a corresponding column for lastname to match primary prediction data format\n",
    "clinton_df['lastname'] = 'Clinton'\n",
    "trump_df['lastname'] = 'Trump'\n",
    "\n",
    "# rename and rearrange columns for consistency with primary data\n",
    "clinton_df.columns = ['date', 'forecast', 'lastname']\n",
    "trump_df.columns = ['date', 'forecast', 'lastname']\n",
    "clinton_df = clinton_df[['lastname', 'forecast', 'date']]\n",
    "trump_df = trump_df[['lastname', 'forecast', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append general election forecast data with primary election forecast data\n",
    "forecast_df = prim_df.append(clinton_df, ignore_index=True)\n",
    "forecast_df = forecast_df.append(trump_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv's path and load it into a panda's dataframe\n",
    "path = '~/Dropbox/Summer_of_Tweets/Deduped_Tweets/deduped_tweets.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# duplicated the tweet created at columns to made a modify it to a MM/DD/YY format\n",
    "df['date'] = df['created_at']\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['date'] = df['date'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the cand names from the original sheet and make a parallel array of each cand's last name to match polling data\n",
    "cand = df.Candidate.unique()\n",
    "cand_lastname = ['Carson', 'Sanders', 'Jindal', 'Jindal', 'Fiorina', 'Fiorina', 'Christie', 'Christie', 'Sanders',\n",
    "                 'Pataki', 'Perry', 'Gilmore', 'Huckabee', 'Trump', 'Clinton', 'Clinton', 'Bush', 'Bush', 'Webb',\n",
    "                 'Kasich', 'Kasich', 'Lessig', 'Chaffee', 'Graham', 'Graham', 'Rubio', 'Rubio', \"O'Malley\", \"O'Malley\", \n",
    "                 'Huckabee', 'Sanders', 'Paul', 'Paul', 'Carson', 'Trump', 'Trump', 'Perry',\n",
    "                 'Santorum', 'Santorum', 'Walker', 'Walker', 'Cruz', 'Cruz']\n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "cand_to_lastname = {}\n",
    "for (cand, lastname) in zip(cand, cand_lastname):\n",
    "    cand_to_lastname[cand] = lastname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a parallel list of each cand's last name to be appeneded to the original dataframe\n",
    "lastname_col = []\n",
    "for cand in df.Candidate:\n",
    "  lastname_col.append(cand_to_lastname[cand])\n",
    "  \n",
    "df['lastname'] = lastname_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(forecast_df, on=['date', 'lastname'])\n",
    "polling_merged = merged_df.to_csv('polling_merged_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99032, 48)\n"
     ]
    }
   ],
   "source": [
    "print merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp function to properly sort the tweets by date in ascending order\n",
    "def sort_sheet (is_cand):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "      \n",
    "  # set file pathway variables an expand to HOME\n",
    "  path = '~/Dropbox/Summer_of_Tweets/working_sheets/'\n",
    "  tweet_list = \"Tweet_List.xlsx\"\n",
    "  cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "  pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "  path = os.path.expanduser(path)\n",
    "      \n",
    "  # set sheet var to either pres or PAC\n",
    "  if(is_cand):\n",
    "    tweet_sheet = cand_tweets\n",
    "    sheetname = 'candidate'\n",
    "  else:\n",
    "    tweet_sheet = pac_tweets\n",
    "    sheetname = 'pac'\n",
    "        \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_book = load_workbook(path + tweet_list)\n",
    "  list_writer = pd.ExcelWriter(path + tweet_list, engine='openpyxl')\n",
    "  list_writer.book = list_book\n",
    "  list_writer.sheets = dict((ws.title, ws) for ws in list_book.worksheets)\n",
    "  list_df = pd.read_excel(path + tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  work_book = load_workbook(path + tweet_sheet)\n",
    "  tweet_writer = pd.ExcelWriter(path + tweet_sheet, engine='openpyxl')\n",
    "  tweet_writer.book = work_book\n",
    "  tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)    \n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "       \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    tweets_df = pd.read_excel(path + tweet_sheet, sheetname=name)\n",
    "    \n",
    "    tweets_df = tweets_df.sort_values('id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
