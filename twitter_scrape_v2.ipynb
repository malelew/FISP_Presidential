{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FISP Presidential Project Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "#import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import tweepy\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "from openpyxl import load_workbook\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Twitter API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreceated function from when data was saved to a Google sheet\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "def authenticate_gspread():\n",
    "  # scopes that your application should be granted access\n",
    "  scope = ['https://spreadsheets.google.com/feeds'] \n",
    "  # Create a Credentials object from the service account's credentials and the scopes\n",
    "  credentials = ServiceAccountCredentials.from_json_keyfile_name('auth.json', scope)\n",
    "  gc = gspread.authorize(credentials)\n",
    "  return gc\n",
    "  \n",
    "# gets the list of cand or pac and returns it in a list\n",
    "def gspread_get_lists(worksheet, is_cand):\n",
    "  names = filter(lambda x: len(x) > 0, worksheet.col_values(2))\n",
    "  max_ids = worksheet.col_values(3)[:len(names)]\n",
    "  counts = worksheet.col_values(4)[:len(names)]\n",
    "  indices = range(1,len(names)+1)\n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dropbox import DropboxOAuth2FlowNoRedirect\n",
    "def authenticate_dropbox():\n",
    "  auth_flow = DropboxOAuth2FlowNoRedirect(ac.APP_KEY, ac.APP_SECRET)\n",
    "  \n",
    "  authorize_url = auth_flow.start()\n",
    "  print \"1. Go to: \" + authorize_url\n",
    "  print \"2. Click \\\"Allow\\\" (you might have to log in first).\"\n",
    "  print \"3. Copy the authorization code.\"\n",
    "  auth_code = raw_input(\"Enter the authorization code here: \").strip()\n",
    "  \n",
    "  try:\n",
    "    oauth_result = auth_flow.finish(auth_code)\n",
    "  except Exception, e:\n",
    "    print ('Error: %s' % (e,))\n",
    "    return\n",
    "  \n",
    "  dbx = dropbox.Dropbox(oauth_result.access_token)\n",
    "  return dbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the rows with multiple tweets checked and make an individual row for each tweets\n",
    "def expand_lists(df):\n",
    "  # create a list for each columns and a dict to later convert into an df\n",
    "  id_ = []\n",
    "  ratings = []\n",
    "  sources = []\n",
    "  tweets = {'id': id_, 'rating': ratings, 'source': sources}\n",
    "  \n",
    "  # loop thru each row and if tweet id is stored in a list then create df \n",
    "  # with each id in a separate row with its fact check data\n",
    "  for index, row in df.iterrows():\n",
    "    if (type(row[0]) == list):\n",
    "      for i in row[0]:\n",
    "        id_.append(i)\n",
    "        ratings.append(row[1])\n",
    "        sources.append(row[2])\n",
    "      # drop the row containing multiple tweets\n",
    "      df.drop(index, inplace=True)\n",
    "  # create new df with tweets in their own row, then append them to the original dataframe\n",
    "  new_df = pd.DataFrame(tweets)\n",
    "  df = df.append(new_df)\n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupe_check(tweets_df, cand_name):\n",
    "  # check for duplicates\n",
    "  dupe_df = tweets_df[tweets_df.id.duplicated()]\n",
    "  \n",
    "  # if there exists a duplicate save it to a csv\n",
    "  if (len(dupe_df) > 0):\n",
    "    # make sure that the dupe_folder dir exists\n",
    "    dupe_df.to_csv('./dupe_folder/' + cand_name + '.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp function to properly sort the tweets by date in ascending order\n",
    "def sort_sheet (tweet_sheet, sheetname, tweet_list):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "        \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_book = load_workbook(tweet_list)\n",
    "  list_writer = pd.ExcelWriter(tweet_list, engine='openpyxl')\n",
    "  list_writer.book = list_book\n",
    "  list_writer.sheets = dict((ws.title, ws) for ws in list_book.worksheets)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  work_book = load_workbook(tweet_sheet)\n",
    "  tweet_writer = pd.ExcelWriter(tweet_sheet, engine='openpyxl')\n",
    "  tweet_writer.book = work_book\n",
    "  tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)    \n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "       \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    \n",
    "    tweets_df = tweets_df.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_to_lastname(handle_df, handle_col, lastname_dict):\n",
    "  # make a parallel list of each cand's last name to be appeneded to the original dataframe\n",
    "  lastname_col = []\n",
    "  for handle in handle_df[handle_col]:\n",
    "    lastname_col.append(lastname_dict[handle])\n",
    "    \n",
    "  handle_df['lastname'] = lastname_col\n",
    "  return handle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file pathway variables an expand to HOME\n",
    "path = '~/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/'\n",
    "tweet_list = \"Tweet_List.xlsx\"\n",
    "cand_tweets = \"Presidential_Tweets.xlsx\"\n",
    "pac_tweets = \"PAC_Tweets.xlsx\"\n",
    "path = os.path.expanduser(path)\n",
    "\n",
    "# sheetnames\n",
    "cand_sheet = 'candidate'\n",
    "pac_sheet = 'pac'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets ↓\n",
    "\n",
    "All the following functions write to excel or csv sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pull Func\n",
    "\n",
    "The following functin gets the most up to date tweets and writes them to the master excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(tweet_sheet, sheetname, tweet_list):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheet_name=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "   \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # Lessign has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    # grab new tweets since last id and save it to a dataframe\n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    \n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False)\n",
    "  \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(100)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.to_excel(list_writer, sheet_name=sheetname, index=False)\n",
    "  tweet_writer.save()\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/Tweet_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/Presidential_Tweets.xlsx\n",
      "INFO:__main__:Downloading 132 tweets from BernieSanders\n",
      "INFO:__main__:Updated new tweets on spreadsheet for BernieSanders\n",
      "INFO:__main__:Downloading 24 tweets from BobbyJindal\n",
      "INFO:__main__:Updated new tweets on spreadsheet for BobbyJindal\n",
      "INFO:__main__:Downloading 24 tweets from CarlyFiorina\n",
      "INFO:__main__:Updated new tweets on spreadsheet for CarlyFiorina\n",
      "INFO:__main__:Downloading 0 tweets from ChrisChristie\n",
      "INFO:__main__:Downloading 155 tweets from gov_gilmore\n",
      "INFO:__main__:Updated new tweets on spreadsheet for gov_gilmore\n",
      "INFO:__main__:Downloading 59 tweets from GovernorPataki\n",
      "INFO:__main__:Updated new tweets on spreadsheet for GovernorPataki\n",
      "INFO:__main__:Downloading 27 tweets from GovernorPerry\n",
      "INFO:__main__:Updated new tweets on spreadsheet for GovernorPerry\n",
      "INFO:__main__:Downloading 734 tweets from GovMikeHuckabee\n",
      "INFO:__main__:Updated new tweets on spreadsheet for GovMikeHuckabee\n",
      "INFO:__main__:Downloading 102 tweets from HillaryClinton\n",
      "INFO:__main__:Updated new tweets on spreadsheet for HillaryClinton\n",
      "INFO:__main__:Downloading 228 tweets from JebBush\n",
      "INFO:__main__:Updated new tweets on spreadsheet for JebBush\n",
      "INFO:__main__:Downloading 1 tweets from JimWebbUSA\n",
      "INFO:__main__:Updated new tweets on spreadsheet for JimWebbUSA\n",
      "INFO:__main__:Downloading 480 tweets from JohnKasich\n",
      "INFO:__main__:Updated new tweets on spreadsheet for JohnKasich\n",
      "INFO:__main__:Downloading 15 tweets from LincolnChafee\n",
      "INFO:__main__:Updated new tweets on spreadsheet for LincolnChafee\n",
      "INFO:__main__:Downloading 505 tweets from LindseyGrahamSC\n",
      "INFO:__main__:Updated new tweets on spreadsheet for LindseyGrahamSC\n",
      "INFO:__main__:Downloading 861 tweets from marcorubio\n",
      "INFO:__main__:Updated new tweets on spreadsheet for marcorubio\n",
      "INFO:__main__:Downloading 184 tweets from MartinOMalley\n",
      "INFO:__main__:Updated new tweets on spreadsheet for MartinOMalley\n",
      "INFO:__main__:Downloading 1120 tweets from POTUS\n",
      "INFO:__main__:Updated new tweets on spreadsheet for POTUS\n",
      "INFO:__main__:Downloading 334 tweets from RandPaul\n",
      "INFO:__main__:Updated new tweets on spreadsheet for RandPaul\n",
      "INFO:__main__:Downloading 41 tweets from RealBenCarson\n",
      "INFO:__main__:Updated new tweets on spreadsheet for RealBenCarson\n",
      "INFO:__main__:Downloading 1120 tweets from realDonaldTrump\n",
      "INFO:__main__:Updated new tweets on spreadsheet for realDonaldTrump\n",
      "INFO:__main__:Downloading 29 tweets from RickSantorum\n",
      "INFO:__main__:Updated new tweets on spreadsheet for RickSantorum\n",
      "INFO:__main__:Downloading 1037 tweets from ScottWalker\n",
      "INFO:__main__:Updated new tweets on spreadsheet for ScottWalker\n",
      "INFO:__main__:Downloading 1027 tweets from tedcruz\n",
      "INFO:__main__:Updated new tweets on spreadsheet for tedcruz\n",
      "INFO:__main__:Done appending new tweets\n",
      "INFO:__main__:Time Elapsed: 44\n",
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/Tweet_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/working_sheets--THIS_IS_ACTUAL_DATA/PAC_Tweets.xlsx\n",
      "INFO:__main__:Downloading 0 tweets from america_leads\n",
      "INFO:__main__:Downloading 0 tweets from America_Next\n",
      "INFO:__main__:Downloading 63 tweets from americasliberty\n",
      "INFO:__main__:Updated new tweets on spreadsheet for americasliberty\n",
      "INFO:__main__:Downloading 0 tweets from BelieveAgainGOP\n",
      "INFO:__main__:Downloading 0 tweets from carlyforamerica\n",
      "INFO:__main__:Downloading 0 tweets from cspac\n",
      "INFO:__main__:Downloading 0 tweets from CorrectRecord\n",
      "INFO:__main__:Downloading 0 tweets from feelthebernorg\n",
      "INFO:__main__:Downloading 1832 tweets from greatamericapac\n",
      "INFO:__main__:Updated new tweets on spreadsheet for greatamericapac\n",
      "INFO:__main__:Downloading 0 tweets from NewDay4America\n",
      "INFO:__main__:Downloading 0 tweets from OppandFreedom\n",
      "INFO:__main__:Downloading 0 tweets from OurRevival\n",
      "INFO:__main__:Downloading 100 tweets from prioritiesUSA\n",
      "INFO:__main__:Updated new tweets on spreadsheet for prioritiesUSA\n",
      "INFO:__main__:Downloading 0 tweets from progressivekick\n",
      "INFO:__main__:Downloading 0 tweets from PAGPAC\n",
      "INFO:__main__:Downloading 0 tweets from r2rusa\n",
      "INFO:__main__:Downloading 0 tweets from securestrength\n",
      "INFO:__main__:Downloading 0 tweets from DraftRunBenRun\n",
      "INFO:__main__:Downloading 11 tweets from The_Purple_PAC\n",
      "INFO:__main__:Updated new tweets on spreadsheet for The_Purple_PAC\n",
      "INFO:__main__:Downloading 0 tweets from Unintimidated16\n",
      "INFO:__main__:Downloading 0 tweets from WorkingAgainPAC\n",
      "INFO:__main__:Downloading 184 tweets from RebuildingAmNow\n",
      "INFO:__main__:Updated new tweets on spreadsheet for RebuildingAmNow\n",
      "INFO:__main__:Done appending new tweets\n",
      "INFO:__main__:Time Elapsed: 10\n"
     ]
    }
   ],
   "source": [
    "collect_data(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "collect_data(path + pac_tweets, pac_sheet, path + tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update of metadata\n",
    "\n",
    "A tweets ability to stay in the public discouse is dependent on the number of retweets and favorites. The initial pull of a tweet will not complete picture of the tweets effectiveness. This script allows us to continously update a tweet's metadata counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params: is_cand - determines whether to pull candidates tweets or PAC tweets\n",
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(tweet_sheet, sheetname, tweet_list):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():       \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # Lessig has deleted this account, so skip it while updating tweets\n",
    "    if (name == 'Lessig2016'):\n",
    "      continue\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df, name)\n",
    "    \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    # 100 second pause between data pulls to avoid token exceptions\n",
    "    time.sleep(20)\n",
    "  \n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "def update_metadata(tweets_df, updates_df, cand_name): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  ## loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['created_at'], inplace=True)\n",
    "  \n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collect_addition_data(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "collect_addition_data(path + pac_tweets, pac_sheet, path + tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_full_url(short_urls, full_urls):\n",
    "for i, us in enumerate(short_urls):\n",
    "full = []\n",
    "  if not us.startswith(\"http\"):\n",
    "    continue\n",
    "  for url in us.split(\" \"):\n",
    "    if not url.startswith(\"http\"):\n",
    "      continue\n",
    "    try:\n",
    "      r = requests.head(url, allow_redirects=True)\n",
    "      full.append(r.url)\n",
    "    except:\n",
    "      logger.info(\"Error occurred for URL - %s\" % url)\n",
    "      continue\n",
    "  if i % 500 == 0:\n",
    "      logger.info(\"Extracting URL %d/%d\" % (i, len(short_urls)))\n",
    "      time.sleep(60)\n",
    "  full_urls[i] = \" \".join(full)q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_full_url(tweets_df, updates_df, cand_name):\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "    \n",
    "  logger.info(\"Successfully download the list...\")\n",
    "  for e, entry in enumerate(list_df):\n",
    "    if e < 15:\n",
    "      continue\n",
    "\n",
    "    name, since_id, count, index = entry[0], entry[1],entry[2], entry[3]\n",
    "\n",
    "    short_urls = worksheet.col_values(6)\n",
    "    logger.info(\"Downloaded %s URL\", name)\n",
    "    url_datas = ['' for i in xrange(len(short_urls))]\n",
    "    url_datas[0] = 'full URL'\n",
    "\n",
    "    get_full_url(short_urls, url_datas) # transfer short url to full urls and store in url_datas\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    while count < len(short_urls):\n",
    "      amount = min(100, len(short_urls) - count)\n",
    "      cells = worksheet.range('I'+str(count)+':'+'I'+str(count+amount-1))\n",
    "      assert(len(cells) == amount)\n",
    "      for i in range(amount):\n",
    "        cells[i].value = url_datas[count-1]\n",
    "        count += 1\n",
    "      worksheet.update_cells(cells)\n",
    "      logger.info(\"Update cells %d/%d for %s\" %(count, len(short_urls), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_full_url(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "update_full_url(path + pac_tweets, pac_sheet, path + tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert into one large csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_xlsx_csv (tweet_sheet, sheetname, tweet_list):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4) \n",
    "  \n",
    "  #merged_corpus = pd.DataFrame(columns=['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'full URL'])\n",
    "  merged_df = pd.DataFrame()\n",
    "\n",
    "  initial_loop = True\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():\n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    if(name == 'POTUS'):\n",
    "      continue\n",
    "    \n",
    "    if (initial_loop):\n",
    "      merged_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      merged_df['Name'] = name\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      initial_loop = False \n",
    "    \n",
    "    else:\n",
    "      # read current cand tweet sheet\n",
    "      curr_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      curr_df['Name'] = name\n",
    "      #print (curr_df)\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      \n",
    "      merged_df = merged_df.append(curr_df)\n",
    "      if(name =='ChrisChristie'):\n",
    "        break\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  merged_df.to_csv('merged_corpus.csv', encoding='utf-8')\n",
    "  \n",
    "  logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_xlsx_csv(path + cand_tweets, cand_sheet, path + tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaPo Fact Checking\n",
    "The cell below collects fact checks from the Washington Post's '2016 Election Fact Checker' and 'RealDonaldContext' chrome extension. The election fact checker data was hand collected and is stored in a json file while the extension data is pulled directly from the online hosted json file from the extension's developer blog.\n",
    "They are collected into an single dataframe consisting of the tweet id, rating, and source. They are then merged with a master sheet using tweet id.\n",
    "\n",
    "['2016 Election Fact Checker'](https://www.washingtonpost.com/graphics/politics/2016-election/fact-checker/)\n",
    "\n",
    "['RealDonaldContext'](https://chrome.google.com/webstore/detail/realdonaldcontext/ddbkmnomngnlcdglabflidgmhmcafogn?hl=en-US)\n",
    "\n",
    "['RealDonaldContext json file'](https://www.pbump.net/files/post/extension/core/data.php)\n",
    "\n",
    "['Rating System Scale'](https://www.washingtonpost.com/news/fact-checker/about-the-fact-checker/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code is from the fact checking portion of this project. It grabs the fact checked tweets from\n",
    "# the WaPo Trump tweet fact checking extension and adds the ratings to correspoding tweets in the spreadsheet\n",
    "\n",
    "# sheetnames\n",
    "trump_sheet = 'realDonaldTrump'\n",
    "potus_sheet = 'POTUS'\n",
    "\n",
    "logger.info(\"Start...\")\n",
    "\n",
    "# read in WaPo fact checks of Donald Trump from the WaPo Trump tweet chrome extension\n",
    "trump_check = pd.read_json('https://www.pbump.net/files/post/extension/core/data.php')\n",
    "# rename columns and remove text columns\n",
    "trump_check.columns = ['id', 'rating', 'tweet', 'source']\n",
    "trump_check = trump_check[['id', 'rating', 'source']]\n",
    "# call expand lists to turn fact checks of multiple tweets into multiple columns\n",
    "trump_check = expand_lists(trump_check)\n",
    "\n",
    "# load pre-election fact checks and filter for just id, rating, and source\n",
    "election_checks = pd.read_json('preelection_wapo.json')\n",
    "election_checks = election_checks[['id', 'rating', 'source']]\n",
    "\n",
    "# append the hand collected data with the data collected from the extension\n",
    "trump_check = trump_check.append(election_checks, ignore_index=True)\n",
    "trump_check.columns = ['id', 'WAPO_RATING', 'WAPO_SOURCE']\n",
    "logger.info(\"read in fact checks\")\n",
    "\n",
    "# set file pathway variables an expand to HOME\n",
    "in_path = '~/Dropbox/Summer_of_Tweets/fact_checking/Presidential_Fact_Checking.xlsx'\n",
    "in_path = os.path.expanduser(in_path)\n",
    "\n",
    "# properly load spreadsheet to append new data\n",
    "work_book = load_workbook(in_path)\n",
    "tweet_writer = pd.ExcelWriter(in_path, engine='openpyxl')\n",
    "tweet_writer.book = work_book\n",
    "tweet_writer.sheets = dict((ws.title, ws) for ws in work_book.worksheets)\n",
    "tweets_df = pd.read_excel(in_path, sheetname=trump_sheet, dtype={'id': str})\n",
    "logger.info(\"Downloaded excel sheets list\")\n",
    "\n",
    "# change data type to match excel sheet's\n",
    "trump_check['id'] = trump_check['id'].astype(str)\n",
    "#merge the fact check data set with the tweets set using tweet id\n",
    "merged_df = tweets_df.merge(trump_check, on='id', how='left')\n",
    "\n",
    "logger.info(merged_df.shape) # used for debugging\n",
    "# write merged data to the excel sheet\n",
    "merged_df.to_excel(tweet_writer, sheet_name=trump_sheet, index=False)\n",
    "tweet_writer.save()\n",
    "\n",
    "# merged_df.to_csv('WaPo.csv', encoding='utf-8') # used for viewing test results\n",
    "\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follower Growth\n",
    "The follower growth for Hillary Clinton and Donald Trump is being collected for the project. After some research on which sites are best for follower growth data, [Trackalytics](http://www.trackalytics.com) is the best free resource for tracking follower growth. However it does not have comprehensive follower growth data for the rest of the candidates, the others either are not present on the site or their data starts to get collected well into the election cycle.\n",
    "\n",
    "The data is scraped using the IMPORTHTML function in google sheets. Information on the function and how to use it can be found [here](http://lenagroeger.s3.amazonaws.com/talks/orlando/gettingdata.html)  while the sheet itself can be found [here](https://docs.google.com/spreadsheets/d/1rahomcsDJFf_za0S_Tbzi1kv79bdNM2ZqNZ_H7XcMIM/edit?usp=sharing). \n",
    "\n",
    "The following function runs to clean the data sheet to move daily delta in followers into its own column and then downloading and moving the sheet onto the FISP dropbox.\n",
    "\n",
    "Implemented using the df2gspread module, documentation for the module can be found [here](https://github.com/maybelinot/df2gspread)\n",
    "\n",
    "[Trump Follower Tracker](http://www.trackalytics.com/twitter/profile/RealDonaldTrump/)\n",
    "\n",
    "[Clinton Follower Tracker](http://www.trackalytics.com/twitter/profile/HillaryClinton/)\n",
    "\n",
    "*The Site is missing data for July 3-5th 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the follower growth sheeet in my drive\n",
    "#sheet_path = \"archive/fisp_twitter/sheets/follower_growth_test\"\n",
    "sheet_path = \"./follower_growth.xlsx\"\n",
    "\n",
    "# currently it contains Clinton and Trump follower growth\n",
    "clinton_sheet = \"HillaryClinton\"\n",
    "trump_sheet = \"realDonaldTrump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passed into the apply func to remove daily change in metadata for twitter accounts\n",
    "def split_func (string):\n",
    "  return string.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module for downloading the sheets from the drive \n",
    "# from df2gspread import gspread2df as g2d \n",
    "\n",
    "def gen_cand_followers (file_path, sheet_name):\n",
    "  # download the follower growth sheet from gdrive\n",
    "  #df = g2d.download(file_path, sheet_name, col_names = True) # use this line if directly importing from gSheets\n",
    "  df = pd.read_excel(file_path, sheet_name) # use this line if importing locally aftering downloading it from Sheets\n",
    "\n",
    "  # take the top row and convert it to the column header, this code is only needed if data is pulled from gdrive\n",
    "  #df.columns = df.iloc[0]\n",
    "  #df = df.reindex(df.index.drop(0))\n",
    "  \n",
    "  # trim out the change data that is appended at the end ot the daily value for the metadata\n",
    "  df['Followers'] = df.Followers_change.apply(split_func)\n",
    "  df['Following'] = df.Following_change.apply(split_func)\n",
    "  df['Tweets'] = df.Tweets_change.apply(split_func)\n",
    "  \n",
    "  # drop the change rows\n",
    "  df = df.drop(['Followers_change', 'Following_change', 'Tweets_change', \n",
    "                'Lists_change', 'Favourites_change', 'Tweets', 'id', 'Following'], 1)\n",
    "  \n",
    "  # convert date to MM/DD/YYYY format and rename the column\n",
    "  df['Date'] = pd.to_datetime(df.Date)\n",
    "  df['Date'] = df['Date'].dt.strftime('%m/%d/%Y')\n",
    "  df.columns = ['date', 'follower_count']\n",
    "  \n",
    "  # add col with candidate name\n",
    "  df['handle'] = sheet_name\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_follower_df = gen_cand_followers (sheet_path, clinton_sheet)\n",
    "trump_follower_df = gen_cand_followers (sheet_path, trump_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04/25/2018</td>\n",
       "      <td>51,164,021</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04/24/2018</td>\n",
       "      <td>51,122,142</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04/23/2018</td>\n",
       "      <td>51,078,478</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/22/2018</td>\n",
       "      <td>51,042,524</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04/21/2018</td>\n",
       "      <td>51,005,809</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date follower_count           handle\n",
       "0  04/25/2018     51,164,021  realDonaldTrump\n",
       "1  04/24/2018     51,122,142  realDonaldTrump\n",
       "2  04/23/2018     51,078,478  realDonaldTrump\n",
       "3  04/22/2018     51,042,524  realDonaldTrump\n",
       "4  04/21/2018     51,005,809  realDonaldTrump"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_follower_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append row with each candidate's last name\n",
    "clinton_follower_df['lastname'] = 'Clinton'\n",
    "trump_follower_df['lastname'] = 'Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cand_followers_df = clinton_follower_df.append(trump_follower_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_growth_df = pd.read_csv('follower_count_growth.csv')\n",
    "follower_growth_df = handle_to_lastname(follower_growth_df, 'handle', cand_to_lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_growth_df = follower_growth_df.append(gen_cand_followers_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_growth_df.date = pd.to_datetime(follower_growth_df['date'].str.replace(\"-\", \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv's path and load it into a panda's dataframe\n",
    "path = '~/Dropbox/Summer_of_Tweets/Deduped_Tweets/polling_merged.csv'\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "# duplicated the tweet created at columns to made a modify it to a MM/DD/YY format\n",
    "df['date'] = df['created_at']\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['date'] = pd.to_datetime(df['date'].dt.strftime('%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge value needs to be sorted to go through merge_asof \n",
    "df = df.sort_values(['date'])\n",
    "follower_growth_df = follower_growth_df.sort_values(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_growth_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the full data set with the general cadidate's follower growth using the date and lastname fields\n",
    "merged_df = pd.merge_asof(df, follower_growth_df, on='date', by='lastname')\n",
    "follower_df = merged_df.to_csv('followers_polling_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wayback Machine Follower Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path variables \n",
    "twitter_pages = '~/fisp_testing/new_pull/' # replace with path to wayback machine archive directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_follower_growth(pages_dir):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # intialize dataframe\n",
    "  follower_count_df = pd.DataFrame(columns=['date', 'follower_count', 'handle'])\n",
    "  \n",
    "  # Get the handles from dir present in dir of twitter pages pulled\n",
    "  handles = [name for name in os.listdir(os.path.expanduser(pages_dir)) if not name.startswith('.')]\n",
    "  \n",
    "  # loop through each handle present and extract date and corresponding follower count\n",
    "  for handle in handles:\n",
    "    # dict with key:value equaling date:follower_count\n",
    "    follower_count_dict = {}\n",
    "    \n",
    "    # keep a list of dates failed for debugging purposes\n",
    "    failed_dates = []\n",
    "    passed_dates = []\n",
    "    \n",
    "    logger.info(\"Current Handle: %s\", handle)\n",
    "    \n",
    "    # make a list of the dates present in the wayback archive\n",
    "    dates = [date for date in os.listdir(os.path.expanduser(pages_dir + handle)) if not date.startswith('.')]\n",
    "    \n",
    "    # for each archive date find 'follower_count' element and extract total\n",
    "    for date in dates:\n",
    "      # intialize len of number and bool that keeps track of whether number has ended\n",
    "      num_len = 0\n",
    "      num = False\n",
    "      \n",
    "      # convert string to datetime object\n",
    "      count_date = datetime.strptime(date, \"%Y%m%d%H%M%S\") # currently not being utilized and converted after the fact\n",
    "      \n",
    "      # get the path of the specific date's archived html\n",
    "      page = pages_dir + handle + '/' + date + '/twitter.com' + '/' + handle\n",
    "      # open the file as a BeuatifulSoup object then convert it to an str for easier search indexing\n",
    "      soup = BeautifulSoup(open(os.path.expanduser(page)), 'html.parser')\n",
    "      soup = str(soup)\n",
    "      \n",
    "      # get the index of where the follower_count element is\n",
    "      init = soup.find('followers_count')\n",
    "      \n",
    "      # for debugging purposes keep track of failed follower_count search\n",
    "      if (init == -1):\n",
    "        failed_dates.append(date[0:4])\n",
    "        continue\n",
    "      \n",
    "      # iterate thru the string until a digit is reach\n",
    "      while (not num):\n",
    "        init+=1 # keep track of where the number's initial index\n",
    "        num = soup[init].isdigit() # will return whether current char is a digit\n",
    "      \n",
    "      # iterate thru the number until you reach a char that is not a digit\n",
    "      while (num):\n",
    "        num_len+=1 # keep track of the len of the number\n",
    "        num = soup[init:init+num_len].isdigit() # will return whether current char is a digit\n",
    "      \n",
    "      # slice the number out of the html string and convert to an int\n",
    "      follower_count = int(soup[init:init+num_len-1])\n",
    "      # store the follower count in a dict with a key:value of date:follower_count\n",
    "      follower_count_dict[date] = follower_count\n",
    "    \n",
    "    # initialize a temporary dataframe to store the current handle's follower_count growth\n",
    "    temp_df = pd.DataFrame.from_dict(follower_count_dict, orient='index')\n",
    "    temp_df.reset_index(level=0, inplace=True)\n",
    "    temp_df['handle'] = handle # add a col indicating corresponding handle\n",
    "    temp_df.columns = ['date', 'follower_count', 'handle'] # rename columns\n",
    "    \n",
    "    # append the new data to the comprehensive dataframe with the growth for all present candidates\n",
    "    follower_count_df = follower_count_df.append(temp_df)\n",
    "  logger.info('Number of failed follower_count element searches %d', len(failed_dates))\n",
    "  logger.info('Done!')\n",
    "  return follower_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Current Handle: BobbyJindal\n",
      "INFO:__main__:Current Handle: gov_gilmore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nope!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Current Handle: ChrisChristie\n",
      "INFO:__main__:Current Handle: Lessig2016\n",
      "INFO:__main__:Current Handle: GovMikeHuckabee\n",
      "INFO:__main__:Current Handle: LindseyGrahamSC\n",
      "INFO:__main__:Current Handle: RandPaul\n",
      "INFO:__main__:Current Handle: GovernorPataki\n",
      "INFO:__main__:Current Handle: ScottWalker\n",
      "INFO:__main__:Current Handle: MartinOMalley\n",
      "INFO:__main__:Current Handle: RickSantorum\n",
      "INFO:__main__:Current Handle: JohnKasich\n",
      "INFO:__main__:Current Handle: marcorubio\n",
      "INFO:__main__:Current Handle: RealBenCarson\n",
      "INFO:__main__:Current Handle: JimWebbUSA\n",
      "INFO:__main__:Current Handle: tedcruz\n",
      "INFO:__main__:Current Handle: GovernorPerry\n",
      "INFO:__main__:Current Handle: BernieSanders\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nope!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Current Handle: CarlyFiorina\n",
      "INFO:__main__:Done!\n"
     ]
    }
   ],
   "source": [
    "follower_count_df = compile_follower_growth(twitter_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this to save resulting dataframe\n",
    "#follower_count_df.to_csv('follower_count_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Follower Count Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_count_df = pd.read_csv('follower_count_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properly sort the data\n",
    "follower_count_df = follower_count_df.sort_values(['handle','date'])\n",
    "follower_count_df = follower_count_df.reset_index()\n",
    "follower_count_df = follower_count_df[['date', 'follower_count', 'handle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150505022756</td>\n",
       "      <td>35215</td>\n",
       "      <td>BernieSanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150518030446</td>\n",
       "      <td>35215</td>\n",
       "      <td>BernieSanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150519234044</td>\n",
       "      <td>37016</td>\n",
       "      <td>BernieSanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150519234046</td>\n",
       "      <td>37017</td>\n",
       "      <td>BernieSanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150519234110</td>\n",
       "      <td>37016</td>\n",
       "      <td>BernieSanders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  follower_count         handle\n",
       "0  20150505022756           35215  BernieSanders\n",
       "1  20150518030446           35215  BernieSanders\n",
       "2  20150519234044           37016  BernieSanders\n",
       "3  20150519234046           37017  BernieSanders\n",
       "4  20150519234110           37016  BernieSanders"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follower_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_duplicate_dates(follower_count_df):\n",
    "  # keep a list of each row to be dropped from the dataframe\n",
    "  entries_to_drop = []\n",
    "  # loop through each row\n",
    "  for index, row in follower_count_df.iterrows():\n",
    "    # take the current date and modify it to remove time \n",
    "    curr_date = int((row['date']) / 1000000)\n",
    "    #curr_date = row['date'][0:8] # use this line if date is saved as a string\n",
    "    \n",
    "    # the first row cannot be compared\n",
    "    if (index != 0):\n",
    "      # if prev MMDDYYYY matches the current add it to be removed\n",
    "      if (curr_date == prev_date[1]):\n",
    "        entries_to_drop.append(prev_date[0])\n",
    "    # set prev_date to current_date for next iteration\n",
    "    prev_date = (index, curr_date)\n",
    "  \n",
    "  logger.info(\"expected size of dataframe: %d\", len(follower_count_df) - len(entries_to_drop))\n",
    "  entries_to_keep = set(range(len(follower_count_df))) - set(entries_to_drop)\n",
    "  follower_count_df = follower_count_df.take(list(entries_to_keep))\n",
    "  logger.info(\"actual size of dataframe %d\", len(follower_count_df))\n",
    "  return follower_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from YYYYMMDDHHMMSS format to YYYYMMDD \n",
    "temp_series = pd.Series((follower_count_df.date / 1000000)).astype(int)\n",
    "follower_count_df.date = pd.to_datetime(temp_series, format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the data by handles and date\n",
    "follower_count_df.sort_values(['handle','date']).to_csv('follower_count_growth.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polling Data Match\n",
    "\n",
    "This set of code will take the polling data from 538 and match them to their corresponding day's tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# load in the data\n",
    "prim_df = pd.read_csv('./data/national_primary_poll_average_2016.csv')\n",
    "# drop unncessary columns\n",
    "prim_df = prim_df[['lastname', 'poll_avg', 'forecastdate']]\n",
    "\n",
    "# reformat dates into a mm/dd/YY format and then rename columns for consistency\n",
    "prim_df['forecastdate'] = pd.to_datetime(prim_df.forecastdate)\n",
    "prim_df['forecastdate'] = prim_df['forecastdate'].dt.strftime('%m/%d/%Y')\n",
    "prim_df.columns = ['lastname', 'forecast', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sanders', 'Clinton', 'Trump', 'Kasich', 'Cruz', 'Rubio', 'Carson',\n",
       "       'Bush', 'Fiorina', 'Christie', 'Santorum', 'Paul', \"O'Malley\",\n",
       "       'Huckabee', 'Pataki', 'Graham', 'Jindal', 'Lessig', 'Chafee',\n",
       "       'Webb', 'Walker', 'Perry'], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_df.lastname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the nat data and filter out for polls-only data\n",
    "nat_df = pd.read_csv('./data/national_topline.csv')\n",
    "nat_df = nat_df[nat_df.type == 'polls-only']\n",
    "\n",
    "# reformat dates into a mm/dd/YY format\n",
    "nat_df['forecastdate'] = pd.to_datetime(nat_df.forecastdate)\n",
    "nat_df['forecastdate'] = nat_df['forecastdate'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "# remove all but date and prediction score for trump and clinton\n",
    "nat_df = nat_df[['forecastdate', 'ecwin_clinton', 'ecwin_trump']]\n",
    "nat_df.head()\n",
    "\n",
    "# create separate dataframes for each candidate to make it easier to manipulate and combine with primary data\n",
    "clinton_df = nat_df[['forecastdate', 'ecwin_clinton']]\n",
    "trump_df = nat_df[['forecastdate', 'ecwin_trump']]\n",
    "\n",
    "# add a corresponding column for lastname to match primary prediction data format\n",
    "clinton_df['lastname'] = 'Clinton'\n",
    "trump_df['lastname'] = 'Trump'\n",
    "\n",
    "# rename and rearrange columns for consistency with primary data\n",
    "clinton_df.columns = ['date', 'forecast', 'lastname']\n",
    "trump_df.columns = ['date', 'forecast', 'lastname']\n",
    "clinton_df = clinton_df[['lastname', 'forecast', 'date']]\n",
    "trump_df = trump_df[['lastname', 'forecast', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append general election forecast data with primary election forecast data\n",
    "forecast_df = prim_df.append(clinton_df, ignore_index=True)\n",
    "forecast_df = forecast_df.append(trump_df, ignore_index=True)\n",
    "#forecast_df.to_csv('538_polling.csv') # export forecast to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sanders', 'Clinton', 'Trump', 'Kasich', 'Cruz', 'Rubio', 'Carson',\n",
       "       'Bush', 'Fiorina', 'Christie', 'Santorum', 'Paul', \"O'Malley\",\n",
       "       'Huckabee', 'Pataki', 'Graham', 'Jindal', 'Lessig', 'Chafee',\n",
       "       'Webb', 'Walker', 'Perry'], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_df.lastname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set csv's path and load it into a panda's dataframe\n",
    "path = '~/Dropbox/Summer_of_Tweets/Deduped_Tweets/deduped_tweets.csv'\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "# duplicated the tweet created at columns to made a modify it to a MM/DD/YY format\n",
    "df['date'] = df['created_at']\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['date'] = df['date'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the cand names from the original sheet and make a parallel array of each cand's last name to match polling data\n",
    "cand = df.Candidate.unique()\n",
    "cand_lastname = ['Carson', 'Sanders', 'Jindal', 'Jindal', 'Fiorina', 'Fiorina', 'Christie', 'Christie', 'Sanders',\n",
    "                 'Pataki', 'Perry', 'Gilmore', 'Huckabee', 'Trump', 'Clinton', 'Clinton', 'Bush', 'Bush', 'Webb',\n",
    "                 'Kasich', 'Kasich', 'Lessig', 'Chaffee', 'Graham', 'Graham', 'Rubio', 'Rubio', \"O'Malley\", \"O'Malley\", \n",
    "                 'Huckabee', 'Sanders', 'Paul', 'Paul', 'Carson', 'Trump', 'Trump', 'Perry',\n",
    "                 'Santorum', 'Santorum', 'Walker', 'Walker', 'Cruz', 'Cruz']\n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "cand_to_lastname = {}\n",
    "for (cand, lastname) in zip(cand, cand_lastname):\n",
    "    cand_to_lastname[cand] = lastname\n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "lastname_to_cand = {}\n",
    "for (cand, lastname) in zip(cand_lastname, cand):\n",
    "    lastname_to_cand[cand] = lastname\n",
    "\n",
    "    \n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "lastname_to_cand = {}\n",
    "for (cand, lastname) in zip(cand_lastname, cand):\n",
    "    lastname_to_cand[cand] = lastname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ben_Carson', 'BernieSanders', 'BobbyJindal', 'Bobby_Jindal',\n",
       "       'CarlyFiorina', 'Carly_Fiorina', 'ChrisChristie', 'Chris_Christie',\n",
       "       'feelthebernorg', 'GovernorPataki', 'GovernorPerry', 'gov_gilmore',\n",
       "       'GovMikeHuckabee', 'greatamericapac', 'HillaryClinton',\n",
       "       'Hillary_Clinton', 'JebBush', 'Jeb_Bush', 'JimWebbUSA',\n",
       "       'JohnKasich', 'John_Kasich', 'Lessig2016', 'LincolnChafee',\n",
       "       'Lindsey_Graham', 'LindseyGrahamSC', 'marcorubio', 'Marco_Rubio',\n",
       "       'MartinOMalley', \"Martin_O'Malley\", 'Mike_Huckabee',\n",
       "       'progressivekick', 'RandPaul', 'Rand_Paul', 'RealBenCarson',\n",
       "       'realDonaldTrump', 'RebuildingAmNow', 'Rick_Perry', 'RickSantorum',\n",
       "       'Rick_Santorum', 'ScottWalker', 'Scott_Walker', 'tedcruz',\n",
       "       'Ted_Cruz'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Candidate.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the cand names from the original sheet and make a parallel array of each cand's last name to match polling data\n",
    "cand = df.Candidate.unique()\n",
    "\n",
    "cand_lastname = ['Carson', 'Sanders', 'Jindal', 'Jindal', 'Fiorina', 'Fiorina', 'Christie', 'Christie', 'Sanders',\n",
    "                 'Pataki', 'Perry', 'Gilmore', 'Huckabee', 'Trump', 'Clinton', 'Clinton', 'Bush', 'Bush', 'Webb',\n",
    "                 'Kasich', 'Kasich', 'Lessig', 'Chafee', 'Graham', 'Graham', 'Rubio', 'Rubio', \"O'Malley\", \"O'Malley\", \n",
    "                 'Huckabee', 'Sanders', 'Paul', 'Paul', 'Carson', 'Trump', 'Trump', 'Perry',\n",
    "                 'Santorum', 'Santorum', 'Walker', 'Walker', 'Cruz', 'Cruz']\n",
    "\n",
    "# take the parallel arrays and make a dict with the orig name as key and the lastname as value\n",
    "lastname_to_cand = {}\n",
    "for (cand, lastname) in zip(cand_lastname, cand):\n",
    "    lastname_to_cand[cand] = lastname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastname_to_cand = {'Carson': 'Ben_Carson', 'Sanders': 'BernieSanders', 'Jindal': 'BobbyJindal', \n",
    "                    'Fiorina': 'CarlyFiorina', 'Christie':'ChrisChristie', 'Pataki': 'GovernorPataki',\n",
    "                    'Perry': 'GovernorPerry', 'Gilmore': 'gov_gilmore', 'Huckabee': 'GovMikeHuckabee',\n",
    "                    'Trump': 'realDonaldTrump', 'Clinton': 'Hillary_Clinton', 'Bush': 'JebBush', 'Webb':'JimWebbUSA', \n",
    "                    'Kasich': 'John_Kasich', 'Lessig': 'Lessig2016', 'Chafee': 'LincolnChafee',\n",
    "                    'Graham': 'Lindsey_Graham', 'Rubio': 'marcorubio', \"OMalley\": 'MartinOMalley', 'Paul': 'RandPaul',\n",
    "                    'Perry': 'Rick_Perry','Santorum': 'RickSantorum', 'Walker': 'ScottWalker', 'Cruz': 'tedcruz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'BernieSanders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-304cb9b95351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlastname_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforecast_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mlastname_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastname_to_cand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcand\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mforecast_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lastname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlastname_col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BernieSanders'"
     ]
    }
   ],
   "source": [
    "# make a parallel list of each cand's last name to be appeneded to the original dataframe\n",
    "# lastname_col = []\n",
    "# for cand in forecast_df.lastname:\n",
    "#   lastname_col.append(lastname_to_cand[cand])\n",
    "# \n",
    "# forecast_df['lastname'] = lastname_col\n",
    "\n",
    "#forecast_df.to_csv('538_polling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a parallel list of each cand's last name to be appeneded to the original dataframe\n",
    "lastname_col = []\n",
    "for cand in df.Candidate:\n",
    "  lastname_col.append(cand_to_lastname[cand])\n",
    "  \n",
    "df['lastname'] = lastname_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_df.to_csv('538_polling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataframe size: 5596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sanders' 'Clinton' 'Trump' 'Kasich' 'Cruz' 'Rubio' 'Carson' 'Bush'\n",
      " 'Fiorina' 'Christie' 'Santorum' 'Paul' \"O'Malley\" 'Huckabee' 'Pataki'\n",
      " 'Graham' 'Jindal' 'Lessig' 'Chafee' 'Webb' 'Walker' 'Perry']\n"
     ]
    }
   ],
   "source": [
    "print forecast_df.lastname.unique()\n",
    "logger.info(\"Dataframe size: %d\", len(forecast_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(forecast_df, on=['date', 'lastname'])\n",
    "polling_merged = merged_df.to_csv('polling_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate</th>\n",
       "      <th>from</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag.</th>\n",
       "      <th>at.</th>\n",
       "      <th>link</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>...</th>\n",
       "      <th>dur_camp</th>\n",
       "      <th>pre_2016</th>\n",
       "      <th>post_ge</th>\n",
       "      <th>pre_prim</th>\n",
       "      <th>post_prim</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>rt_dummy</th>\n",
       "      <th>date</th>\n",
       "      <th>lastname</th>\n",
       "      <th>forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:03:00</td>\n",
       "      <td>RT @OneNation4Ben: I support #BenCarson2016 ht...</td>\n",
       "      <td>#BenCarson2016 #RunBenRun #PJET #tcot</td>\n",
       "      <td>@OneNation4Ben</td>\n",
       "      <td>https://t.co/cO9Z9h2Bfg</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @onenation4ben: i support #bencarson2016</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:01:00</td>\n",
       "      <td>RT @ThePatriot143: Classy act by Ben Carson #G...</td>\n",
       "      <td>#GOPDebate #MomentOfSilence #SanBernardino</td>\n",
       "      <td>@ThePatriot143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @thepatriot143: classy act by ben carson #g...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:01:00</td>\n",
       "      <td>RT @nprpolitics: Extended medical metaphor fro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@nprpolitics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @nprpolitics: extended medical metaphor fro...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:43:00</td>\n",
       "      <td>Carson takes the stage #WinBenWin #BC2DC16</td>\n",
       "      <td>#WinBenWin #BC2DC16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>carson takes the stage #winbenwin #bc2dc16</td>\n",
       "      <td>0</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:41:00</td>\n",
       "      <td>RT @SandyMPool: BEN CARSON WILL BRING LEADERSH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@SandyMPool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @sandympool: ben carson will bring leadersh...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:09:00</td>\n",
       "      <td>RT @BreitbartNews: Ben Carson begins with a mo...</td>\n",
       "      <td>#GOPDebate</td>\n",
       "      <td>@BreitbartNews</td>\n",
       "      <td>http</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @breitbartnews: ben carson begins with a mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:45:00</td>\n",
       "      <td>RT @irgrannyg: Go, Ben! https://t.co/7oevzZnFFR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@irgrannyg</td>\n",
       "      <td>https://t.co/7oevzZnFFR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @irgrannyg: go, ben!</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:03:00</td>\n",
       "      <td>RT @SusanCucinotta: @RealBenCarson #Carsonator...</td>\n",
       "      <td>#Carsonators #Students4Carson</td>\n",
       "      <td>@SusanCucinotta @RealBenCarson @YouthForCarson...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @susancucinotta: #carsonators #students4car...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:00:00</td>\n",
       "      <td>RT @Cry2MarvinsRoom: Ben Carson with the FIRE ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Cry2MarvinsRoom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @cry2marvinsroom: ben carson with the fire ...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:04:00</td>\n",
       "      <td>RT @FoxBusiness: .@RealBenCarson: Right now, t...</td>\n",
       "      <td>#GOPDeb</td>\n",
       "      <td>@FoxBusiness</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @foxbusiness: .@realbencarson: right now, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:07:00</td>\n",
       "      <td>RT @healerhauler: #TheBestMedicineFor America ...</td>\n",
       "      <td>#TheBestMedicineFor</td>\n",
       "      <td>@healerhauler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @healerhauler: #thebestmedicinefor america ...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:11:00</td>\n",
       "      <td>.@CNN Please I asked nicely</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.@cnn please i asked nicely</td>\n",
       "      <td>0</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:11:00</td>\n",
       "      <td>RT @RealBenCarson: https://t.co/wwM2M7x6TS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@RealBenCarson</td>\n",
       "      <td>https://t.co/wwM2M7x6TS</td>\n",
       "      <td>491.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @realbencarson:</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:13:00</td>\n",
       "      <td>RT @healerhauler: If you were wondering about ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@healerhauler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @healerhauler: if you were wondering about ...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 00:27:00</td>\n",
       "      <td>RT @RealBenCarson: https://t.co/eKNNznc5QR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@RealBenCarson</td>\n",
       "      <td>https://t.co/eKNNznc5QR</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @realbencarson:</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:40:00</td>\n",
       "      <td>RT @SandyMPool: BEN CARSON WINS with gifted HA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@SandyMPool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @sandympool: ben carson wins with gifted ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:46:00</td>\n",
       "      <td>RT @AmeliaNaomi14: Ben Carson is the man</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@AmeliaNaomi14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @amelianaomi14: ben carson is the man</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:46:00</td>\n",
       "      <td>RT @catherine_punja: Ben Carson for President</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@catherine_punja</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @catherine_punja: ben carson for president</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:48:00</td>\n",
       "      <td>Carson is 4th from the left and to the left of...</td>\n",
       "      <td>#WinBenWin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>carson is 4th from the left and to the left of...</td>\n",
       "      <td>0</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:07:00</td>\n",
       "      <td>RT @BlytheTravels: And the Golden Globe for be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@BlytheTravels</td>\n",
       "      <td>https://t.co/9N2mZKmU5J</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @blythetravels: and the golden globe for be...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:08:00</td>\n",
       "      <td>@CNN can you ask Ben Carson a question please?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@CNN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@cnn can you ask ben carson a question please?</td>\n",
       "      <td>0</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:09:00</td>\n",
       "      <td>RT @TheFix: Opening statements ranked: 1. Cars...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@TheFix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @thefix: opening statements ranked: 1. cars...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:58:00</td>\n",
       "      <td>\"We need to declare war on Isis\" - Ben Carson ...</td>\n",
       "      <td>#WinBenWin #BC2DC16 #GOPDebate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"we need to declare war on isis\" - ben carson ...</td>\n",
       "      <td>0</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:48:00</td>\n",
       "      <td>Each candidate gets 1 minute to introduce them...</td>\n",
       "      <td>#WinBenWin #BC2DC16 #GOPDebate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>each candidate gets 1 minute to introduce them...</td>\n",
       "      <td>0</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:51:00</td>\n",
       "      <td>RT @RealBenCarson: Lead us tonight Lord, take ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@RealBenCarson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1597.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @realbencarson: lead us tonight lord, take ...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 01:55:00</td>\n",
       "      <td>RT @ElenaNinj: #America NEEDS @RealBenCarson #...</td>\n",
       "      <td>#America #BC2DC16 #carson2016 #ben4potus #winb...</td>\n",
       "      <td>@ElenaNinj @RealBenCarson</td>\n",
       "      <td>https:/</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @elenaninj: #america needs #bc2dc16 #carson...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 00:27:00</td>\n",
       "      <td>RT @RealBenCarson: Today, I released my plan t...</td>\n",
       "      <td>#ISIS</td>\n",
       "      <td>@RealBenCarson</td>\n",
       "      <td>https:/</td>\n",
       "      <td>323.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @realbencarson: today, i released my plan t...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 00:27:00</td>\n",
       "      <td>RT @RealBenCarson: Visiting the so called \"spi...</td>\n",
       "      <td>#GOPDebate</td>\n",
       "      <td>@RealBenCarson</td>\n",
       "      <td>https://t.co/o5clvDbF06</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @realbencarson: visiting the so called \"spi...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 00:27:00</td>\n",
       "      <td>RT @RealBenCarson: Check out Candy's new Chris...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@RealBenCarson</td>\n",
       "      <td>https://t.co/lQy2VJ5dOY</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @realbencarson: check out candy's new chris...</td>\n",
       "      <td>1</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ben_Carson</td>\n",
       "      <td>DraftRunBenRun</td>\n",
       "      <td>6.77E+17</td>\n",
       "      <td>2015-12-16 02:14:00</td>\n",
       "      <td>.@CNN Hey one question for Ben Carson would be...</td>\n",
       "      <td>#WinBenWin #BC2DC16 #GOPDebate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.@cnn hey one question for ben carson would be...</td>\n",
       "      <td>0</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Carson</td>\n",
       "      <td>13.491012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99002</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 18:03:00</td>\n",
       "      <td>RT @betsy_klein: Twins for @tedcruz in Towson,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@betsy_klein @tedcruz</td>\n",
       "      <td>https://t.co/p7a6U8RuCo</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @betsy_klein: twins for in towson, md</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99003</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 18:12:00</td>\n",
       "      <td>#CruzCrew: Join us live here: https://t.co/zzv...</td>\n",
       "      <td>#CruzCrew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/zzvTc4izqZ</td>\n",
       "      <td>157.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#cruzcrew: join us live here:</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99004</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 18:25:00</td>\n",
       "      <td>RT @MSNBC: LIVE NOW: @tedcruz holds a campaign...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@MSNBC @tedcruz</td>\n",
       "      <td>https://t.co/PuZyZ0pnLg https://t.co/gqm2kTuXnM</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @msnbc: live now: holds a campaign rally in...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99005</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 18:28:00</td>\n",
       "      <td>RT @jamesoliphant: Cruz crowd in Towson. https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@jamesoliphant</td>\n",
       "      <td>https://t.co/pSkMkCqAhN</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @jamesoliphant: cruz crowd in towson.</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99006</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 19:10:00</td>\n",
       "      <td>\"We have full confidence that a Cruz White Hou...</td>\n",
       "      <td>#ChooseCruz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/duuwFF5swR</td>\n",
       "      <td>478.0</td>\n",
       "      <td>951.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"we have full confidence that a cruz white hou...</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99007</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 19:35:00</td>\n",
       "      <td>Stop the monkey business. #AbolishTheIRS: http...</td>\n",
       "      <td>#AbolishTheIRS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/QjbHUcdxzU https://t.co/RgR0HbU5i7</td>\n",
       "      <td>470.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stop the monkey business. #abolishtheirs:</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99008</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 20:45:00</td>\n",
       "      <td>My tax plan is this simple... https://t.co/ZLK...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/ZLKZVzMh4f https://t.co/Aivb7pSfWs</td>\n",
       "      <td>464.0</td>\n",
       "      <td>657.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>my tax plan is this simple...</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99009</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 20:57:00</td>\n",
       "      <td>RT @haaretzcom: Why there is no greater friend...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@haaretzcom</td>\n",
       "      <td>https://t.co/LYYDC3SsWr https://t.co/vl5ot5ebAy</td>\n",
       "      <td>382.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @haaretzcom: why there is no greater friend...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99010</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 21:03:00</td>\n",
       "      <td>RT @bbryantjones: Why I Will Be Voting For Ted...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@bbryantjones @yidwithlid</td>\n",
       "      <td>https://t.co/AyqiYlBwSp</td>\n",
       "      <td>308.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @bbryantjones: why i will be voting for ted...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99011</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 22:01:00</td>\n",
       "      <td>RT @dcexaminer: Here's how Ted Cruz celebrates...</td>\n",
       "      <td>#TaxDay</td>\n",
       "      <td>@dcexaminer</td>\n",
       "      <td>https://t.co/9H0V1rpem6 https://t.co/YfjcyU4Pj1</td>\n",
       "      <td>183.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @dcexaminer: here's how ted cruz celebrates...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99012</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 22:02:00</td>\n",
       "      <td>#AbolishTheIRS: https://t.co/ZLKZVzMh4f https:...</td>\n",
       "      <td>#AbolishTheIRS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/ZLKZVzMh4f https://t.co/age0ZcZ9gH</td>\n",
       "      <td>340.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#abolishtheirs:</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99013</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 22:12:00</td>\n",
       "      <td>#CruzCrew: Don't miss me on the @oreillyfactor...</td>\n",
       "      <td>#CruzCrew</td>\n",
       "      <td>@oreillyfactor @FoxNews</td>\n",
       "      <td>https://t.co/HAOKWYRhtn</td>\n",
       "      <td>264.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#cruzcrew: don't miss me on the @oreillyfactor...</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99014</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 22:13:00</td>\n",
       "      <td>RT @MonicaCrowley: So great to see my friend @...</td>\n",
       "      <td>#fierce</td>\n",
       "      <td>@MonicaCrowley @TedCruz @FoxNews</td>\n",
       "      <td>https://t.co/TwgPaKsyJt</td>\n",
       "      <td>414.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @monicacrowley: so great to see my friend t...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99015</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 22:42:00</td>\n",
       "      <td>RT @MarkHalperin: Watch: @tedcruz, @bruceredde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@MarkHalperin @tedcruz @brucereddenjr @MattNegrin</td>\n",
       "      <td>https://t.co/cEJH0G0p7I</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @markhalperin: watch: @tedcruz, @bruceredde...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99016</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 23:31:00</td>\n",
       "      <td>RT @oreillyfactor: Ted Cruz joins The Factor f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@oreillyfactor</td>\n",
       "      <td>https://t.co/QmPcH9cI5v</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @oreillyfactor: ted cruz joins the factor f...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99017</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 23:40:00</td>\n",
       "      <td>RT @RabbiShmuley: With @tedcruz night before t...</td>\n",
       "      <td>#NewYork</td>\n",
       "      <td>@RabbiShmuley @tedcruz</td>\n",
       "      <td>https://t.co/RXGHxZs2du</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @rabbishmuley: with night before the #newyo...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99018</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 13:22:00</td>\n",
       "      <td>RT @nytpolitics: Ted Cruz's views over 2 decad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@nytpolitics</td>\n",
       "      <td>https://</td>\n",
       "      <td>488.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @nytpolitics: ted cruz's views over 2 decad...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99019</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 13:12:00</td>\n",
       "      <td>It's Tax Day! Join the fight to #AbolishTheIRS...</td>\n",
       "      <td>#AbolishTheIRS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/QjbHUcdxzU https://t.co/c8wQ4ye5rx</td>\n",
       "      <td>451.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>it's tax day! join the fight to #abolishtheirs:</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99020</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 13:19:00</td>\n",
       "      <td>RT @ABCPolitics: .@tedcruz to @GMA: I'm the on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@ABCPolitics @GMA</td>\n",
       "      <td>https://t.co/RdK6Nu2WvO https:/</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @abcpolitics: .@tedcruz to @gma: i'm the on...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99021</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 13:32:00</td>\n",
       "      <td>RT @GA6thForCruz: Ted Cruz in @WSJ A Simple Fl...</td>\n",
       "      <td>#ChooseCruz #PJNET #ChooseCruz #IRS #jobs</td>\n",
       "      <td>@GA6thForCruz @WSJ</td>\n",
       "      <td>https://t.co/LpI0ALR5wQ</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @ga6thforcruz: ted cruz in @wsj a simple fl...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99022</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 13:34:00</td>\n",
       "      <td>RT @RonRoesener: This addresses something no o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@RonRoesener</td>\n",
       "      <td>https://t</td>\n",
       "      <td>391.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @ronroesener: this addresses something no o...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99023</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 13:36:00</td>\n",
       "      <td>RT @Nightline: Ted Cruzs wife Heidi Cruz says ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Nightline</td>\n",
       "      <td>https://t.co/9iFTNBvo8T https://t.co/2qqKooOI5y</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @nightline: ted cruzs wife heidi cruz says ...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99024</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 13:43:00</td>\n",
       "      <td>RT @trscoop: WATCH: Ted Cruzs Town Hall on Goo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@trscoop</td>\n",
       "      <td>https://t.co/5x413C6HNv https://t.co/TCysnHSUOY</td>\n",
       "      <td>305.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @trscoop: watch: ted cruzs town hall on goo...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99025</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 14:23:00</td>\n",
       "      <td>\"In the case of Ted Cruz, we have a candidate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/dVdBlRlr5J</td>\n",
       "      <td>659.0</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"in the case of ted cruz, we have a candidate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99026</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 15:09:00</td>\n",
       "      <td>RT @navycrawfish: Why I Support Ted Cruz | PJ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@navycrawfish</td>\n",
       "      <td>https://t.co/bbnmO4pCGU</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @navycrawfish: why i support ted cruz | pj ...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99027</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 15:08:00</td>\n",
       "      <td>RT @JeanieHannaman: VIDEO: Ted Cruz Does CNBCs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@JeanieHannaman</td>\n",
       "      <td>https://t.co/8ZWyhxqTQP</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @jeaniehannaman: video: ted cruz does cnbcs...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99028</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 14:55:00</td>\n",
       "      <td>RT @rickklein: fun talking to @tedcruz and @he...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@rickklein @tedcruz @heidiscruz @GMA</td>\n",
       "      <td>https://t.co/MXPw</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @rickklein: fun talking to and @heidiscruz ...</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99029</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 15:10:00</td>\n",
       "      <td>RT @alexahenning: Ted Cruz: The Tax Basher! ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@alexahenning</td>\n",
       "      <td>https://t.co/SXErrSxHLn</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @alexahenning: ted cruz: the tax basher!</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99030</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 14:38:00</td>\n",
       "      <td>RT @proteinwisdom: Vote Cruz.  https://t.co/AW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@proteinwisdom</td>\n",
       "      <td>https://t.co/AWhJ7NMllC</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rt @proteinwisdom: vote cruz.</td>\n",
       "      <td>1</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99031</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>7.22E+17</td>\n",
       "      <td>2016-04-18 15:41:00</td>\n",
       "      <td>A joyous reminder that you can kiss IRS-stress...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/sLKJ1JVnuq</td>\n",
       "      <td>577.0</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a joyous reminder that you can kiss irs-stress...</td>\n",
       "      <td>0</td>\n",
       "      <td>04/18/2016</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>28.600157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99032 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Candidate            from        id           created_at  \\\n",
       "0      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:03:00   \n",
       "1      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:01:00   \n",
       "2      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:01:00   \n",
       "3      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:43:00   \n",
       "4      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:41:00   \n",
       "5      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:09:00   \n",
       "6      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:45:00   \n",
       "7      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:03:00   \n",
       "8      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:00:00   \n",
       "9      Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:04:00   \n",
       "10     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:07:00   \n",
       "11     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:11:00   \n",
       "12     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:11:00   \n",
       "13     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:13:00   \n",
       "14     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 00:27:00   \n",
       "15     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:40:00   \n",
       "16     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:46:00   \n",
       "17     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:46:00   \n",
       "18     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:48:00   \n",
       "19     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:07:00   \n",
       "20     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:08:00   \n",
       "21     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:09:00   \n",
       "22     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:58:00   \n",
       "23     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:48:00   \n",
       "24     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:51:00   \n",
       "25     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 01:55:00   \n",
       "26     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 00:27:00   \n",
       "27     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 00:27:00   \n",
       "28     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 00:27:00   \n",
       "29     Ben_Carson  DraftRunBenRun  6.77E+17  2015-12-16 02:14:00   \n",
       "...           ...             ...       ...                  ...   \n",
       "99002     tedcruz         tedcruz  7.22E+17  2016-04-18 18:03:00   \n",
       "99003     tedcruz         tedcruz  7.22E+17  2016-04-18 18:12:00   \n",
       "99004     tedcruz         tedcruz  7.22E+17  2016-04-18 18:25:00   \n",
       "99005     tedcruz         tedcruz  7.22E+17  2016-04-18 18:28:00   \n",
       "99006     tedcruz         tedcruz  7.22E+17  2016-04-18 19:10:00   \n",
       "99007     tedcruz         tedcruz  7.22E+17  2016-04-18 19:35:00   \n",
       "99008     tedcruz         tedcruz  7.22E+17  2016-04-18 20:45:00   \n",
       "99009     tedcruz         tedcruz  7.22E+17  2016-04-18 20:57:00   \n",
       "99010     tedcruz         tedcruz  7.22E+17  2016-04-18 21:03:00   \n",
       "99011     tedcruz         tedcruz  7.22E+17  2016-04-18 22:01:00   \n",
       "99012     tedcruz         tedcruz  7.22E+17  2016-04-18 22:02:00   \n",
       "99013     tedcruz         tedcruz  7.22E+17  2016-04-18 22:12:00   \n",
       "99014     tedcruz         tedcruz  7.22E+17  2016-04-18 22:13:00   \n",
       "99015     tedcruz         tedcruz  7.22E+17  2016-04-18 22:42:00   \n",
       "99016     tedcruz         tedcruz  7.22E+17  2016-04-18 23:31:00   \n",
       "99017     tedcruz         tedcruz  7.22E+17  2016-04-18 23:40:00   \n",
       "99018     tedcruz         tedcruz  7.22E+17  2016-04-18 13:22:00   \n",
       "99019     tedcruz         tedcruz  7.22E+17  2016-04-18 13:12:00   \n",
       "99020     tedcruz         tedcruz  7.22E+17  2016-04-18 13:19:00   \n",
       "99021     tedcruz         tedcruz  7.22E+17  2016-04-18 13:32:00   \n",
       "99022     tedcruz         tedcruz  7.22E+17  2016-04-18 13:34:00   \n",
       "99023     tedcruz         tedcruz  7.22E+17  2016-04-18 13:36:00   \n",
       "99024     tedcruz         tedcruz  7.22E+17  2016-04-18 13:43:00   \n",
       "99025     tedcruz         tedcruz  7.22E+17  2016-04-18 14:23:00   \n",
       "99026     tedcruz         tedcruz  7.22E+17  2016-04-18 15:09:00   \n",
       "99027     tedcruz         tedcruz  7.22E+17  2016-04-18 15:08:00   \n",
       "99028     tedcruz         tedcruz  7.22E+17  2016-04-18 14:55:00   \n",
       "99029     tedcruz         tedcruz  7.22E+17  2016-04-18 15:10:00   \n",
       "99030     tedcruz         tedcruz  7.22E+17  2016-04-18 14:38:00   \n",
       "99031     tedcruz         tedcruz  7.22E+17  2016-04-18 15:41:00   \n",
       "\n",
       "                                                    text  \\\n",
       "0      RT @OneNation4Ben: I support #BenCarson2016 ht...   \n",
       "1      RT @ThePatriot143: Classy act by Ben Carson #G...   \n",
       "2      RT @nprpolitics: Extended medical metaphor fro...   \n",
       "3             Carson takes the stage #WinBenWin #BC2DC16   \n",
       "4      RT @SandyMPool: BEN CARSON WILL BRING LEADERSH...   \n",
       "5      RT @BreitbartNews: Ben Carson begins with a mo...   \n",
       "6        RT @irgrannyg: Go, Ben! https://t.co/7oevzZnFFR   \n",
       "7      RT @SusanCucinotta: @RealBenCarson #Carsonator...   \n",
       "8      RT @Cry2MarvinsRoom: Ben Carson with the FIRE ...   \n",
       "9      RT @FoxBusiness: .@RealBenCarson: Right now, t...   \n",
       "10     RT @healerhauler: #TheBestMedicineFor America ...   \n",
       "11                           .@CNN Please I asked nicely   \n",
       "12            RT @RealBenCarson: https://t.co/wwM2M7x6TS   \n",
       "13     RT @healerhauler: If you were wondering about ...   \n",
       "14            RT @RealBenCarson: https://t.co/eKNNznc5QR   \n",
       "15     RT @SandyMPool: BEN CARSON WINS with gifted HA...   \n",
       "16              RT @AmeliaNaomi14: Ben Carson is the man   \n",
       "17         RT @catherine_punja: Ben Carson for President   \n",
       "18     Carson is 4th from the left and to the left of...   \n",
       "19     RT @BlytheTravels: And the Golden Globe for be...   \n",
       "20        @CNN can you ask Ben Carson a question please?   \n",
       "21     RT @TheFix: Opening statements ranked: 1. Cars...   \n",
       "22     \"We need to declare war on Isis\" - Ben Carson ...   \n",
       "23     Each candidate gets 1 minute to introduce them...   \n",
       "24     RT @RealBenCarson: Lead us tonight Lord, take ...   \n",
       "25     RT @ElenaNinj: #America NEEDS @RealBenCarson #...   \n",
       "26     RT @RealBenCarson: Today, I released my plan t...   \n",
       "27     RT @RealBenCarson: Visiting the so called \"spi...   \n",
       "28     RT @RealBenCarson: Check out Candy's new Chris...   \n",
       "29     .@CNN Hey one question for Ben Carson would be...   \n",
       "...                                                  ...   \n",
       "99002  RT @betsy_klein: Twins for @tedcruz in Towson,...   \n",
       "99003  #CruzCrew: Join us live here: https://t.co/zzv...   \n",
       "99004  RT @MSNBC: LIVE NOW: @tedcruz holds a campaign...   \n",
       "99005  RT @jamesoliphant: Cruz crowd in Towson. https...   \n",
       "99006  \"We have full confidence that a Cruz White Hou...   \n",
       "99007  Stop the monkey business. #AbolishTheIRS: http...   \n",
       "99008  My tax plan is this simple... https://t.co/ZLK...   \n",
       "99009  RT @haaretzcom: Why there is no greater friend...   \n",
       "99010  RT @bbryantjones: Why I Will Be Voting For Ted...   \n",
       "99011  RT @dcexaminer: Here's how Ted Cruz celebrates...   \n",
       "99012  #AbolishTheIRS: https://t.co/ZLKZVzMh4f https:...   \n",
       "99013  #CruzCrew: Don't miss me on the @oreillyfactor...   \n",
       "99014  RT @MonicaCrowley: So great to see my friend @...   \n",
       "99015  RT @MarkHalperin: Watch: @tedcruz, @bruceredde...   \n",
       "99016  RT @oreillyfactor: Ted Cruz joins The Factor f...   \n",
       "99017  RT @RabbiShmuley: With @tedcruz night before t...   \n",
       "99018  RT @nytpolitics: Ted Cruz's views over 2 decad...   \n",
       "99019  It's Tax Day! Join the fight to #AbolishTheIRS...   \n",
       "99020  RT @ABCPolitics: .@tedcruz to @GMA: I'm the on...   \n",
       "99021  RT @GA6thForCruz: Ted Cruz in @WSJ A Simple Fl...   \n",
       "99022  RT @RonRoesener: This addresses something no o...   \n",
       "99023  RT @Nightline: Ted Cruzs wife Heidi Cruz says ...   \n",
       "99024  RT @trscoop: WATCH: Ted Cruzs Town Hall on Goo...   \n",
       "99025  \"In the case of Ted Cruz, we have a candidate ...   \n",
       "99026  RT @navycrawfish: Why I Support Ted Cruz | PJ ...   \n",
       "99027  RT @JeanieHannaman: VIDEO: Ted Cruz Does CNBCs...   \n",
       "99028  RT @rickklein: fun talking to @tedcruz and @he...   \n",
       "99029  RT @alexahenning: Ted Cruz: The Tax Basher! ht...   \n",
       "99030  RT @proteinwisdom: Vote Cruz.  https://t.co/AW...   \n",
       "99031  A joyous reminder that you can kiss IRS-stress...   \n",
       "\n",
       "                                                hashtag.  \\\n",
       "0                  #BenCarson2016 #RunBenRun #PJET #tcot   \n",
       "1             #GOPDebate #MomentOfSilence #SanBernardino   \n",
       "2                                                    NaN   \n",
       "3                                    #WinBenWin #BC2DC16   \n",
       "4                                                    NaN   \n",
       "5                                             #GOPDebate   \n",
       "6                                                    NaN   \n",
       "7                          #Carsonators #Students4Carson   \n",
       "8                                                    NaN   \n",
       "9                                                #GOPDeb   \n",
       "10                                   #TheBestMedicineFor   \n",
       "11                                                   NaN   \n",
       "12                                                   NaN   \n",
       "13                                                   NaN   \n",
       "14                                                   NaN   \n",
       "15                                                   NaN   \n",
       "16                                                   NaN   \n",
       "17                                                   NaN   \n",
       "18                                            #WinBenWin   \n",
       "19                                                   NaN   \n",
       "20                                                   NaN   \n",
       "21                                                   NaN   \n",
       "22                        #WinBenWin #BC2DC16 #GOPDebate   \n",
       "23                        #WinBenWin #BC2DC16 #GOPDebate   \n",
       "24                                                   NaN   \n",
       "25     #America #BC2DC16 #carson2016 #ben4potus #winb...   \n",
       "26                                                 #ISIS   \n",
       "27                                            #GOPDebate   \n",
       "28                                                   NaN   \n",
       "29                        #WinBenWin #BC2DC16 #GOPDebate   \n",
       "...                                                  ...   \n",
       "99002                                                NaN   \n",
       "99003                                          #CruzCrew   \n",
       "99004                                                NaN   \n",
       "99005                                                NaN   \n",
       "99006                                        #ChooseCruz   \n",
       "99007                                     #AbolishTheIRS   \n",
       "99008                                                NaN   \n",
       "99009                                                NaN   \n",
       "99010                                                NaN   \n",
       "99011                                            #TaxDay   \n",
       "99012                                     #AbolishTheIRS   \n",
       "99013                                          #CruzCrew   \n",
       "99014                                            #fierce   \n",
       "99015                                                NaN   \n",
       "99016                                                NaN   \n",
       "99017                                           #NewYork   \n",
       "99018                                                NaN   \n",
       "99019                                     #AbolishTheIRS   \n",
       "99020                                                NaN   \n",
       "99021          #ChooseCruz #PJNET #ChooseCruz #IRS #jobs   \n",
       "99022                                                NaN   \n",
       "99023                                                NaN   \n",
       "99024                                                NaN   \n",
       "99025                                                NaN   \n",
       "99026                                                NaN   \n",
       "99027                                                NaN   \n",
       "99028                                                NaN   \n",
       "99029                                                NaN   \n",
       "99030                                                NaN   \n",
       "99031                                                NaN   \n",
       "\n",
       "                                                     at.  \\\n",
       "0                                         @OneNation4Ben   \n",
       "1                                         @ThePatriot143   \n",
       "2                                           @nprpolitics   \n",
       "3                                                    NaN   \n",
       "4                                            @SandyMPool   \n",
       "5                                         @BreitbartNews   \n",
       "6                                             @irgrannyg   \n",
       "7      @SusanCucinotta @RealBenCarson @YouthForCarson...   \n",
       "8                                       @Cry2MarvinsRoom   \n",
       "9                                           @FoxBusiness   \n",
       "10                                         @healerhauler   \n",
       "11                                                   NaN   \n",
       "12                                        @RealBenCarson   \n",
       "13                                         @healerhauler   \n",
       "14                                        @RealBenCarson   \n",
       "15                                           @SandyMPool   \n",
       "16                                        @AmeliaNaomi14   \n",
       "17                                      @catherine_punja   \n",
       "18                                                   NaN   \n",
       "19                                        @BlytheTravels   \n",
       "20                                                  @CNN   \n",
       "21                                               @TheFix   \n",
       "22                                                   NaN   \n",
       "23                                                   NaN   \n",
       "24                                        @RealBenCarson   \n",
       "25                             @ElenaNinj @RealBenCarson   \n",
       "26                                        @RealBenCarson   \n",
       "27                                        @RealBenCarson   \n",
       "28                                        @RealBenCarson   \n",
       "29                                                   NaN   \n",
       "...                                                  ...   \n",
       "99002                              @betsy_klein @tedcruz   \n",
       "99003                                                NaN   \n",
       "99004                                    @MSNBC @tedcruz   \n",
       "99005                                     @jamesoliphant   \n",
       "99006                                                NaN   \n",
       "99007                                                NaN   \n",
       "99008                                                NaN   \n",
       "99009                                        @haaretzcom   \n",
       "99010                          @bbryantjones @yidwithlid   \n",
       "99011                                        @dcexaminer   \n",
       "99012                                                NaN   \n",
       "99013                            @oreillyfactor @FoxNews   \n",
       "99014                   @MonicaCrowley @TedCruz @FoxNews   \n",
       "99015  @MarkHalperin @tedcruz @brucereddenjr @MattNegrin   \n",
       "99016                                     @oreillyfactor   \n",
       "99017                             @RabbiShmuley @tedcruz   \n",
       "99018                                       @nytpolitics   \n",
       "99019                                                NaN   \n",
       "99020                                  @ABCPolitics @GMA   \n",
       "99021                                 @GA6thForCruz @WSJ   \n",
       "99022                                       @RonRoesener   \n",
       "99023                                         @Nightline   \n",
       "99024                                           @trscoop   \n",
       "99025                                                NaN   \n",
       "99026                                      @navycrawfish   \n",
       "99027                                    @JeanieHannaman   \n",
       "99028               @rickklein @tedcruz @heidiscruz @GMA   \n",
       "99029                                      @alexahenning   \n",
       "99030                                     @proteinwisdom   \n",
       "99031                                                NaN   \n",
       "\n",
       "                                                  link  retweets  favorites  \\\n",
       "0                              https://t.co/cO9Z9h2Bfg       8.0        0.0   \n",
       "1                                                  NaN      29.0        0.0   \n",
       "2                                                  NaN      18.0        0.0   \n",
       "3                                                  NaN       8.0        8.0   \n",
       "4                                                  NaN       7.0        0.0   \n",
       "5                                                 http      45.0        0.0   \n",
       "6                              https://t.co/7oevzZnFFR       7.0        0.0   \n",
       "7                                                  NaN      13.0        0.0   \n",
       "8                                                  NaN       7.0        0.0   \n",
       "9                                                  NaN      28.0        0.0   \n",
       "10                                                 NaN      24.0        0.0   \n",
       "11                                                 NaN       7.0       11.0   \n",
       "12                             https://t.co/wwM2M7x6TS     491.0        0.0   \n",
       "13                                                 NaN      12.0        0.0   \n",
       "14                             https://t.co/eKNNznc5QR     227.0        0.0   \n",
       "15                                                 NaN       6.0        0.0   \n",
       "16                                                 NaN      13.0        0.0   \n",
       "17                                                 NaN       7.0        0.0   \n",
       "18                                                 NaN       7.0        8.0   \n",
       "19                             https://t.co/9N2mZKmU5J      10.0        0.0   \n",
       "20                                                 NaN      18.0       30.0   \n",
       "21                                                 NaN      36.0        0.0   \n",
       "22                                                 NaN      32.0       37.0   \n",
       "23                                                 NaN       7.0        9.0   \n",
       "24                                                 NaN    1597.0        0.0   \n",
       "25                                             https:/      20.0        0.0   \n",
       "26                                             https:/     323.0        0.0   \n",
       "27                             https://t.co/o5clvDbF06      97.0        0.0   \n",
       "28                             https://t.co/lQy2VJ5dOY      76.0        0.0   \n",
       "29                                                 NaN      16.0       16.0   \n",
       "...                                                ...       ...        ...   \n",
       "99002                          https://t.co/p7a6U8RuCo     233.0        0.0   \n",
       "99003                          https://t.co/zzvTc4izqZ     157.0      263.0   \n",
       "99004  https://t.co/PuZyZ0pnLg https://t.co/gqm2kTuXnM     154.0        0.0   \n",
       "99005                          https://t.co/pSkMkCqAhN     243.0        0.0   \n",
       "99006                          https://t.co/duuwFF5swR     478.0      951.0   \n",
       "99007  https://t.co/QjbHUcdxzU https://t.co/RgR0HbU5i7     470.0      730.0   \n",
       "99008  https://t.co/ZLKZVzMh4f https://t.co/Aivb7pSfWs     464.0      657.0   \n",
       "99009  https://t.co/LYYDC3SsWr https://t.co/vl5ot5ebAy     382.0        0.0   \n",
       "99010                          https://t.co/AyqiYlBwSp     308.0        0.0   \n",
       "99011  https://t.co/9H0V1rpem6 https://t.co/YfjcyU4Pj1     183.0        0.0   \n",
       "99012  https://t.co/ZLKZVzMh4f https://t.co/age0ZcZ9gH     340.0      626.0   \n",
       "99013                          https://t.co/HAOKWYRhtn     264.0      503.0   \n",
       "99014                          https://t.co/TwgPaKsyJt     414.0        0.0   \n",
       "99015                          https://t.co/cEJH0G0p7I     332.0        0.0   \n",
       "99016                          https://t.co/QmPcH9cI5v     173.0        0.0   \n",
       "99017                          https://t.co/RXGHxZs2du     381.0        0.0   \n",
       "99018                                         https://     488.0        0.0   \n",
       "99019  https://t.co/QjbHUcdxzU https://t.co/c8wQ4ye5rx     451.0      723.0   \n",
       "99020                  https://t.co/RdK6Nu2WvO https:/     283.0        0.0   \n",
       "99021                          https://t.co/LpI0ALR5wQ     185.0        0.0   \n",
       "99022                                        https://t     391.0        0.0   \n",
       "99023  https://t.co/9iFTNBvo8T https://t.co/2qqKooOI5y     172.0        0.0   \n",
       "99024  https://t.co/5x413C6HNv https://t.co/TCysnHSUOY     305.0        0.0   \n",
       "99025                          https://t.co/dVdBlRlr5J     659.0     1207.0   \n",
       "99026                          https://t.co/bbnmO4pCGU     219.0        0.0   \n",
       "99027                          https://t.co/8ZWyhxqTQP     200.0        0.0   \n",
       "99028                                https://t.co/MXPw     140.0        0.0   \n",
       "99029                          https://t.co/SXErrSxHLn     196.0        0.0   \n",
       "99030                          https://t.co/AWhJ7NMllC     253.0        0.0   \n",
       "99031                          https://t.co/sLKJ1JVnuq     577.0     1216.0   \n",
       "\n",
       "         ...     dur_camp  pre_2016  post_ge  pre_prim  post_prim  \\\n",
       "0        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "1        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "2        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "3        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "4        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "5        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "6        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "7        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "8        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "9        ...          1.0       0.0      0.0       1.0        0.0   \n",
       "10       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "11       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "12       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "13       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "14       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "15       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "16       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "17       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "18       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "19       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "20       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "21       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "22       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "23       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "24       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "25       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "26       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "27       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "28       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "29       ...          1.0       0.0      0.0       1.0        0.0   \n",
       "...      ...          ...       ...      ...       ...        ...   \n",
       "99002    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99003    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99004    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99005    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99006    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99007    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99008    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99009    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99010    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99011    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99012    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99013    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99014    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99015    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99016    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99017    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99018    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99019    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99020    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99021    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99022    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99023    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99024    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99025    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99026    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99027    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99028    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99029    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99030    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "99031    ...          1.0       0.0      0.0       1.0        0.0   \n",
       "\n",
       "                                              clean_text  rt_dummy  \\\n",
       "0            rt @onenation4ben: i support #bencarson2016         1   \n",
       "1      rt @thepatriot143: classy act by ben carson #g...         1   \n",
       "2      rt @nprpolitics: extended medical metaphor fro...         1   \n",
       "3             carson takes the stage #winbenwin #bc2dc16         0   \n",
       "4      rt @sandympool: ben carson will bring leadersh...         1   \n",
       "5      rt @breitbartnews: ben carson begins with a mo...         1   \n",
       "6                                rt @irgrannyg: go, ben!         1   \n",
       "7      rt @susancucinotta: #carsonators #students4car...         1   \n",
       "8      rt @cry2marvinsroom: ben carson with the fire ...         1   \n",
       "9      rt @foxbusiness: .@realbencarson: right now, t...         1   \n",
       "10     rt @healerhauler: #thebestmedicinefor america ...         1   \n",
       "11                           .@cnn please i asked nicely         0   \n",
       "12                                    rt @realbencarson:         1   \n",
       "13     rt @healerhauler: if you were wondering about ...         1   \n",
       "14                                    rt @realbencarson:         1   \n",
       "15     rt @sandympool: ben carson wins with gifted ha...         1   \n",
       "16              rt @amelianaomi14: ben carson is the man         1   \n",
       "17         rt @catherine_punja: ben carson for president         1   \n",
       "18     carson is 4th from the left and to the left of...         0   \n",
       "19     rt @blythetravels: and the golden globe for be...         1   \n",
       "20        @cnn can you ask ben carson a question please?         0   \n",
       "21     rt @thefix: opening statements ranked: 1. cars...         1   \n",
       "22     \"we need to declare war on isis\" - ben carson ...         0   \n",
       "23     each candidate gets 1 minute to introduce them...         0   \n",
       "24     rt @realbencarson: lead us tonight lord, take ...         1   \n",
       "25     rt @elenaninj: #america needs #bc2dc16 #carson...         1   \n",
       "26     rt @realbencarson: today, i released my plan t...         1   \n",
       "27     rt @realbencarson: visiting the so called \"spi...         1   \n",
       "28     rt @realbencarson: check out candy's new chris...         1   \n",
       "29     .@cnn hey one question for ben carson would be...         0   \n",
       "...                                                  ...       ...   \n",
       "99002           rt @betsy_klein: twins for in towson, md         1   \n",
       "99003                      #cruzcrew: join us live here:         0   \n",
       "99004  rt @msnbc: live now: holds a campaign rally in...         1   \n",
       "99005           rt @jamesoliphant: cruz crowd in towson.         1   \n",
       "99006  \"we have full confidence that a cruz white hou...         0   \n",
       "99007          stop the monkey business. #abolishtheirs:         0   \n",
       "99008                      my tax plan is this simple...         0   \n",
       "99009  rt @haaretzcom: why there is no greater friend...         1   \n",
       "99010  rt @bbryantjones: why i will be voting for ted...         1   \n",
       "99011  rt @dcexaminer: here's how ted cruz celebrates...         1   \n",
       "99012                                    #abolishtheirs:         0   \n",
       "99013  #cruzcrew: don't miss me on the @oreillyfactor...         0   \n",
       "99014  rt @monicacrowley: so great to see my friend t...         1   \n",
       "99015  rt @markhalperin: watch: @tedcruz, @bruceredde...         1   \n",
       "99016  rt @oreillyfactor: ted cruz joins the factor f...         1   \n",
       "99017  rt @rabbishmuley: with night before the #newyo...         1   \n",
       "99018  rt @nytpolitics: ted cruz's views over 2 decad...         1   \n",
       "99019    it's tax day! join the fight to #abolishtheirs:         0   \n",
       "99020  rt @abcpolitics: .@tedcruz to @gma: i'm the on...         1   \n",
       "99021  rt @ga6thforcruz: ted cruz in @wsj a simple fl...         1   \n",
       "99022  rt @ronroesener: this addresses something no o...         1   \n",
       "99023  rt @nightline: ted cruzs wife heidi cruz says ...         1   \n",
       "99024  rt @trscoop: watch: ted cruzs town hall on goo...         1   \n",
       "99025  \"in the case of ted cruz, we have a candidate ...         0   \n",
       "99026  rt @navycrawfish: why i support ted cruz | pj ...         1   \n",
       "99027  rt @jeaniehannaman: video: ted cruz does cnbcs...         1   \n",
       "99028  rt @rickklein: fun talking to and @heidiscruz ...         1   \n",
       "99029        rt @alexahenning: ted cruz: the tax basher!         1   \n",
       "99030                      rt @proteinwisdom: vote cruz.         1   \n",
       "99031  a joyous reminder that you can kiss irs-stress...         0   \n",
       "\n",
       "             date  lastname   forecast  \n",
       "0      12/16/2015    Carson  13.491012  \n",
       "1      12/16/2015    Carson  13.491012  \n",
       "2      12/16/2015    Carson  13.491012  \n",
       "3      12/16/2015    Carson  13.491012  \n",
       "4      12/16/2015    Carson  13.491012  \n",
       "5      12/16/2015    Carson  13.491012  \n",
       "6      12/16/2015    Carson  13.491012  \n",
       "7      12/16/2015    Carson  13.491012  \n",
       "8      12/16/2015    Carson  13.491012  \n",
       "9      12/16/2015    Carson  13.491012  \n",
       "10     12/16/2015    Carson  13.491012  \n",
       "11     12/16/2015    Carson  13.491012  \n",
       "12     12/16/2015    Carson  13.491012  \n",
       "13     12/16/2015    Carson  13.491012  \n",
       "14     12/16/2015    Carson  13.491012  \n",
       "15     12/16/2015    Carson  13.491012  \n",
       "16     12/16/2015    Carson  13.491012  \n",
       "17     12/16/2015    Carson  13.491012  \n",
       "18     12/16/2015    Carson  13.491012  \n",
       "19     12/16/2015    Carson  13.491012  \n",
       "20     12/16/2015    Carson  13.491012  \n",
       "21     12/16/2015    Carson  13.491012  \n",
       "22     12/16/2015    Carson  13.491012  \n",
       "23     12/16/2015    Carson  13.491012  \n",
       "24     12/16/2015    Carson  13.491012  \n",
       "25     12/16/2015    Carson  13.491012  \n",
       "26     12/16/2015    Carson  13.491012  \n",
       "27     12/16/2015    Carson  13.491012  \n",
       "28     12/16/2015    Carson  13.491012  \n",
       "29     12/16/2015    Carson  13.491012  \n",
       "...           ...       ...        ...  \n",
       "99002  04/18/2016      Cruz  28.600157  \n",
       "99003  04/18/2016      Cruz  28.600157  \n",
       "99004  04/18/2016      Cruz  28.600157  \n",
       "99005  04/18/2016      Cruz  28.600157  \n",
       "99006  04/18/2016      Cruz  28.600157  \n",
       "99007  04/18/2016      Cruz  28.600157  \n",
       "99008  04/18/2016      Cruz  28.600157  \n",
       "99009  04/18/2016      Cruz  28.600157  \n",
       "99010  04/18/2016      Cruz  28.600157  \n",
       "99011  04/18/2016      Cruz  28.600157  \n",
       "99012  04/18/2016      Cruz  28.600157  \n",
       "99013  04/18/2016      Cruz  28.600157  \n",
       "99014  04/18/2016      Cruz  28.600157  \n",
       "99015  04/18/2016      Cruz  28.600157  \n",
       "99016  04/18/2016      Cruz  28.600157  \n",
       "99017  04/18/2016      Cruz  28.600157  \n",
       "99018  04/18/2016      Cruz  28.600157  \n",
       "99019  04/18/2016      Cruz  28.600157  \n",
       "99020  04/18/2016      Cruz  28.600157  \n",
       "99021  04/18/2016      Cruz  28.600157  \n",
       "99022  04/18/2016      Cruz  28.600157  \n",
       "99023  04/18/2016      Cruz  28.600157  \n",
       "99024  04/18/2016      Cruz  28.600157  \n",
       "99025  04/18/2016      Cruz  28.600157  \n",
       "99026  04/18/2016      Cruz  28.600157  \n",
       "99027  04/18/2016      Cruz  28.600157  \n",
       "99028  04/18/2016      Cruz  28.600157  \n",
       "99029  04/18/2016      Cruz  28.600157  \n",
       "99030  04/18/2016      Cruz  28.600157  \n",
       "99031  04/18/2016      Cruz  28.600157  \n",
       "\n",
       "[99032 rows x 48 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99032, 48)\n"
     ]
    }
   ],
   "source": [
    "print merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Lines to Pull and Update Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collect_data(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "#collect_data(path + pac_tweets, pac_sheet, path + tweet_list)\n",
    "\n",
    "collect_addition_data(path + cand_tweets, cand_sheet, path + tweet_list)\n",
    "#collect_addition_data(path + pac_tweets, pac_sheet, path + tweet_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#curr_size = len(follower_count_df)\n",
    "#prev_size = 0\n",
    "#while (curr_size != prev_size):\n",
    "#  prev_size = curr_size\n",
    "#  entries_to_drop = []\n",
    "#  for index, row in follower_count_df.iterrows():\n",
    "#    curr_date = int((row['date']) / 1000000)\n",
    "#    #curr_date = (row['date'][0:8])\n",
    "#    if (index != 0):\n",
    "#      if (curr_date == prev_date[1]):\n",
    "#        print 'here'\n",
    "#        entries_to_drop.append(prev_date[0])\n",
    "#    prev_date = (index, curr_date)\n",
    "#\n",
    "#  print  \"to drop: \" + str(len(entries_to_drop))\n",
    "#  entries_to_keep = set(range(len(follower_count_df))) - set(entries_to_drop)\n",
    "#  #test_df = follower_count_df.drop(entries_to_drop, axis=0)\n",
    "#  follower_count_df = follower_count_df.take(list(entries_to_keep))\n",
    "#  curr_size = len(follower_count_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
